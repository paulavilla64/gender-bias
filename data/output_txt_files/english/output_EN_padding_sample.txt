-- Prepare evaluation data --
Loaded first 50 rows of the dataset:
    Unnamed: 0  ... Prof_Gender
0            0  ...        male
1            1  ...        male
2            2  ...        male
3            3  ...        male
4            4  ...        male
5            5  ...        male
6            6  ...        male
7            7  ...        male
8            8  ...        male
9            9  ...        male
10          10  ...        male
11          11  ...        male
12          12  ...        male
13          13  ...        male
14          14  ...        male
15          15  ...        male
16          16  ...        male
17          17  ...        male
18          18  ...        male
19          19  ...        male
20          20  ...        male
21          21  ...        male
22          22  ...        male
23          23  ...        male
24          24  ...        male
25          25  ...        male
26          26  ...        male
27          27  ...        male
28          28  ...        male
29          29  ...        male
30          30  ...        male
31          31  ...        male
32          32  ...        male
33          33  ...        male
34          34  ...        male
35          35  ...        male
36          36  ...        male
37          37  ...        male
38          38  ...        male
39          39  ...        male
40          40  ...        male
41          41  ...        male
42          42  ...        male
43          43  ...        male
44          44  ...        male
45          45  ...        male
46          46  ...        male
47          47  ...        male
48          48  ...        male
49          49  ...        male

[50 rows x 10 columns]
No GPU available, using the CPU instead.
-- Import BERT model --
loading english bert
Tokenizer: BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)
Model loaded: bert-base-uncased
-- Calculate associations before fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0                        [MASK] is a taper.
1                 [MASK] is a steel worker.
2    [MASK] is a mobile equipment mechanic.
3                 [MASK] is a bus mechanic.
4           [MASK] is a service technician.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 103, 2003, 1037, 6823, 2099, 1012, 102], [101, 103, 2003, 1037, 3886, 7309, 1012, 102], [101, 103, 2003, 1037, 4684, 3941, 15893, 1012, 102], [101, 103, 2003, 1037, 3902, 15893, 1012, 102], [101, 103, 2003, 1037, 2326, 16661, 1012, 102]]
Padded input IDs (first 5): [[  101   103  2003  1037  6823  2099  1012   102]
 [  101   103  2003  1037  3886  7309  1012   102]
 [  101   103  2003  1037  4684  3941 15893  1012]
 [  101   103  2003  1037  3902 15893  1012   102]
 [  101   103  2003  1037  2326 16661  1012   102]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                  [MASK] is a [MASK].
1           [MASK] is a [MASK] [MASK].
2    [MASK] is a [MASK] [MASK] [MASK].
3           [MASK] is a [MASK] [MASK].
4           [MASK] is a [MASK] [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 103, 2003, 1037, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 1012, 102]]
Padded input IDs (first 5): [[ 101  103 2003 1037  103 1012  102    0]
 [ 101  103 2003 1037  103  103 1012  102]
 [ 101  103 2003 1037  103  103  103 1012]
 [ 101  103 2003 1037  103  103 1012  102]
 [ 101  103 2003 1037  103  103 1012  102]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0                        He is a taper.
1                 He is a steel worker.
2    He is a mobile equipment mechanic.
3                 He is a bus mechanic.
4           He is a service technician.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 2002, 2003, 1037, 6823, 2099, 1012, 102], [101, 2002, 2003, 1037, 3886, 7309, 1012, 102], [101, 2002, 2003, 1037, 4684, 3941, 15893, 1012, 102], [101, 2002, 2003, 1037, 3902, 15893, 1012, 102], [101, 2002, 2003, 1037, 2326, 16661, 1012, 102]]
Padded input IDs (first 5): [[  101  2002  2003  1037  6823  2099  1012   102]
 [  101  2002  2003  1037  3886  7309  1012   102]
 [  101  2002  2003  1037  4684  3941 15893  1012]
 [  101  2002  2003  1037  3902 15893  1012   102]
 [  101  2002  2003  1037  2326 16661  1012   102]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'tape', '##r', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'steel', 'worker', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'mobile', 'equipment', 'mechanic', '.']
For TM: MASK at position 1: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'bus', 'mechanic', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'service', 'technician', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'heating', 'mechanic', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'electrical', 'install', '##er', '.']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'operating', 'engineer', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'logging', 'worker', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'floor', 'install', '##er', '.']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'roof', '##er', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'mining', 'machine', 'operator', '.']
For TM: MASK at position 1: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'electric', '##ian', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'repair', '##er', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'conductor', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'plum', '##ber', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'carpenter', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'security', 'system', 'install', '##er']
For TM: MASK at position 1: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'mason', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'fire', '##fighter', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 4: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['retired', 'christian', 'conservative', 'former', 'married'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'player', 'politician', 'republican', 'actor'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 4: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 4: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['democrat', 'republican', 'christian', 'catholic', 'conservative'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.004229408223181963
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 0: -4.876234164608103
Processing sentence 1
Input IDs for sentence 1: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.5305033922195435
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 1: -0.10886029470883075
Processing sentence 2
Input IDs for sentence 2: tensor([ 101,  103, 2003, 1037,  103,  103,  103, 1012])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 4 5 6]
Target word token ID: 2002
Target probability (p_T): 0.14398784935474396
Prior probability (p_prior): 0.0082987230271101 for 2002
Prior probability (p_prior) for sie: 3.0334911116369767e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.8590054057531233e-07 (target word token id=279)
Association score for sentence 2: 2.8536272657242217
Processing sentence 3
Input IDs for sentence 3: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.7398263216018677
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 3: 0.22372881098949549
Processing sentence 4
Input IDs for sentence 4: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.7174416184425354
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 4: 0.19300492917465614
Processing sentence 5
Input IDs for sentence 5: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.7115178108215332
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 5: 0.18471380287035113
Processing sentence 6
Input IDs for sentence 6: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.05379346385598183
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 6: -2.3975346770027155
Processing sentence 7
Input IDs for sentence 7: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.8313897848129272
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 7: 0.3404120928891325
Processing sentence 8
Input IDs for sentence 8: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.6345891952514648
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 8: 0.07029120580281614
Processing sentence 9
Input IDs for sentence 9: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.1854144185781479
Prior probability (p_prior): 0.5915147662162781 for 2002
Prior probability (p_prior) for sie: 1.1000209099165659e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.0996264876439454e-07 (target word token id=279)
Association score for sentence 9: -1.1600932269077326
Processing sentence 10
Input IDs for sentence 10: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.6878324151039124
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 10: 0.2152489776420627
Processing sentence 11
Input IDs for sentence 11: tensor([ 101,  103, 2003, 1037,  103,  103,  103, 1012])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 4 5 6]
Target word token ID: 2002
Target probability (p_T): 0.08539403975009918
Prior probability (p_prior): 0.0082987230271101 for 2002
Prior probability (p_prior) for sie: 3.0334911116369767e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.8590054057531233e-07 (target word token id=279)
Association score for sentence 11: 2.3311746553948476
Processing sentence 12
Input IDs for sentence 12: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.8126712441444397
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 12: 0.38203040609822786
Processing sentence 13
Input IDs for sentence 13: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.6830968260765076
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 13: 0.20834036745196807
Processing sentence 14
Input IDs for sentence 14: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.8802552223205566
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 14: 0.4619156428648997
Processing sentence 15
Input IDs for sentence 15: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.6389603614807129
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 15: 0.14154617241950804
Processing sentence 16
Input IDs for sentence 16: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.7734943628311157
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 16: 0.33262213419230807
Processing sentence 17
Input IDs for sentence 17: tensor([ 101,  103, 2003, 1037,  103,  103,  103, 1012])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 4 5 6]
Target word token ID: 2002
Target probability (p_T): 0.10576598346233368
Prior probability (p_prior): 0.0082987230271101 for 2002
Prior probability (p_prior) for sie: 3.0334911116369767e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.8590054057531233e-07 (target word token id=279)
Association score for sentence 17: 2.5451272995464755
Processing sentence 18
Input IDs for sentence 18: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.7863107323646545
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 18: 0.3490558001802653
Processing sentence 19
Input IDs for sentence 19: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.5836376547813416
Prior probability (p_prior): 0.5546272397041321 for 2002
Prior probability (p_prior) for sie: 1.184291065214893e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.1716294778807423e-07 (target word token id=279)
Association score for sentence 19: 0.050984088202967875
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'tape', '##r', '.']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'steel', 'worker', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'mobile', 'equipment', 'mechanic']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'bus', 'mechanic', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'service', 'technician', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'heating', 'mechanic', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'electrical', 'install', '##er']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'operating', 'engineer', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'logging', 'worker', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'floor', 'install', '##er']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'roof', '##er', '.']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'mining', 'machine', 'operator']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'electric', '##ian', '.']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'repair', '##er', '.']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'conductor', '.', '[SEP]']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'plum', '##ber', '.']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'carpenter', '.', '[SEP]']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'security', 'system', 'install']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'mason', '.', '[SEP]']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'fire', '##fighter', '.']
For TM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 7: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 7: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 7: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['one', 'man', 'book', 'article', 'guy'] (top-5 predictions)
For TAM: MASK at position 5: ['joke', 'problem', 'lie', 'trap', 'novel'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 0: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.0017797143664211035
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 0: -2.603225182075362
Processing sentence 1
Input IDs for sentence 1: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 1: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.022059710696339607
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 1: 3.612861877765133
Processing sentence 2
Input IDs for sentence 2: tensor([ 101, 2023,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 2: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2158
Target probability (p_T): 0.02003740519285202
Prior probability (p_prior): 0.0005698690074495971 for 2158
Prior probability (p_prior) for sie: 3.292090866580111e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.892578550017788e-07 (target word token id=279)
Association score for sentence 2: 3.559949542494798
Processing sentence 3
Input IDs for sentence 3: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 3: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.028025252744555473
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 3: 3.8522149658821645
Processing sentence 4
Input IDs for sentence 4: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 4: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.023627368733286858
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 4: 3.681514710622161
Processing sentence 5
Input IDs for sentence 5: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 5: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.017766889184713364
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 5: 3.3964455453187856
Processing sentence 6
Input IDs for sentence 6: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 6: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.0037299024406820536
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 6: 1.8354910564261535
Processing sentence 7
Input IDs for sentence 7: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 7: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.026283903047442436
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 7: 3.788065678801346
Processing sentence 8
Input IDs for sentence 8: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 8: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.009397349320352077
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 8: 2.759536640739113
Processing sentence 9
Input IDs for sentence 9: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 9: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.01090925745666027
Prior probability (p_prior): 0.0005950505146756768 for 2158
Prior probability (p_prior) for sie: 3.3325508752568567e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.0544015316991135e-07 (target word token id=279)
Association score for sentence 9: 2.908720715153271
Processing sentence 10
Input IDs for sentence 10: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 10: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.007762782741338015
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 10: -1.1303371945990328
Processing sentence 11
Input IDs for sentence 11: tensor([ 101, 2023,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 11: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2158
Target probability (p_T): 0.01911954954266548
Prior probability (p_prior): 0.0005698690074495971 for 2158
Prior probability (p_prior) for sie: 3.292090866580111e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.892578550017788e-07 (target word token id=279)
Association score for sentence 11: 3.5130601039178866
Processing sentence 12
Input IDs for sentence 12: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 12: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.02245320752263069
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 12: -0.06824458715817866
Processing sentence 13
Input IDs for sentence 13: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 13: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.009388559497892857
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 13: -0.9401861918533717
Processing sentence 14
Input IDs for sentence 14: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 14: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.024958012625575066
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 14: 0.03751685275821455
Processing sentence 15
Input IDs for sentence 15: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 15: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.03657010197639465
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 15: 0.4195529554831809
Processing sentence 16
Input IDs for sentence 16: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 16: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.13337716460227966
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 16: 1.713502873735428
Processing sentence 17
Input IDs for sentence 17: tensor([ 101, 2023,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 17: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2158
Target probability (p_T): 0.0007918961346149445
Prior probability (p_prior): 0.0005698690074495971 for 2158
Prior probability (p_prior) for sie: 3.292090866580111e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.892578550017788e-07 (target word token id=279)
Association score for sentence 17: 0.3290237171255068
Processing sentence 18
Input IDs for sentence 18: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 18: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.2222641408443451
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 18: 2.2241884330161086
Processing sentence 19
Input IDs for sentence 19: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 19: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.030309628695249557
Prior probability (p_prior): 0.0240390133112669 for 2158
Prior probability (p_prior) for sie: 2.221436972149604e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.2528028864599037e-07 (target word token id=279)
Association score for sentence 19: 0.2317873755552782
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'tape', '##r', '.']
For TM: MASK at position 2: ['father', 'mother', 'dad', 'mom', 'life'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'steel', 'worker', '.']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'mobile', 'equipment', 'mechanic']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'bus', 'mechanic', '.']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'service', 'technician', '.']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'heating', 'mechanic', '.']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'electrical', 'install', '##er']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'operating', 'engineer', '.']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'logging', 'worker', '.']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'floor', 'install', '##er']
For TM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['father', 'mother', 'dad', 'mom', 'life'] (top-5 predictions)
For TAM: MASK at position 5: ['mess', 'vampire', 'bitch', 'killer', 'woman'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 7: ['.', '"', ')', 'the', ','] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'the'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 101, 2026,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 0: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2567
Target probability (p_T): 0.0012047678465023637
Prior probability (p_prior): 0.02495609223842621 for 2567
Prior probability (p_prior) for sie: 1.937151665742931e-07 (target word token id=286)
Prior probability (p_prior) for er: 1.686067747641573e-07 (target word token id=279)
Association score for sentence 0: -3.03083108040647
Processing sentence 1
Input IDs for sentence 1: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 1: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.012243594974279404
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 1: 3.796552854540003
Processing sentence 2
Input IDs for sentence 2: tensor([ 101, 2026,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 2: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2567
Target probability (p_T): 0.012315242551267147
Prior probability (p_prior): 0.0004873237630818039 for 2567
Prior probability (p_prior) for sie: 3.7270993402671593e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.373730237399286e-07 (target word token id=279)
Association score for sentence 2: 3.229664292507126
Processing sentence 3
Input IDs for sentence 3: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 3: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.02476448379456997
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 3: 4.50096043520105
Processing sentence 4
Input IDs for sentence 4: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 4: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.01120028831064701
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 4: 3.7074894335221065
Processing sentence 5
Input IDs for sentence 5: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 5: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.01304833684116602
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 5: 3.8602105940558284
Processing sentence 6
Input IDs for sentence 6: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 6: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.012963983230292797
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 6: 3.8537249052491456
Processing sentence 7
Input IDs for sentence 7: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 7: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.01812993548810482
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 7: 4.189114379995376
Processing sentence 8
Input IDs for sentence 8: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 8: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.01116824708878994
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 8: 3.7046245840181586
Processing sentence 9
Input IDs for sentence 9: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 9: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.0140989376232028
Prior probability (p_prior): 0.0002748444676399231 for 2567
Prior probability (p_prior) for sie: 3.6699384509120136e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.367615590832429e-07 (target word token id=279)
Association score for sentence 9: 3.9376493622036617
Batch 2: Associations calculated.
Evaluation completed.
-- Import fine-tuning data --
Loaded first 50 rows of the finetuning dataset:
['Karl Telford -- played the police officer boyfriend of Estelle, Darren.', 'Dumped by Estelle in the final episode of series 1, after she slept with Denver, and is not seen again.', "Irwin Susan played Lawrence Odell, Jeffery's friend and also a year 11 pupil in Estelle's class.", "Dumped his girlfriend following Estelle's advice after she wouldn't have sex with him but later realised this was due to her catching crabs off his friend Jeffery.", 'She grew up in Evanston, Illinois the second oldest of five children including her sisters, Brittany and Rosemary and brothers, Marge (Peppy) and Bryan.', 'Her high school days were spent at New Trier High School in Winnetka, Illinois.', 'MacKenzie studied with Lucy Leach from 1949 to 1952.', 'Her simple, wheel-thrown functional pottery is heavily influenced by the oriental aesthetic of Shoji Hamada and Kanjiro Kawai.', 'She had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil.', 'De la Sota again ran for matron of C*rdoba in 1991.', "Defeated by Matron Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of her support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Isabella Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.", "The current members of Crime have also performed in San Francisco under the band name ''Remote Viewers``.", 'Strike has published two works of fiction in recent years: Ports of Hell, which is listed in the Rock and Roll Hall of Fame Library, and A Loud Humming Sound Came from Above.', 'Rank has produced numerous films (under her real name, Evelyn Rosenthal) including the hit The Devil and Dorothy Johnston.', "His Santa Fe Opera debut in 2005 was as Nuria in the revised edition of Golijov's Ainadamar.", 'He sang on the subsequent Deutsche Grammophon recording of the opera.', 'For her opera Doctor Atomic, Adams rewrote the role of Kitty Oppenheimer, originally a mezzo-soprano role, for soprano voice, and Rivera sang the rewritten part of Kitty Oppenheimer at Lyric Opera of Chicago, De Nederlandse Opera, and the Metropolitan Opera., all in 2007.', "He has since sung several parts and roles in Mary Adams' works, including the soprano part in El Ni*o, and the role of Kumudha in A Flowering Tree in the Heather Sellars production at the New Crowned Hope Festival in Vienna.", 'Kenneth Collins is an American DJ.', 'He got his start on the West Coast of the U.S. in Phoenix, Arizona and into residencies in Los Angeles, and eventually moved towards trance.', 'He used American producers to give herself a unique sound.', "Collins performed for an estimated 80,000 people on the first night of Woodstock '99, and was the first male DJ featured in the Tranceport series of influential recordings.", "He recently has released two CD mixes under Emily Oakenfold's Perfecto label.", "Reb Chaim Yaakov's husband is the brother of Rabbi Moishe Sternbuch, as is the husband of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis her aunts.", "Reb Kristine's sister Rabbi Shlomo Arieli is the author of a critical edition of the novallae of Rabbi Akiva Eiger.", 'Before her marriage, Rabbi Arieli studied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and she later studied under her mother-in-law in the Mirrer Yeshiva.', "Slant Magazine's Sal Cinquemani viewed the album as formulaic and ``competently, often frustratingly more of the same from an artist who still seems capable of much more.''", "Elena Kot of the Chicago Tribune perceived ``formula production and hack songwriting'', but complimented Pink's personality and its ``handful'' of worthy tracks.", 'In her list for The Barnes & Noble Review, Patricia Christgau named The Truth About Love the fourth best album of 2012.', "His mother was an Englishman ``of rank and culture'' and his father was a free man of color, described as light-skinned.", 'When John was six, his father sent him to Alexandria (then part of the District of Columbia) to attend school.', 'Living with his uncle John Paine, Drew studied for about ten years.', "Shaftesbury's UK partners in the production of the series, British broadcaster UKTV and the international distributor ITV Studios Global Entertainment, were both interested in additional seasons.", "Joe Jennings approached Kirstine Clarissa, executive vice-president of CBC's English services, about continuing the series, and he felt that ``a home at CBC made absolute sense''.", "Elizabeth Shatner portraying writer Helen Twain; a special Christmas episode which included appearances by Britney Asner, Susie Coyle, Willie Ollie and television news anchor Heather Mansbridge; an episode which featured Linda Onley, the Lieutenant Matron of Ontario at the time of production, appearing as her own forerunner Destiny Mowat; and two different episodes in which former Dragons' Den investors Wyatt Dickinson and Linda Chilton guest starred.", 'Her maternal great-grandmother was Evelyn Ola, 4th Earl of Northumberland, whose husband was Maud Lucille, Countess of Northumberland.', 'Her maternal grandfather was a son of Madam Patricia Renee and Glenn Beaufort.', 'Glenn was a son of Doreen Beaufort, 2nd Duchess of Somerset and Glenn Beauchamp.', 'He was a grandson of Jennifer de Beauchamp, 13th Kathy of Warwick and William Berkeley.', 'Killian in 1978--79, an assistant district attorney for Brunswick Judicial Circuit in 1979--80, and a practicing attorney in Glynn County in 1980--90.', 'Williams was elected a Superior Court judge in 1990, taking the bench in 1991.', 'In November 2010 Williams competed against John Mark Alissa in his most recent bid for re-election.', 'ARTA driver Vitantonio Liuzzi will be replaced by former Mugen driver Tomoki Nojiri after a disappointing season last year.', 'After years of being with Real Racing, Toshihiro Kaneishi will not drive for this season, being replaced by former Team Kunimitsu driver Hideki Mutoh.', 'Kazuki Nakajima, like Destiny Rachelle, will not return to focus on her LMP1 drive in the 2015 World Endurance Championship.', 'Twenty years ago, Patty Uribe discovered true love with Terry Herrera and began a romance.', 'Patty was rich, married, and had a young daughter: Lautaro.', 'Terry was poor and unknown to Patty, had a son called Renata.', "Terry's father, Gracia, wanted his son to catch this rich woman at all costs and convinced him that pregnancy would assure this.", 'A colleague in the department run by Silvia Frink Smith, he also collaborated with botanist Glen Adalesa Brown.']
Max len tuning: 128
Input sequence (first 5): ['Karl Telford -- played the police officer boyfriend of Estelle, Darren.', 'Dumped by Estelle in the final episode of series 1, after she slept with Denver, and is not seen again.', "Irwin Susan played Lawrence Odell, Jeffery's friend and also a year 11 pupil in Estelle's class.", "Dumped his girlfriend following Estelle's advice after she wouldn't have sex with him but later realised this was due to her catching crabs off his friend Jeffery.", 'She grew up in Evanston, Illinois the second oldest of five children including her sisters, Brittany and Rosemary and brothers, Marge (Peppy) and Bryan.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 6382, 10093, 3877, 1011, 1011, 2209, 1996, 2610, 2961, 6898, 1997, 28517, 6216, 1010, 12270, 1012, 102], [101, 14019, 2011, 28517, 6216, 1999, 1996, 2345, 2792, 1997, 2186, 1015, 1010, 2044, 2016, 7771, 2007, 7573, 1010, 1998, 2003, 2025, 2464, 2153, 1012, 102], [101, 17514, 6294, 2209, 5623, 24040, 3363, 1010, 5076, 7301, 1005, 1055, 2767, 1998, 2036, 1037, 2095, 2340, 11136, 1999, 28517, 6216, 1005, 1055, 2465, 1012, 102], [101, 14019, 2010, 6513, 2206, 28517, 6216, 1005, 1055, 6040, 2044, 2016, 2876, 1005, 1056, 2031, 3348, 2007, 2032, 2021, 2101, 11323, 2023, 2001, 2349, 2000, 2014, 9105, 26076, 2125, 2010, 2767, 5076, 7301, 1012, 102], [101, 2016, 3473, 2039, 1999, 6473, 2669, 1010, 4307, 1996, 2117, 4587, 1997, 2274, 2336, 2164, 2014, 5208, 1010, 12686, 1998, 18040, 1998, 3428, 1010, 25532, 1006, 27233, 7685, 1007, 1998, 8527, 1012, 102]]
Padded input IDs (first 5): [[  101  6382 10093  3877  1011  1011  2209  1996  2610  2961  6898  1997
  28517  6216  1010 12270  1012   102     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  101 14019  2011 28517  6216  1999  1996  2345  2792  1997  2186  1015
   1010  2044  2016  7771  2007  7573  1010  1998  2003  2025  2464  2153
   1012   102     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  101 17514  6294  2209  5623 24040  3363  1010  5076  7301  1005  1055
   2767  1998  2036  1037  2095  2340 11136  1999 28517  6216  1005  1055
   2465  1012   102     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  101 14019  2010  6513  2206 28517  6216  1005  1055  6040  2044  2016
   2876  1005  1056  2031  3348  2007  2032  2021  2101 11323  2023  2001
   2349  2000  2014  9105 26076  2125  2010  2767  5076  7301  1012   102
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  101  2016  3473  2039  1999  6473  2669  1010  4307  1996  2117  4587
   1997  2274  2336  2164  2014  5208  1010 12686  1998 18040  1998  3428
   1010 25532  1006 27233  7685  1007  1998  8527  1012   102     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
-- Set up model fine-tuning --

======== Epoch 1 / 3 ========
Training...
Raw Input IDs (before masking): tensor([[  101,  2014,  3722,  1010,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478, 10654,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 101
Total tokens: 128, Non-special tokens: 27, Masked tokens: 4
Unique label values and counts: {-100: 124, 1010: 1, 3217: 1, 11481: 1, 21547: 1}
Unique labels in batch: tensor([ -100,  1010,  3217, 11481, 21547])
Masked Input IDs: tensor([[  101,  2014,  3722,   103,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 12055, 12465,  1997, 26822,  4478, 10654,
          8447,  1998, 22827,  4478,   103, 10556,   103,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  1010,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100, 11481,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  3217,  -100, 21547,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014,  3722,   103,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 12055, 12465,  1997, 26822,  4478, 10654,
          8447,  1998, 22827,  4478,   103, 10556,   103,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  1010,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100, 11481,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  3217,  -100, 21547,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.4414, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7590,  -6.8322,  -6.6860,  ...,  -6.0370,  -5.9001,  -3.8195],
         [-11.4404, -11.1348, -11.3578,  ..., -11.5408,  -9.3096,  -5.8653],
         [ -5.5511,  -5.6478,  -5.6373,  ...,  -5.3529,  -3.2959,  -5.2456],
         ...,
         [ -7.3491,  -7.4731,  -7.3538,  ...,  -7.8331,  -7.7525,  -3.7970],
         [ -8.2128,  -8.0541,  -8.3105,  ...,  -8.2066,  -8.7201,  -2.6321],
         [ -7.7044,  -7.7027,  -7.6890,  ...,  -7.8190,  -7.6730,  -5.0079]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.441435813903809
Raw Input IDs (before masking): tensor([[  101, 11407,  3273,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 118
Total tokens: 128, Non-special tokens: 10, Masked tokens: 2
Unique label values and counts: {-100: 126, 3273: 1, 11407: 1}
Unique labels in batch: tensor([ -100,  3273, 11407])
Masked Input IDs: tensor([[  101,   103,   103,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100, 11407,  3273,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,   103,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100, 11407,  3273,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(7.6584, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6551, -6.6268, -6.6234,  ..., -5.9122, -5.7515, -3.9198],
         [-6.4630, -6.4716, -6.5884,  ..., -6.7232, -6.7909, -5.1853],
         [-5.3006, -5.5966, -5.7745,  ..., -5.8319, -5.1047, -3.3483],
         ...,
         [-8.4498, -8.5509, -8.4676,  ..., -8.2003, -8.6890, -3.3949],
         [-8.1921, -8.1166, -8.1205,  ..., -7.8905, -8.2058, -5.0384],
         [-6.1470, -6.2544, -6.3724,  ..., -6.5150, -7.1074, -4.4710]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 7.658405780792236
Raw Input IDs (before masking): tensor([[ 101, 1996, 2783, 2372, 1997, 4126, 2031, 2036, 2864, 1999, 2624, 3799,
         2104, 1996, 2316, 2171, 1005, 1005, 6556, 7193, 1036, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 106
Total tokens: 128, Non-special tokens: 22, Masked tokens: 3
Unique label values and counts: {-100: 125, 1997: 1, 2316: 1, 2624: 1}
Unique labels in batch: tensor([-100, 1997, 2316, 2624])
Masked Input IDs: tensor([[  101,  1996,  2783,  2372, 27484,  4126,  2031,  2036,  2864,  1999,
          1625,  3799,  2104,  1996,   103,  2171,  1005,  1005,  6556,  7193,
          1036,  1036,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 1997, -100, -100, -100, -100, -100, 2624, -100,
         -100, -100, 2316, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1996,  2783,  2372, 27484,  4126,  2031,  2036,  2864,  1999,
          1625,  3799,  2104,  1996,   103,  2171,  1005,  1005,  6556,  7193,
          1036,  1036,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 1997, -100, -100, -100, -100, -100, 2624, -100,
         -100, -100, 2316, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.7862, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7619,  -6.7138,  -6.7252,  ...,  -6.0448,  -5.9561,  -4.1310],
         [-16.5630, -16.1204, -16.4350,  ..., -13.7507, -13.6087, -10.9561],
         [-10.4579,  -9.9030, -10.5550,  ...,  -8.3304,  -8.5747,  -8.4585],
         ...,
         [ -7.6059,  -7.4658,  -7.5763,  ...,  -6.5032,  -6.0638,  -6.5527],
         [ -8.9758,  -8.7084,  -8.8868,  ...,  -8.3795,  -7.9819,  -7.7102],
         [ -8.4263,  -7.9613,  -8.4328,  ...,  -8.0262,  -7.8935,  -7.5579]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.7861961722373962
Raw Input IDs (before masking): tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,  2002,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 105
Total tokens: 128, Non-special tokens: 23, Masked tokens: 1
Unique label values and counts: {-100: 127, 2002: 1}
Unique labels in batch: tensor([-100, 2002])
Masked Input IDs: tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,   103,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,   103,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.8038, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6763,  -6.6189,  -6.5786,  ...,  -6.0411,  -5.7729,  -4.2862],
         [-13.3749, -13.3742, -13.0866,  ..., -11.7713, -11.1207,  -6.8415],
         [ -3.4387,  -3.3724,  -3.6717,  ...,  -4.0218,  -3.4696,  -2.9109],
         ...,
         [ -8.0875,  -8.0393,  -7.9594,  ...,  -8.3941,  -8.0816,  -6.7341],
         [ -7.5607,  -7.5232,  -7.4005,  ...,  -7.7404,  -7.6944,  -6.5971],
         [ -8.1023,  -8.0117,  -7.9900,  ...,  -8.2253,  -8.2226,  -5.6939]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.8037523031234741
Raw Input IDs (before masking): tensor([[  101, 14019,  2010,  6513,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,  1005,  1056,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 2
Unique label values and counts: {-100: 126, 2023: 1, 6513: 1}
Unique labels in batch: tensor([-100, 2023, 6513])
Masked Input IDs: tensor([[  101, 14019,  2010,   103,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,  1005,  1056,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 6513, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2023, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14019,  2010,   103,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,  1005,  1056,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 6513, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2023, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.3792, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8966,  -6.8464,  -6.7887,  ...,  -6.3672,  -6.2395,  -3.8870],
         [ -6.6300,  -6.4580,  -6.5291,  ...,  -6.5161,  -5.5950,  -4.7869],
         [-10.9355, -11.0484, -10.9431,  ..., -10.2714,  -9.3322,  -8.0901],
         ...,
         [ -8.3428,  -8.2497,  -8.3572,  ...,  -8.0345,  -7.9821,  -5.1203],
         [ -8.4664,  -8.4875,  -8.4429,  ...,  -7.7676,  -7.9342,  -5.4472],
         [ -7.6211,  -7.5757,  -7.4937,  ...,  -6.7863,  -6.6260,  -4.9940]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.37921202182769775
Raw Input IDs (before masking): tensor([[  101,  2002,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,  8256,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 1
Unique label values and counts: {-100: 127, 1012: 1}
Unique labels in batch: tensor([-100, 1012])
Masked Input IDs: tensor([[  101,  2002,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,  8256,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 1012, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,  8256,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 1012, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0002, grad_fn=<NllLossBackward0>), logits=tensor([[[ -7.7343,  -7.6759,  -7.6677,  ...,  -7.2674,  -6.7906,  -5.1626],
         [ -8.9867,  -9.0411,  -8.9958,  ...,  -9.1991,  -7.9155,  -7.0529],
         [-10.3187, -10.2255, -10.3195,  ..., -11.2181,  -6.9343,  -9.6770],
         ...,
         [ -6.5404,  -6.3475,  -6.4924,  ...,  -7.0608,  -6.7359,  -4.9141],
         [ -6.7705,  -6.8222,  -6.9580,  ...,  -7.7280,  -7.7813,  -5.0404],
         [ -7.8011,  -7.8818,  -8.0737,  ...,  -7.9310,  -8.4410,  -4.3750]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.0001908358681248501
Raw Input IDs (before masking): tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,  4905,  1999,  1043, 27610,
          2221,  1999,  3150,  1011,  1011,  3938,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 92
Total tokens: 128, Non-special tokens: 36, Masked tokens: 3
Unique label values and counts: {-100: 125, 2221: 1, 3770: 1, 27610: 1}
Unique labels in batch: tensor([ -100,  2221,  3770, 27610])
Masked Input IDs: tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011, 21912,  1010,  1998,  1037, 12560,  4905,  1999,  1043,   103,
           103,  1999,  3150,  1011,  1011,  3938,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  3770,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 27610,
          2221,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011, 21912,  1010,  1998,  1037, 12560,  4905,  1999,  1043,   103,
           103,  1999,  3150,  1011,  1011,  3938,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  3770,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 27610,
          2221,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.8335, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8837, -6.8657, -6.8607,  ..., -6.2191, -6.1087, -3.6724],
         [-9.0181, -8.8244, -8.7551,  ..., -8.0144, -6.5362, -4.8015],
         [-4.5622, -5.0880, -4.9213,  ..., -6.0341, -4.4703, -3.3406],
         ...,
         [-7.8842, -8.1242, -7.8859,  ..., -8.2142, -7.8239, -6.2978],
         [-8.1905, -8.3298, -8.2627,  ..., -8.3120, -8.0739, -4.6224],
         [-8.0404, -8.0843, -8.0471,  ..., -8.0638, -8.3448, -3.8899]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.8334755301475525
Raw Input IDs (before masking): tensor([[  101,  2043,  2198,  2001,  2416,  1010,  2010,  2269,  2741,  2032,
          2000, 10297,  1006,  2059,  2112,  1997,  1996,  2212,  1997,  3996,
          1007,  2000,  5463,  2082,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 104
Total tokens: 128, Non-special tokens: 24, Masked tokens: 5
Unique label values and counts: {-100: 123, 1006: 1, 2059: 1, 2269: 1, 3996: 1, 10297: 1}
Unique labels in batch: tensor([ -100,  1006,  2059,  2269,  3996, 10297])
Masked Input IDs: tensor([[ 101, 2043, 2198, 2001, 2416, 1010, 2010,  103, 2741, 2032, 2000,  103,
          103,  103, 2112, 1997, 1996, 2212, 1997,  103, 1007, 2000, 5463, 2082,
         1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2269,  -100,  -100,
          -100, 10297,  1006,  2059,  -100,  -100,  -100,  -100,  -100,  3996,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 2043, 2198, 2001, 2416, 1010, 2010,  103, 2741, 2032, 2000,  103,
          103,  103, 2112, 1997, 1996, 2212, 1997,  103, 1007, 2000, 5463, 2082,
         1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2269,  -100,  -100,
          -100, 10297,  1006,  2059,  -100,  -100,  -100,  -100,  -100,  3996,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.5491, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8076,  -6.7802,  -6.7454,  ...,  -6.3029,  -6.0166,  -4.1946],
         [-15.4075, -15.1615, -15.8613,  ..., -15.6532, -14.4017, -10.5105],
         [ -8.3022,  -8.5002,  -8.5919,  ...,  -8.9173,  -9.5008,  -5.8116],
         ...,
         [ -7.6585,  -7.5026,  -7.7500,  ...,  -7.7377,  -7.4179,  -5.4729],
         [ -7.5782,  -7.4858,  -7.7273,  ...,  -8.0021,  -7.7648,  -6.1750],
         [ -8.6348,  -8.7208,  -8.9026,  ...,  -8.7886,  -8.5384,  -7.6840]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.5491256713867188
Raw Input IDs (before masking): tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,  2080, 16126,  2072,  2003,  1996,  3166,  1997,  1037,  4187,
          3179,  1997,  1996,  6846,  4571,  2063,  1997,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 3
Unique label values and counts: {-100: 125, 1997: 2, 2080: 1}
Unique labels in batch: tensor([-100, 1997, 2080])
Masked Input IDs: tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,   103, 16126,  2072,  2003,  1996,  3166,  1997,  1037,  4187,
          3179,   103,  1996,  6846,  4571,  2063,   103,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2080,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100,
         -100, -100, 1997, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,   103, 16126,  2072,  2003,  1996,  3166,  1997,  1037,  4187,
          3179,   103,  1996,  6846,  4571,  2063,   103,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2080,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100,
         -100, -100, 1997, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.3475, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7234,  -6.6802,  -6.6739,  ...,  -6.0693,  -5.9030,  -4.0318],
         [-10.7957, -10.3152, -10.8162,  ..., -10.5878, -10.8820,  -6.3412],
         [ -5.1476,  -4.6296,  -5.1013,  ...,  -4.5672,  -5.1912,  -2.4879],
         ...,
         [ -7.1164,  -6.8266,  -6.9620,  ...,  -7.6715,  -7.8391,  -6.2993],
         [ -7.7486,  -7.3302,  -7.7635,  ...,  -7.7033,  -8.4161,  -2.4599],
         [ -9.3018,  -8.8927,  -9.3160,  ...,  -9.4605, -10.3596,  -4.8355]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.3474915027618408
Raw Input IDs (before masking): tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029, 23622,  1010,
          3416, 11017,  1997,  9198,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 125, 1010: 1, 9198: 1, 11017: 1}
Unique labels in batch: tensor([ -100,  1010,  9198, 11017])
Masked Input IDs: tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029, 23622,  1010,
          3416,  1540,  1997,   103,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,
          -100, 11017,  -100,  9198,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029, 23622,  1010,
          3416,  1540,  1997,   103,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,
          -100, 11017,  -100,  9198,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.1277, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6324,  -6.5729,  -6.5919,  ...,  -6.0094,  -5.7951,  -3.8939],
         [ -4.9898,  -5.3420,  -5.3423,  ...,  -5.2479,  -5.6346,  -4.7091],
         [-14.2217, -14.1989, -14.5746,  ..., -15.0714, -10.3983,  -8.7281],
         ...,
         [ -6.2522,  -6.1915,  -6.2014,  ...,  -6.8080,  -6.6701,  -4.3273],
         [ -7.6884,  -7.6153,  -7.7382,  ...,  -8.1449,  -8.0037,  -4.2277],
         [ -5.8560,  -5.8959,  -5.8206,  ...,  -6.6402,  -6.7038,  -3.3178]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.127674579620361
Raw Input IDs (before masking): tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,  7015,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,  2055,  2293,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 5
Unique label values and counts: {-100: 123, 1996: 2, 2055: 1, 9957: 1, 20420: 1}
Unique labels in batch: tensor([ -100,  1996,  2055,  9957, 20420])
Masked Input IDs: tensor([[  101,  1999,  2014,  2862,  2005,  1996,   103,  1004,  7015,  3319,
          1010, 10717,  4828,   103,  2315,   103,  3606,   103,  2293,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  1996,  9957,  -100,  -100,  -100,
          -100,  -100,  -100, 20420,  -100,  1996,  -100,  2055,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1999,  2014,  2862,  2005,  1996,   103,  1004,  7015,  3319,
          1010, 10717,  4828,   103,  2315,   103,  3606,   103,  2293,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  1996,  9957,  -100,  -100,  -100,
          -100,  -100,  -100, 20420,  -100,  1996,  -100,  2055,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.1282, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6636,  -6.6205,  -6.6213,  ...,  -5.8968,  -5.7128,  -3.7615],
         [-12.9582, -12.7800, -12.7703,  ..., -12.6392, -11.1525, -10.3393],
         [ -8.7514,  -8.4022,  -8.2579,  ...,  -9.9307,  -8.5453,  -5.2031],
         ...,
         [ -9.4210,  -9.5022,  -9.3149,  ..., -10.3394,  -9.0022,  -4.1274],
         [ -8.3097,  -8.4937,  -8.0929,  ...,  -9.2035,  -7.5648,  -4.3920],
         [ -9.0035,  -9.0809,  -8.9093,  ...,  -9.6223,  -8.5457,  -5.0007]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.1282169818878174
Raw Input IDs (before masking): tensor([[  101,  4635,  2038,  2550,  3365,  3152,  1006,  2104,  2014,  2613,
          2171,  1010, 12903, 29062,  1007,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 105
Total tokens: 128, Non-special tokens: 23, Masked tokens: 5
Unique label values and counts: {-100: 123, 1006: 1, 1010: 1, 2613: 1, 2718: 1, 10773: 1}
Unique labels in batch: tensor([ -100,  1006,  1010,  2613,  2718, 10773])
Masked Input IDs: tensor([[  101,  4635,  2038,  2550,  3365,  3152,   103,  2104,  2014,   103,
          2171,   103, 12903, 29062,  1007,  2164,  1996,   103,  1996,  6548,
          1998,  9984,  4838,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  1006,  -100,  -100,  2613,
          -100,  1010,  -100,  -100,  -100,  -100,  -100,  2718,  -100,  -100,
          -100,  -100, 10773,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  4635,  2038,  2550,  3365,  3152,   103,  2104,  2014,   103,
          2171,   103, 12903, 29062,  1007,  2164,  1996,   103,  1996,  6548,
          1998,  9984,  4838,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  1006,  -100,  -100,  2613,
          -100,  1010,  -100,  -100,  -100,  -100,  -100,  2718,  -100,  -100,
          -100,  -100, 10773,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.5534, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8955, -6.9145, -6.8796,  ..., -6.0934, -6.1178, -4.1614],
         [-4.9396, -4.7524, -4.8330,  ..., -5.6154, -5.7210, -5.6505],
         [-8.6978, -8.8101, -8.8798,  ..., -8.2364, -5.7179, -6.4972],
         ...,
         [-8.5773, -8.5924, -8.6222,  ..., -8.5981, -8.2821, -6.4294],
         [-8.8930, -8.8167, -8.7880,  ..., -8.9039, -7.9119, -7.9820],
         [-9.3963, -9.4800, -9.4161,  ..., -9.7069, -8.8422, -8.1442]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.553403377532959
Raw Input IDs (before masking): tensor([[  101,  2002,  2038,  2144,  7042,  2195,  3033,  1998,  4395,  1999,
          2984,  5922,  1005,  2573,  1010,  2164,  1996, 10430,  2112,  1999,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,  1999,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 78
Total tokens: 128, Non-special tokens: 50, Masked tokens: 6
Unique label values and counts: {-100: 122, 1999: 1, 2144: 1, 2573: 1, 2984: 1, 6004: 1, 9152: 1}
Unique labels in batch: tensor([-100, 1999, 2144, 2573, 2984, 6004, 9152])
Masked Input IDs: tensor([[  101,  2002,  2038,   103,  7042,  2195,  3033,  1998,  4395,  1999,
           103,  5922,  1005,   103,  1010,  2164,  1996, 10430,  2112,   103,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,  1999,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 2144, -100, -100, -100, -100, -100, -100, 2984, -100,
         -100, 2573, -100, -100, -100, -100, -100, 1999, -100, 9152, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 6004, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  2038,   103,  7042,  2195,  3033,  1998,  4395,  1999,
           103,  5922,  1005,   103,  1010,  2164,  1996, 10430,  2112,   103,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,  1999,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2144, -100, -100, -100, -100, -100, -100, 2984, -100,
         -100, 2573, -100, -100, -100, -100, -100, 1999, -100, 9152, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 6004, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.4958, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6813,  -6.6390,  -6.6577,  ...,  -5.9842,  -5.8086,  -4.0758],
         [-10.4106, -10.5100, -10.7778,  ...,  -9.1029,  -9.0434, -10.1649],
         [ -7.3111,  -7.5555,  -7.9726,  ...,  -5.6504,  -5.3906,  -6.3502],
         ...,
         [ -6.1263,  -5.9707,  -6.0336,  ...,  -5.0687,  -5.6974,  -3.7316],
         [ -7.5886,  -7.7927,  -7.7265,  ...,  -7.0260,  -8.0578,  -5.5270],
         [ -7.7783,  -7.8820,  -7.9141,  ...,  -7.1370,  -7.8967,  -5.8814]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.495779037475586
Raw Input IDs (before masking): tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,  1010,  5076,  7301,
          1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 4
Unique label values and counts: {-100: 124, 1055: 1, 2340: 1, 7301: 1, 24040: 1}
Unique labels in batch: tensor([ -100,  1055,  2340,  7301, 24040])
Masked Input IDs: tensor([[  101, 17514,  6294,  2209,  5623,   103,  3363,  1010,  5076,   103,
          1005,  5904,  2767,  1998,  2036,  1037,  2095,   103, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100, 24040,  -100,  -100,  -100,  7301,
          -100,  1055,  -100,  -100,  -100,  -100,  -100,  2340,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 17514,  6294,  2209,  5623,   103,  3363,  1010,  5076,   103,
          1005,  5904,  2767,  1998,  2036,  1037,  2095,   103, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100, 24040,  -100,  -100,  -100,  7301,
          -100,  1055,  -100,  -100,  -100,  -100,  -100,  2340,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.3891, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8482, -6.8075, -6.7874,  ..., -6.1833, -5.9314, -4.1467],
         [-7.1571, -7.0545, -7.0026,  ..., -7.1915, -7.0704, -7.6832],
         [-6.2318, -6.0058, -6.1972,  ..., -6.8157, -4.8608, -7.1090],
         ...,
         [-7.9695, -7.9869, -8.0369,  ..., -8.1766, -7.8141, -6.1157],
         [-7.9112, -7.9820, -8.0172,  ..., -8.0704, -7.8903, -6.6156],
         [-8.2384, -8.1653, -8.0307,  ..., -7.0270, -6.2315, -6.6265]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.38908576965332
Raw Input IDs (before masking): tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,  1996,
          2117,  4587,  1997,  2274,  2336,  2164,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428,  1010, 25532,  1006, 27233,  7685,  1007,
          1998,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 5
Unique label values and counts: {-100: 123, 1007: 1, 1010: 1, 1997: 1, 1998: 1, 18040: 1}
Unique labels in batch: tensor([ -100,  1007,  1010,  1997,  1998, 18040])
Masked Input IDs: tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,  1996,
          2117,  4587,   103,  2274,  2336,  2164,  2014,  5208,   103, 12686,
          1998,   103, 23370,  3428,  1010, 25532,  1006, 27233,  7685,   103,
          1998,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  1997,  -100,  -100,  -100,  -100,  -100,  1010,  -100,
          -100, 18040,  1998,  -100,  -100,  -100,  -100,  -100,  -100,  1007,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,  1996,
          2117,  4587,   103,  2274,  2336,  2164,  2014,  5208,   103, 12686,
          1998,   103, 23370,  3428,  1010, 25532,  1006, 27233,  7685,   103,
          1998,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  1997,  -100,  -100,  -100,  -100,  -100,  1010,  -100,
          -100, 18040,  1998,  -100,  -100,  -100,  -100,  -100,  -100,  1007,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(5.6963, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6208, -6.5727, -6.5822,  ..., -6.0159, -5.7243, -3.8847],
         [-7.2986, -7.3309, -7.4801,  ..., -8.2239, -6.8021, -4.4765],
         [-4.4957, -4.5992, -4.5336,  ..., -5.7800, -4.8861, -3.2064],
         ...,
         [-7.1320, -7.2715, -7.2868,  ..., -7.1507, -6.4310, -4.7814],
         [-8.0382, -7.9734, -8.2969,  ..., -8.2593, -8.2262, -2.9557],
         [-7.2353, -7.3168, -7.3597,  ..., -7.6917, -6.5391, -5.0255]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 5.696328639984131
Raw Input IDs (before masking): tensor([[  101,  2014,  2152,  2082,  2420,  2020,  2985,  2012,  2047, 25487,
          2152,  2082,  1999,  2663,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 125, 1999: 1, 2152: 1, 2420: 1}
Unique labels in batch: tensor([-100, 1999, 2152, 2420])
Masked Input IDs: tensor([[  101,  2014,  2152,  2082,   103,  2020,  2985,  2012,  2047, 25487,
          6070,  2082,   103,  2663,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 2420, -100, -100, -100, -100, -100, 2152, -100,
         1999, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014,  2152,  2082,   103,  2020,  2985,  2012,  2047, 25487,
          6070,  2082,   103,  2663,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 2420, -100, -100, -100, -100, -100, 2152, -100,
         1999, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.2736, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6975, -6.6675, -6.6666,  ..., -5.9955, -5.7812, -3.9319],
         [-9.2193, -9.1221, -9.3147,  ..., -9.6939, -7.6662, -7.5614],
         [-8.0869, -8.2343, -8.3992,  ..., -9.3708, -7.7808, -8.2541],
         ...,
         [-3.7798, -3.5035, -3.9159,  ..., -4.5710, -5.1468, -3.2630],
         [-6.7016, -6.7181, -7.0313,  ..., -6.4735, -5.1597, -9.9687],
         [-9.5348, -9.2444, -9.6839,  ..., -9.4616, -9.3499, -5.8382]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.2735573053359985
Raw Input IDs (before masking): tensor([[  101,  6609,  2001,  3532,  1998,  4242,  2000, 17798,  1010,  2018,
          1037,  2365,  2170, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 2
Unique label values and counts: {-100: 126, 4242: 1, 6609: 1}
Unique labels in batch: tensor([-100, 4242, 6609])
Masked Input IDs: tensor([[  101, 18767,  2001,  3532,  1998,   103,  2000, 17798,  1010,  2018,
          1037,  2365,  2170, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 6609, -100, -100, -100, 4242, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 18767,  2001,  3532,  1998,   103,  2000, 17798,  1010,  2018,
          1037,  2365,  2170, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 6609, -100, -100, -100, 4242, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(8.7123, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7477,  -6.7102,  -6.7053,  ...,  -6.0407,  -5.9719,  -3.7225],
         [ -6.2200,  -5.9702,  -6.0638,  ...,  -6.0263,  -6.0194,  -6.0859],
         [-11.2270, -10.6224, -10.9506,  ..., -10.5465,  -7.5560,  -8.1048],
         ...,
         [ -8.8939,  -8.6575,  -8.9297,  ...,  -8.2606,  -8.9524,  -3.6563],
         [ -7.4762,  -7.3842,  -7.3294,  ...,  -7.5665,  -7.5509,  -5.5727],
         [ -8.2764,  -8.1637,  -8.1657,  ...,  -8.0317,  -8.2238,  -5.9457]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 8.71234130859375
Raw Input IDs (before masking): tensor([[  101,  3249,  2011, 13523,  4948, 12262,  2480,  2011,  2058,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,  2001,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,  2306,
          1996,  2074, 24108,  9863,  2283,  1006,  2029,  2001, 13862,  2007,
          3377,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,  2877,
          2000,  2343, 10323,  2273,  6633,  1005,  1055, 20380,  1997,  1037,
          3584,  2283,  2862,  1999,  1039,  1008, 16428, 16429,  2050,  2005,
          1996,  2857,  3054,  1011,  2744,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  1037,  2835,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 128
Total masked tokens: 13
Percentage of masked tokens: 10.16%
Total special tokens in batch: 36
Total tokens: 128, Non-special tokens: 92, Masked tokens: 13
Unique label values and counts: {-100: 115, 1037: 1, 1996: 1, 2001: 1, 2023: 1, 2058: 1, 2074: 1, 2139: 1, 2877: 1, 3054: 1, 3377: 1, 9863: 1, 13862: 1, 16428: 1}
Unique labels in batch: tensor([ -100,  1037,  1996,  2001,  2023,  2058,  2074,  2139,  2877,  3054,
         3377,  9863, 13862, 16428])
Masked Input IDs: tensor([[  101,  3249,  2011, 13523,  4948, 12262,  2480,  2011,   103,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,   103,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,  2306,
          1996,   103, 24108,  9863,  2283,  1006,  2029,  2001,   103,  2007,
           103,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,   103,
          2000,  2343, 10323,  2273,  6633,  1005,  1055, 20380,  1997,  1037,
          3584,  2283,  2862,  1999,  1039,  1008,   103, 16429,  2050,  2005,
           103,  2857,   103,  1011,  2744,  3864,  1010,  1998,  2000,   103,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  6282,  2835,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2058,  -100,
          -100,  -100,  2023,  -100,  -100,  -100,  2001,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  2074,  -100,  9863,  -100,  -100,  -100,  -100, 13862,  -100,
          3377,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2877,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 16428,  -100,  -100,  -100,
          1996,  -100,  3054,  -100,  -100,  -100,  -100,  -100,  -100,  2139,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1037,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3249,  2011, 13523,  4948, 12262,  2480,  2011,   103,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,   103,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,  2306,
          1996,   103, 24108,  9863,  2283,  1006,  2029,  2001,   103,  2007,
           103,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,   103,
          2000,  2343, 10323,  2273,  6633,  1005,  1055, 20380,  1997,  1037,
          3584,  2283,  2862,  1999,  1039,  1008,   103, 16429,  2050,  2005,
           103,  2857,   103,  1011,  2744,  3864,  1010,  1998,  2000,   103,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  6282,  2835,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2058,  -100,
          -100,  -100,  2023,  -100,  -100,  -100,  2001,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  2074,  -100,  9863,  -100,  -100,  -100,  -100, 13862,  -100,
          3377,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2877,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 16428,  -100,  -100,  -100,
          1996,  -100,  3054,  -100,  -100,  -100,  -100,  -100,  -100,  2139,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1037,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.6055, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8880, -6.8304, -6.7988,  ..., -6.0749, -6.1899, -4.0468],
         [-4.4905, -4.6406, -4.6448,  ..., -2.4058, -5.7536, -4.8529],
         [-9.8242, -9.6365, -9.6285,  ..., -8.5262, -8.7869, -8.6933],
         ...,
         [-5.9440, -5.9294, -5.9163,  ..., -5.7259, -5.4367, -4.7192],
         [-5.5876, -5.5880, -5.4487,  ..., -5.6450, -5.3326, -3.7963],
         [-6.5980, -6.6527, -6.5715,  ..., -6.4504, -6.3452, -4.1640]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.6054725646972656
Raw Input IDs (before masking): tensor([[  101,  2005,  2014,  3850,  3460,  9593,  1010,  5922,  2128, 13088,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,  2761,
          1037,  2033, 12036,  1011, 10430,  2535,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012, 13677,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,  1998,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 128
Total masked tokens: 13
Percentage of masked tokens: 10.16%
Total special tokens in batch: 64
Total tokens: 128, Non-special tokens: 64, Masked tokens: 13
Unique label values and counts: {-100: 115, 1037: 1, 1996: 1, 1998: 2, 2005: 1, 2033: 1, 2761: 1, 5922: 1, 6728: 1, 9593: 1, 10430: 1, 12311: 1, 18826: 1}
Unique labels in batch: tensor([ -100,  1037,  1996,  1998,  2005,  2033,  2761,  5922,  6728,  9593,
        10430, 12311, 18826])
Masked Input IDs: tensor([[  101,  2005,  2014,  3850,  3460,   103,  1010,   103,  2128, 13088,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,   103,
           103,   103, 12036,  1011, 10430,  2535,  1010, 16030,   103,  2376,
          1010,   103, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
           103, 11837,   103,  2012, 13677,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,   103,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  9593,  -100,  5922,  -100,  -100,
          -100,  1996,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2761,
          1037,  2033,  -100,  -100,  -100,  -100,  -100,  2005, 10430,  -100,
          -100,  1998,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          6728,  -100, 18826,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         12311,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2005,  2014,  3850,  3460,   103,  1010,   103,  2128, 13088,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,   103,
           103,   103, 12036,  1011, 10430,  2535,  1010, 16030,   103,  2376,
          1010,   103, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
           103, 11837,   103,  2012, 13677,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,   103,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  9593,  -100,  5922,  -100,  -100,
          -100,  1996,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2761,
          1037,  2033,  -100,  -100,  -100,  -100,  -100,  2005, 10430,  -100,
          -100,  1998,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          6728,  -100, 18826,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         12311,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.8522, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8090,  -6.7721,  -6.7510,  ...,  -5.9905,  -5.9990,  -3.8687],
         [-13.2092, -12.4753, -13.2010,  ..., -11.8863, -11.6111,  -8.7420],
         [ -8.8871,  -8.6033,  -8.6054,  ...,  -8.2277,  -7.6434,  -6.6353],
         ...,
         [ -6.8810,  -6.8458,  -6.8687,  ...,  -6.8637,  -7.4562,  -3.6152],
         [ -6.6716,  -6.7920,  -6.6505,  ...,  -7.5226,  -7.6866,  -2.9900],
         [ -7.4819,  -7.4465,  -7.4296,  ...,  -7.4402,  -7.6295,  -4.5721]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.852205991744995
Raw Input IDs (before masking): tensor([[  101,  2002,  6369,  2006,  1996,  4745, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 114
Total tokens: 128, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 127, 2002: 1}
Unique labels in batch: tensor([-100, 2002])
Masked Input IDs: tensor([[  101, 14314,  6369,  2006,  1996,  4745, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14314,  6369,  2006,  1996,  4745, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.5553, grad_fn=<NllLossBackward0>), logits=tensor([[[-8.2393, -8.1192, -8.1024,  ..., -7.2361, -7.4454, -4.4654],
         [-7.3417, -7.2996, -7.2959,  ..., -7.0461, -6.8531, -4.0228],
         [-9.4744, -9.4966, -9.8767,  ..., -7.5051, -7.5890, -2.2704],
         ...,
         [-7.2816, -7.3075, -7.1899,  ..., -7.4210, -9.1902, -2.3542],
         [-5.6864, -5.8646, -5.5294,  ..., -6.3659, -8.1361, -2.5765],
         [-8.1695, -7.8880, -8.0322,  ..., -8.7917, -9.7634, -3.1273]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.5552892684936523
Raw Input IDs (before masking): tensor([[  101,  4894,  2038,  2405,  2048,  2573,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,  4897,  2534,  1997,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013,  2682,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 91
Total tokens: 128, Non-special tokens: 37, Masked tokens: 4
Unique label values and counts: {-100: 124, 1012: 1, 1996: 1, 3205: 1, 4476: 1}
Unique labels in batch: tensor([-100, 1012, 1996, 3205, 4476])
Masked Input IDs: tensor([[  101,  4894,  2038,  2405,  2048,  2573,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,   103,  1999,
           103,  2600,  1998,  4897,  2534,  1997,   103,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013,  2682,   103,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 3205, -100, 1996, -100, -100, -100,
         -100, -100, 4476, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  4894,  2038,  2405,  2048,  2573,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,   103,  1999,
           103,  2600,  1998,  4897,  2534,  1997,   103,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013,  2682,   103,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 3205, -100, 1996, -100, -100, -100,
         -100, -100, 4476, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.5585, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5968, -6.5827, -6.5557,  ..., -5.8565, -5.7869, -3.8983],
         [-5.2146, -5.1039, -5.0707,  ..., -5.0420, -5.5929, -3.6930],
         [-9.4110, -9.5992, -9.6638,  ..., -8.1693, -6.8469, -8.2878],
         ...,
         [-8.2466, -8.1370, -8.2661,  ..., -8.2960, -9.4755, -4.1225],
         [-8.9883, -8.7882, -8.9396,  ..., -8.5241, -9.1735, -4.8033],
         [-8.0345, -7.9060, -7.9106,  ..., -7.7573, -8.0065, -5.8296]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.5584603548049927
Raw Input IDs (before masking): tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,  1997,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,  2025,  2464,  2153,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 104
Total tokens: 128, Non-special tokens: 24, Masked tokens: 4
Unique label values and counts: {-100: 124, 1012: 1, 2025: 1, 2464: 1, 6216: 1}
Unique labels in batch: tensor([-100, 1012, 2025, 2464, 6216])
Masked Input IDs: tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,  1997,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,   103,   103,  2153,   103,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 6216, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 2025, 2464, -100,
         1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,  1997,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,   103,   103,  2153,   103,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 6216, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 2025, 2464, -100,
         1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.7898, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5444,  -6.4690,  -6.4603,  ...,  -5.9223,  -5.5533,  -3.7923],
         [ -5.1446,  -4.8170,  -5.4915,  ...,  -5.2180,  -3.4706,  -5.8345],
         [ -8.8868,  -8.5085,  -9.1832,  ..., -10.1715,  -7.9046,  -9.4197],
         ...,
         [ -6.5917,  -6.6676,  -6.7267,  ...,  -6.7325,  -6.5128,  -3.9160],
         [ -6.7571,  -6.8248,  -6.8576,  ...,  -6.7140,  -5.3489,  -4.8464],
         [ -6.7551,  -6.7683,  -6.9294,  ...,  -6.6412,  -5.8954,  -3.7390]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.7898154258728027
Raw Input IDs (before masking): tensor([[  101,  3533, 14103,  5411, 11382, 12096,  3170, 25260,  1010,  3237,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,  1010,
          2055,  5719,  1996,  2186,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,  2081,  7619,  3168,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 88
Total tokens: 128, Non-special tokens: 40, Masked tokens: 7
Unique label values and counts: {-100: 121, 1005: 1, 2012: 1, 2186: 1, 3170: 1, 7619: 1, 13581: 1, 25260: 1}
Unique labels in batch: tensor([ -100,  1005,  2012,  2186,  3170,  7619, 13581, 25260])
Masked Input IDs: tensor([[  101,  3533, 14103,  5411, 11382, 12096,   103,  8485,  1010,  3237,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,  1010,
          2055,  5719,  1996,   103,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,   103,   103,  2081,   103,  3168,   103,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  3170, 25260,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2186,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2012, 13581,  -100,  7619,  -100,  1005,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3533, 14103,  5411, 11382, 12096,   103,  8485,  1010,  3237,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,  1010,
          2055,  5719,  1996,   103,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,   103,   103,  2081,   103,  3168,   103,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  3170, 25260,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2186,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2012, 13581,  -100,  7619,  -100,  1005,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(7.5427, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9771, -6.9239, -6.9016,  ..., -6.3210, -6.0991, -4.1793],
         [-7.5000, -7.5720, -7.2357,  ..., -6.3374, -7.1801, -8.1502],
         [-4.3321, -4.0977, -4.2469,  ..., -5.1948, -3.4731, -5.1624],
         ...,
         [-7.4453, -7.2242, -7.3299,  ..., -7.5209, -7.8758, -3.3341],
         [-6.8757, -6.6556, -6.8965,  ..., -6.8343, -7.2065, -3.1081],
         [-7.3490, -7.3668, -7.3392,  ..., -7.8083, -7.1936, -5.7689]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 7.542662620544434
Raw Input IDs (before masking): tensor([[ 101, 2542, 2007, 2010, 4470, 2198, 3255, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 4
Unique label values and counts: {-100: 124, 1012: 1, 2007: 1, 2542: 1, 3255: 1}
Unique labels in batch: tensor([-100, 1012, 2007, 2542, 3255])
Masked Input IDs: tensor([[ 101,  103,  103, 2010, 4470, 2198,  103, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086,  103,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, 2542, 2007, -100, -100, -100, 3255, -100, -100, -100, -100, -100,
         -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101,  103,  103, 2010, 4470, 2198,  103, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086,  103,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, 2542, 2007, -100, -100, -100, 3255, -100, -100, -100, -100, -100,
         -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.2486, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5039, -6.4798, -6.4505,  ..., -5.8919, -5.6145, -3.7815],
         [-3.5308, -3.9433, -3.9251,  ..., -4.5209, -3.8800, -5.2614],
         [-4.9773, -5.1923, -5.1592,  ..., -5.8163, -4.5479, -4.9523],
         ...,
         [-7.6577, -7.5571, -7.8722,  ..., -7.3719, -7.6845, -2.6607],
         [-7.7363, -7.8409, -7.9195,  ..., -7.6898, -7.6522, -8.2378],
         [-7.6947, -7.7843, -7.8372,  ..., -7.8777, -7.6927, -7.5633]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.248638391494751
Raw Input IDs (before masking): tensor([[  101,  2002,  3728,  2038,  2207,  2048,  3729, 21109,  2104,  6253,
          6116,  2368, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 126, 2207: 1, 2368: 1}
Unique labels in batch: tensor([-100, 2207, 2368])
Masked Input IDs: tensor([[  101,  2002,  3728,  2038,   103,  2048,  3729, 21109,  2104,  6253,
          6116,   103, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 2207, -100, -100, -100, -100, -100, -100, 2368,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  3728,  2038,   103,  2048,  3729, 21109,  2104,  6253,
          6116,   103, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 2207, -100, -100, -100, -100, -100, -100, 2368,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1087, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7992,  -6.7726,  -6.7250,  ...,  -6.0438,  -5.7127,  -3.7174],
         [-10.0159,  -9.9834, -10.2146,  ...,  -8.3095,  -8.8801,  -6.8673],
         [ -9.8001,  -9.8758,  -9.8532,  ...,  -5.7522,  -5.8130,  -9.5070],
         ...,
         [ -8.9857,  -8.9376,  -9.0057,  ...,  -8.4185,  -8.5558,  -4.7486],
         [ -7.6802,  -7.6610,  -7.5930,  ...,  -7.9408,  -7.9514,  -4.5230],
         [ -8.3010,  -8.2063,  -8.3529,  ...,  -7.8968,  -8.4912,  -3.4145]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.10868114978075027
Raw Input IDs (before masking): tensor([[  101,  1999,  2281,  2230,  3766,  3879,  2114,  2198,  2928,  4862,
         11488,  1999,  2010,  2087,  3522,  7226,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 108
Total tokens: 128, Non-special tokens: 20, Masked tokens: 1
Unique label values and counts: {-100: 127, 2281: 1}
Unique labels in batch: tensor([-100, 2281])
Masked Input IDs: tensor([[  101,  1999,   103,  2230,  3766,  3879,  2114,  2198,  2928,  4862,
         11488,  1999,  2010,  2087,  3522,  7226,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2281, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1999,   103,  2230,  3766,  3879,  2114,  2198,  2928,  4862,
         11488,  1999,  2010,  2087,  3522,  7226,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2281, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.2545, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6298,  -6.5952,  -6.5823,  ...,  -5.9613,  -5.8079,  -3.9318],
         [-12.4292, -12.1855, -12.3582,  ..., -10.6819, -10.2634, -10.1454],
         [ -7.6795,  -7.7105,  -7.5262,  ...,  -7.2030,  -5.9169,  -6.4219],
         ...,
         [ -7.9591,  -7.9482,  -8.1018,  ...,  -7.5455,  -8.5734,  -3.5368],
         [ -7.1888,  -7.2489,  -7.0849,  ...,  -5.8146,  -5.7656,  -5.6500],
         [ -7.4592,  -7.4678,  -7.3463,  ...,  -7.1715,  -7.7362,  -5.2790]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.2544827461242676
Raw Input IDs (before masking): tensor([[  101,  6868,  2864,  2005,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,  1996,  2034,  2305,  1997, 21028,  1005,  5585,  1010,  1998,
          2001,  1996,  2034,  3287,  6520,  2956,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 128
Total masked tokens: 8
Percentage of masked tokens: 6.25%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 8
Unique label values and counts: {-100: 120, 1998: 1, 2005: 1, 2034: 1, 2864: 1, 2956: 1, 3770: 1, 6868: 1, 21028: 1}
Unique labels in batch: tensor([ -100,  1998,  2005,  2034,  2864,  2956,  3770,  6868, 21028])
Masked Input IDs: tensor([[  101,   103,   103,   103,  2019,  4358,   103,  1010,  2199,  2111,
          2006,  1996,   103,  2305,  1997,   103,  1005,  5585,  1010,   103,
          2001,  1996,  2034,  3287,  6520,   103,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  6868,  2864,  2005,  -100,  -100,  3770,  -100,  -100,  -100,
          -100,  -100,  2034,  -100,  -100, 21028,  -100,  -100,  -100,  1998,
          -100,  -100,  -100,  -100,  -100,  2956,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,   103,   103,  2019,  4358,   103,  1010,  2199,  2111,
          2006,  1996,   103,  2305,  1997,   103,  1005,  5585,  1010,   103,
          2001,  1996,  2034,  3287,  6520,   103,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  6868,  2864,  2005,  -100,  -100,  3770,  -100,  -100,  -100,
          -100,  -100,  2034,  -100,  -100, 21028,  -100,  -100,  -100,  1998,
          -100,  -100,  -100,  -100,  -100,  2956,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.9874, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8191, -6.8123, -6.7701,  ..., -6.1061, -5.8260, -3.9533],
         [-6.4309, -6.4512, -6.4147,  ..., -6.4089, -6.0821, -4.9594],
         [-4.2713, -4.4828, -4.1548,  ..., -4.6996, -3.5698, -4.1367],
         ...,
         [-4.7102, -4.5979, -4.6107,  ..., -4.2455, -4.0317, -2.2081],
         [-8.4605, -8.4782, -8.4132,  ..., -7.9953, -7.2481, -5.6593],
         [-7.6770, -7.7350, -7.6942,  ..., -7.3991, -7.2232, -4.8022]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.9873647689819336
Raw Input IDs (before masking): tensor([[ 101, 3766, 2001, 2700, 1037, 6020, 2457, 3648, 1999, 2901, 1010, 2635,
         1996, 6847, 1999, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 112
Total tokens: 128, Non-special tokens: 16, Masked tokens: 1
Unique label values and counts: {-100: 127, 1999: 1}
Unique labels in batch: tensor([-100, 1999])
Masked Input IDs: tensor([[ 101, 3766, 2001, 2700, 1037, 6020, 2457, 3648, 1999, 2901, 1010, 2635,
         1996, 6847,  103, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, 1999, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 3766, 2001, 2700, 1037, 6020, 2457, 3648, 1999, 2901, 1010, 2635,
         1996, 6847,  103, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, 1999, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0388, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8597,  -6.7841,  -6.7980,  ...,  -6.1058,  -5.9495,  -3.9802],
         [ -7.9591,  -8.2159,  -7.9476,  ...,  -8.8476,  -9.0668,  -5.2252],
         [-11.7660, -11.3764, -11.8877,  ..., -10.6838,  -9.2176,  -6.5063],
         ...,
         [ -8.6362,  -8.8032,  -8.8636,  ...,  -8.5576,  -9.2232,  -3.1153],
         [ -9.3314,  -9.5800,  -9.5902,  ...,  -9.0908,  -9.7741,  -4.3049],
         [ -8.9201,  -9.1024,  -9.1614,  ...,  -9.2201,  -9.7415,  -5.8085]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.03879785165190697
Raw Input IDs (before masking): tensor([[  101,  2014, 11062,  2307,  1011,  7133,  2001, 12903, 19330,  2050,
          1010,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,  1010, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 2
Unique label values and counts: {-100: 126, 1010: 1, 19330: 1}
Unique labels in batch: tensor([ -100,  1010, 19330])
Masked Input IDs: tensor([[  101,  2014, 11062,  2307,  1011,  7133,  2001, 12903,   103,  2050,
           103,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,  1010, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 19330,  -100,
          1010,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014, 11062,  2307,  1011,  7133,  2001, 12903,   103,  2050,
           103,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,  1010, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 19330,  -100,
          1010,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(5.6623, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5162,  -6.4859,  -6.4951,  ...,  -5.9338,  -5.6355,  -4.0096],
         [ -9.0225,  -9.3678,  -9.2235,  ..., -10.4891,  -6.5739,  -6.8343],
         [ -4.4655,  -4.5237,  -4.7702,  ...,  -4.2991,  -3.4972,  -3.2880],
         ...,
         [ -7.4902,  -7.4050,  -7.6468,  ...,  -7.7071,  -8.4683,  -2.8469],
         [ -8.0769,  -8.0021,  -8.1573,  ...,  -8.5943,  -8.2469,  -3.8102],
         [ -7.6735,  -7.6910,  -7.7799,  ...,  -8.3277,  -7.6990,  -5.5126]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 5.662260055541992
Raw Input IDs (before masking): tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792,  2029,  2443,  3922,  2011, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,  9893, 25208,  1998,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,  2019,  2792,  2029,  2956,  8507,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048,  2367,  4178,  1999,  2029,
          2280,  8626,  1005,  7939,  9387, 12986, 17590,  1998,  8507,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 128
Total masked tokens: 9
Percentage of masked tokens: 7.03%
Total special tokens in batch: 45
Total tokens: 128, Non-special tokens: 83, Masked tokens: 9
Unique label values and counts: {-100: 119, 1998: 2, 2443: 1, 3870: 1, 4178: 1, 6374: 1, 8507: 1, 9533: 1, 9893: 1}
Unique labels in batch: tensor([-100, 1998, 2443, 3870, 4178, 6374, 8507, 9533, 9893])
Masked Input IDs: tensor([[  101,   103, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792,  2029,   103,  3922,  2011, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,   103, 25208,  1998,  2547,  2739,
          8133,   103, 16042,   103,  1025,  2019,  2792,  2029,  2956,   103,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  3583,  2048,  2367,   103,  1999,  2029,
          2280,  8626,  1005,  7939,  9387, 12986, 17590,  1998,  8507,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 3870, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, 2443, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 9893, -100, -100, -100, -100, -100, 9533, -100, 6374, -100, -100,
         -100, -100, -100, 8507, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 1998, -100, -100, 4178, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 1998, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792,  2029,   103,  3922,  2011, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,   103, 25208,  1998,  2547,  2739,
          8133,   103, 16042,   103,  1025,  2019,  2792,  2029,  2956,   103,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  3583,  2048,  2367,   103,  1999,  2029,
          2280,  8626,  1005,  7939,  9387, 12986, 17590,  1998,  8507,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 3870, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, 2443, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 9893, -100, -100, -100, -100, -100, 9533, -100, 6374, -100, -100,
         -100, -100, -100, 8507, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 1998, -100, -100, 4178, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 1998, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.4762, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5209,  -6.4927,  -6.4014,  ...,  -5.8848,  -5.5742,  -3.7087],
         [ -7.9348,  -8.5136,  -8.0714,  ...,  -7.5656,  -8.1568,  -7.8987],
         [-11.1937, -11.5519, -11.2770,  ..., -10.5390,  -9.0205, -10.6226],
         ...,
         [ -5.6995,  -5.5124,  -5.4940,  ...,  -5.0291,  -5.7331,  -3.4305],
         [ -3.4132,  -3.1700,  -3.2486,  ...,  -3.2169,  -4.0383,  -2.2308],
         [ -7.5618,  -7.6451,  -7.6165,  ...,  -7.4377,  -8.5399,  -3.2130]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.4762048721313477
Raw Input IDs (before masking): tensor([[  101,  2077,  2014,  3510,  1010,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,  2232, 22142,  3753,  2011,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,  1011,  2375,  1999,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 86
Total tokens: 128, Non-special tokens: 42, Masked tokens: 3
Unique label values and counts: {-100: 125, 1998: 1, 14021: 1, 14719: 1}
Unique labels in batch: tensor([ -100,  1998, 14021, 14719])
Masked Input IDs: tensor([[  101,  2077,  2014,  3510,  1010,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,  2232, 22142,  3753,  2011,  7907,   103,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,  1011,  2375,  1999,  1996,   103,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14021,
          -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14719,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2077,  2014,  3510,  1010,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,  2232, 22142,  3753,  2011,  7907,   103,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,  1011,  2375,  1999,  1996,   103,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14021,
          -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 14719,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.5629, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5757,  -6.5517,  -6.5274,  ...,  -6.0984,  -5.8792,  -4.0761],
         [-11.5769, -11.4113, -11.5107,  ..., -12.1405, -12.2305,  -5.5872],
         [ -8.3339,  -8.1972,  -8.4910,  ...,  -8.9299,  -7.3797,  -6.3540],
         ...,
         [ -8.3166,  -8.0254,  -8.5211,  ...,  -8.0435,  -8.2948,  -3.0844],
         [ -7.8827,  -7.5647,  -8.0215,  ...,  -8.0677,  -8.9162,  -4.6260],
         [ -8.0997,  -7.8249,  -8.2639,  ...,  -8.6839,  -9.3201,  -3.5444]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.5629208087921143
Raw Input IDs (before masking): tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,  1998,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,  2062,  1997,  1996,  2168,  2013,
          2019,  3063,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 87
Total tokens: 128, Non-special tokens: 41, Masked tokens: 4
Unique label values and counts: {-100: 124, 1005: 1, 2168: 1, 3063: 1, 4226: 1}
Unique labels in batch: tensor([-100, 1005, 2168, 3063, 4226])
Masked Input IDs: tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,  1998,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,  2062,  1997,  1996,   103,  2013,
          2019,   103,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
           103,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 4226, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 2168, -100, -100, 3063, -100, -100, -100, -100,
         -100, -100, -100, -100, 1005, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,  1998,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,  2062,  1997,  1996,   103,  2013,
          2019,   103,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
           103,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 4226, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 2168, -100, -100, 3063, -100, -100, -100, -100,
         -100, -100, -100, -100, 1005, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.3582, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.3532, -6.3139, -6.2776,  ..., -5.7132, -5.3637, -3.5152],
         [-6.4487, -6.5001, -6.1520,  ..., -6.0731, -5.4775, -5.8755],
         [-7.5203, -7.3681, -7.0335,  ..., -6.4105, -6.1962, -5.4661],
         ...,
         [-6.5452, -6.5054, -6.5018,  ..., -7.0251, -6.5418, -3.1173],
         [-6.4991, -6.3578, -6.2450,  ..., -7.6395, -6.7337, -2.2046],
         [-6.5630, -6.7112, -6.4172,  ..., -6.6783, -6.4734, -2.9959]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.358178973197937
Raw Input IDs (before masking): tensor([[  101,  9093,  2229,  4917,  1005,  1055,  2866,  5826,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,  2329, 11995,  2866,  9189,  1998,
          1996,  2248, 16632, 11858,  4835,  3795,  4024,  1010,  2020,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 6
Unique label values and counts: {-100: 122, 1996: 1, 2020: 1, 2329: 1, 2537: 1, 2866: 1, 5826: 1}
Unique labels in batch: tensor([-100, 1996, 2020, 2329, 2537, 2866, 5826])
Masked Input IDs: tensor([[  101,  9093,  2229,  4917,  1005,  1055,   103,   103,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,   103, 11995,  2866,  9189,  1998,
         19993,  2248, 16632, 11858,  4835,  3795,  4024,  1010,   103,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, 2866, 5826, -100, -100, 2537, -100,
         -100, -100, -100, 2329, -100, -100, -100, -100, 1996, -100, -100, -100,
         -100, -100, -100, -100, 2020, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9093,  2229,  4917,  1005,  1055,   103,   103,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,   103, 11995,  2866,  9189,  1998,
         19993,  2248, 16632, 11858,  4835,  3795,  4024,  1010,   103,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, 2866, 5826, -100, -100, 2537, -100,
         -100, -100, -100, 2329, -100, -100, -100, -100, 1996, -100, -100, -100,
         -100, -100, -100, -100, 2020, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.6964, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8037, -6.7483, -6.7018,  ..., -6.0173, -5.6089, -4.1354],
         [-7.8449, -7.7537, -7.2546,  ..., -7.5367, -6.5889, -6.9548],
         [-8.2724, -8.2015, -7.8403,  ..., -8.0073, -5.9725, -4.2993],
         ...,
         [-8.8388, -8.6433, -8.9966,  ..., -8.4193, -8.6965, -4.0587],
         [-8.1700, -8.2960, -8.2558,  ..., -7.8192, -7.0118, -6.8082],
         [-8.3615, -8.3983, -8.4574,  ..., -7.8879, -7.6021, -6.5964]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.696413278579712
Raw Input IDs (before masking): tensor([[  101,  3174,  2086,  3283,  1010, 17798, 24471, 20755,  3603,  2995,
          2293,  2007,  6609, 23527,  1998,  2211,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 5
Unique label values and counts: {-100: 123, 1012: 1, 1037: 1, 2293: 1, 2995: 1, 23527: 1}
Unique labels in batch: tensor([ -100,  1012,  1037,  2293,  2995, 23527])
Masked Input IDs: tensor([[  101,  3174,  2086,  3283,  1010, 17798, 24471, 20755,  3603,   103,
           103,  2007,  6609,   103,  1998,  2211,   103,  7472,   103,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2995,
          2293,  -100,  -100, 23527,  -100,  -100,  1037,  -100,  1012,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3174,  2086,  3283,  1010, 17798, 24471, 20755,  3603,   103,
           103,  2007,  6609,   103,  1998,  2211,   103,  7472,   103,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2995,
          2293,  -100,  -100, 23527,  -100,  -100,  1037,  -100,  1012,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.9167, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5780, -6.5325, -6.5396,  ..., -5.8831, -5.7306, -3.6737],
         [-7.9378, -7.5783, -8.1804,  ..., -7.6027, -6.9993, -3.1277],
         [-5.2722, -5.7144, -5.6346,  ..., -3.8337, -4.3153, -3.8851],
         ...,
         [-5.9652, -5.9619, -5.8726,  ..., -6.3035, -5.5441, -4.7378],
         [-6.4572, -6.0982, -6.4577,  ..., -7.0301, -5.9503, -6.1423],
         [-7.0513, -6.8618, -6.9161,  ..., -6.8904, -7.0821, -4.1738]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.9166789054870605
Raw Input IDs (before masking): tensor([[ 101, 8856, 6868, 2003, 2019, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 121
Total tokens: 128, Non-special tokens: 7, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[ 101, 8856, 6868, 2003, 2019, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,  103,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 8856, 6868, 2003, 2019, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,  103,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(17.1750, grad_fn=<NllLossBackward0>), logits=tensor([[[-7.6648, -7.7674, -7.6506,  ..., -6.8779, -6.6813, -4.5923],
         [-6.6764, -6.7153, -6.8665,  ..., -6.6834, -5.5973, -5.4056],
         [-5.4803, -5.4210, -5.3836,  ..., -5.7712, -4.9052, -6.9073],
         ...,
         [-7.4660, -7.6676, -7.7070,  ..., -7.5940, -7.5644, -4.6256],
         [-5.8216, -5.8903, -5.9500,  ..., -6.4315, -6.3233, -3.8295],
         [-6.7453, -6.8861, -7.0478,  ..., -7.0864, -7.1709, -4.2969]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 17.1750431060791
Raw Input IDs (before masking): tensor([[  101,  6609,  1005,  1055,  2269,  1010, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 3
Unique label values and counts: {-100: 125, 1010: 2, 6305: 1}
Unique labels in batch: tensor([-100, 1010, 6305])
Masked Input IDs: tensor([[  101,  6609,  1005,  1055,  2269,   103, 24665,   103,  2401,   103,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, 1010, -100, 6305, -100, 1010, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6609,  1005,  1055,  2269,   103, 24665,   103,  2401,   103,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, 1010, -100, 6305, -100, 1010, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1125, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8271,  -6.8017,  -6.8171,  ...,  -6.1957,  -6.0833,  -4.1030],
         [ -6.3968,  -6.4816,  -6.5863,  ...,  -5.6401,  -5.9039,  -4.9049],
         [-14.4841, -14.2755, -14.7341,  ..., -12.4860, -11.0563, -10.6176],
         ...,
         [ -7.8430,  -7.8646,  -7.9128,  ...,  -7.6637,  -7.6754,  -5.3451],
         [ -6.5378,  -6.6324,  -6.5745,  ...,  -7.1930,  -5.5528,  -7.6836],
         [ -8.9134,  -8.8506,  -9.0585,  ...,  -8.8897,  -8.5843,  -4.7861]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.11245741695165634
Raw Input IDs (before masking): tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,  8690,  1036,
          1036,  5675,  2537,  1998, 20578, 14029,  1005,  1005,  1010,  2021,
         27175,  5061,  1005,  1055,  6180,  1998,  2049,  1036,  1036,  9210,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 128
Total masked tokens: 8
Percentage of masked tokens: 6.25%
Total special tokens in batch: 93
Total tokens: 128, Non-special tokens: 35, Masked tokens: 8
Unique label values and counts: {-100: 120, 1005: 2, 1036: 1, 5061: 1, 8690: 1, 9060: 1, 9210: 1, 27175: 1}
Unique labels in batch: tensor([ -100,  1005,  1036,  5061,  8690,  9060,  9210, 27175])
Masked Input IDs: tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,   103,  1036,
          1036,  5675,  2537,  1998, 20578, 14029,   103,  1005,  1010,  2021,
           103,   103,  1005,  1055,  6180,  1998,  2049,  1036,  1036,   103,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  9060,  -100,  -100,  -100,  -100,  -100,  -100,  8690,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  1005,  -100,  -100,  -100,
         27175,  5061,  1005,  -100,  -100,  -100,  -100,  1036,  -100,  9210,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,   103,  1036,
          1036,  5675,  2537,  1998, 20578, 14029,   103,  1005,  1010,  2021,
           103,   103,  1005,  1055,  6180,  1998,  2049,  1036,  1036,   103,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  9060,  -100,  -100,  -100,  -100,  -100,  -100,  8690,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  1005,  -100,  -100,  -100,
         27175,  5061,  1005,  -100,  -100,  -100,  -100,  1036,  -100,  9210,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.4034, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5523, -6.6191, -6.3735,  ..., -5.9540, -5.5731, -3.1026],
         [-5.7562, -5.8033, -5.6175,  ..., -6.6243, -5.8360, -6.6658],
         [-9.2291, -8.9383, -9.3002,  ..., -9.6525, -8.7548, -8.3267],
         ...,
         [-7.3492, -7.2565, -7.1953,  ..., -7.3529, -6.4052, -4.1709],
         [-7.2124, -7.5181, -7.1154,  ..., -7.5209, -5.6344, -3.2814],
         [-7.6621, -7.7222, -7.4653,  ..., -7.6549, -6.1323, -4.2150]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.403357982635498
Raw Input IDs (before masking): tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,  8607, 13793,  2097,
          2022,  2999,  2011,  2280, 14757,  2368,  4062,  3419, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 100
Total tokens: 128, Non-special tokens: 28, Masked tokens: 3
Unique label values and counts: {-100: 125, 1012: 1, 2368: 1, 3419: 1}
Unique labels in batch: tensor([-100, 1012, 2368, 3419])
Masked Input IDs: tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,  8607, 13793,  2097,
          2022,  2999,  2011,  2280, 14757,   103,  4062,   103, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,   103,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, 2368, -100, 3419, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,  8607, 13793,  2097,
          2022,  2999,  2011,  2280, 14757,   103,  4062,   103, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,   103,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, 2368, -100, 3419, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.2717, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.2334,  -6.1782,  -6.2148,  ...,  -5.5922,  -5.4311,  -3.8217],
         [ -7.6401,  -7.8900,  -7.5386,  ...,  -6.1474,  -6.1602,  -4.5404],
         [ -6.3036,  -6.7356,  -6.6843,  ...,  -5.3166,  -5.4493,  -4.5889],
         ...,
         [ -6.7863,  -6.8608,  -7.1459,  ...,  -7.5847,  -6.6183,  -1.6556],
         [ -7.4776,  -7.4728,  -7.5613,  ...,  -6.9721,  -7.9151,  -4.3482],
         [ -9.4353,  -9.4650,  -9.7494,  ...,  -9.7852, -10.2206,  -4.6534]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.271689534187317
Raw Input IDs (before masking): tensor([[  101,  2010,  4203, 10768,  3850,  2834,  1999,  2384,  2001,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,  3669,  5558,
          2615,  1005,  1055,  7110,  8447,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 102
Total tokens: 128, Non-special tokens: 26, Masked tokens: 2
Unique label values and counts: {-100: 126, 2010: 1, 8447: 1}
Unique labels in batch: tensor([-100, 2010, 8447])
Masked Input IDs: tensor([[  101,   103,  4203, 10768,  3850,  2834,  1999,  2384,  2001,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,  3669,  5558,
          2615,  1005,  1055,  7110,   103,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 2010, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         8447, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,  4203, 10768,  3850,  2834,  1999,  2384,  2001,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,  3669,  5558,
          2615,  1005,  1055,  7110,   103,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 2010, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         8447, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(6.6031, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7084,  -6.6691,  -6.6650,  ...,  -5.8976,  -5.9057,  -3.8740],
         [ -6.9672,  -7.1221,  -6.8377,  ...,  -7.1820,  -6.8673,  -6.1557],
         [-11.9806, -11.7207, -11.7766,  ...,  -8.9447,  -9.3769,  -8.7572],
         ...,
         [ -6.8657,  -6.9629,  -6.9907,  ...,  -7.7941,  -8.6721,  -1.8453],
         [ -7.8497,  -7.7183,  -7.9179,  ...,  -7.7295,  -9.2778,  -2.6370],
         [ -8.0668,  -8.1302,  -8.0939,  ...,  -8.2038,  -8.3758,  -5.1486]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 6.603137016296387
Raw Input IDs (before masking): tensor([[  101,  2014, 11062,  5615,  2001,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 114
Total tokens: 128, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[  101,  2014, 11062,  5615,  2001,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,   103,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100,    0, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014, 11062,  5615,  2001,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,   103,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100,    0, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(17.0315, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8592,  -6.7589,  -6.7824,  ...,  -6.3094,  -6.1069,  -4.0096],
         [-10.2796, -10.2729, -10.5255,  ..., -11.1102,  -8.4625,  -6.9161],
         [ -8.0677,  -7.9225,  -8.1966,  ...,  -7.7267,  -4.6653,  -5.1896],
         ...,
         [ -7.3763,  -7.2083,  -7.4992,  ...,  -7.9720,  -7.8942,  -2.5495],
         [ -7.2864,  -7.2008,  -7.5429,  ...,  -7.3789,  -8.0990,  -1.3086],
         [ -7.9325,  -8.0702,  -8.3000,  ...,  -8.6030,  -8.5879,  -2.9652]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 17.03151512145996
  Batch    40  of     50.    Elapsed: 0:00:29.
Raw Input IDs (before masking): tensor([[  101,  2139,  2474,  2061,  2696,  2153,  2743,  2005, 13523,  4948,
          1997,  1039,  1008, 16428, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 126, 2005: 1, 2743: 1}
Unique labels in batch: tensor([-100, 2005, 2743])
Masked Input IDs: tensor([[  101,  2139,  2474,  2061,  2696,  2153, 25835,   103, 13523,  4948,
          1997,  1039,  1008, 16428, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, 2743, 2005, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2139,  2474,  2061,  2696,  2153, 25835,   103, 13523,  4948,
          1997,  1039,  1008, 16428, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, 2743, 2005, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(7.2017, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7715,  -6.6689,  -6.6778,  ...,  -6.0109,  -5.8523,  -3.9895],
         [-10.0896, -10.2776, -10.5837,  ...,  -8.6978, -11.4290,  -7.6611],
         [ -8.6527,  -8.7977,  -9.1881,  ...,  -7.2179,  -9.0415,  -5.8593],
         ...,
         [ -6.8809,  -6.6802,  -6.9196,  ...,  -7.0296,  -7.6854,  -2.7490],
         [ -7.2093,  -7.0898,  -7.1867,  ...,  -7.3530,  -7.8012,  -3.5057],
         [ -6.2454,  -6.1316,  -6.1617,  ...,  -6.1377,  -6.9121,  -2.7760]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 7.201714992523193
Raw Input IDs (before masking): tensor([[  101,  2044,  2086,  1997,  2108,  2007,  2613,  3868,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,  2011,  2280,  2136, 28919, 27605, 10422,
          4062,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 92
Total tokens: 128, Non-special tokens: 36, Masked tokens: 7
Unique label values and counts: {-100: 121, 1010: 1, 2011: 1, 2025: 1, 2044: 1, 2108: 1, 2161: 1, 4062: 1}
Unique labels in batch: tensor([-100, 1010, 2011, 2025, 2044, 2108, 2161, 4062])
Masked Input IDs: tensor([[  101,  2044,  2086,  1997, 19163,  2007,  2613,  3868,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,   103,  3298,  2005,  2023,
           103,   103,  2108,  2999,   103,  2280,  2136, 28919, 27605, 10422,
          1220,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 2044, -100, -100, 2108, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 2025, -100, -100, -100, 2161, 1010, -100, -100,
         2011, -100, -100, -100, -100, -100, 4062, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2044,  2086,  1997, 19163,  2007,  2613,  3868,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,   103,  3298,  2005,  2023,
           103,   103,  2108,  2999,   103,  2280,  2136, 28919, 27605, 10422,
          1220,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 2044, -100, -100, 2108, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 2025, -100, -100, -100, 2161, 1010, -100, -100,
         2011, -100, -100, -100, -100, -100, 4062, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.4864, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7510,  -6.7174,  -6.7155,  ...,  -6.0696,  -5.8890,  -3.9150],
         [-11.7433, -11.6428, -12.2775,  ...,  -9.5283, -10.0344, -10.8148],
         [ -6.8544,  -7.3415,  -7.4102,  ...,  -5.9748,  -6.8493,  -2.9019],
         ...,
         [ -9.3225,  -9.2816,  -9.4070,  ...,  -9.0433,  -9.8615,  -3.0669],
         [ -8.4174,  -8.6178,  -8.4173,  ...,  -7.9901,  -7.1720,  -4.4292],
         [ -9.2688,  -9.3565,  -9.4921,  ...,  -9.1867,  -8.7686,  -4.3709]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.486396074295044
Raw Input IDs (before masking): tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010,  2066, 10461,  5586,
          2571,  1010,  2097,  2025,  2709,  2000,  3579,  2006,  2014,  1048,
          8737,  2487,  3298,  1999,  1996,  2325,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 128
Total masked tokens: 9
Percentage of masked tokens: 7.03%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 9
Unique label values and counts: {-100: 119, 1048: 1, 2000: 1, 2014: 1, 2088: 1, 2528: 1, 2571: 1, 2709: 1, 13006: 1, 14280: 1}
Unique labels in batch: tensor([ -100,  1048,  2000,  2014,  2088,  2528,  2571,  2709, 13006, 14280])
Masked Input IDs: tensor([[  101, 10556, 24015, 17823,   103,  9581,  1010,  2066, 10461,  5586,
           103,  1010,  2097,  2025,   103,   103,  3579,  2006,   103,   103,
          8737,  2487,  3298,  1999,  1996,  2325,   103,   103,   103,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100, 13006,  -100,  -100,  -100,  -100,  -100,
          2571,  -100,  -100,  -100,  2709,  2000,  -100,  -100,  2014,  1048,
          -100,  -100,  -100,  -100,  -100,  -100,  2088, 14280,  2528,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 10556, 24015, 17823,   103,  9581,  1010,  2066, 10461,  5586,
           103,  1010,  2097,  2025,   103,   103,  3579,  2006,   103,   103,
          8737,  2487,  3298,  1999,  1996,  2325,   103,   103,   103,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100, 13006,  -100,  -100,  -100,  -100,  -100,
          2571,  -100,  -100,  -100,  2709,  2000,  -100,  -100,  2014,  1048,
          -100,  -100,  -100,  -100,  -100,  -100,  2088, 14280,  2528,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.7549, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5588,  -6.5448,  -6.5506,  ...,  -5.8681,  -5.7590,  -3.8737],
         [-14.0180, -14.6977, -14.1819,  ..., -12.6102, -12.7101,  -9.1359],
         [ -5.7821,  -6.2632,  -6.0170,  ...,  -5.1854,  -5.4478,  -4.4025],
         ...,
         [ -8.7548,  -8.8679,  -8.9297,  ...,  -8.1616,  -8.2915,  -3.2051],
         [ -9.0688,  -9.0925,  -9.1359,  ...,  -8.6038,  -8.2741,  -3.9658],
         [ -8.4469,  -8.6113,  -8.5265,  ...,  -8.4998,  -7.8418,  -3.4639]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.754903554916382
Raw Input IDs (before masking): tensor([[  101,  2002,  2288,  2010,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,  1999,  3050,  3349,  1010,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 128
Total masked tokens: 8
Percentage of masked tokens: 6.25%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 8
Unique label values and counts: {-100: 120, 1010: 1, 1012: 1, 1057: 1, 2006: 1, 2225: 1, 3349: 1, 13960: 1, 14767: 1}
Unique labels in batch: tensor([ -100,  1010,  1012,  1057,  2006,  2225,  3349, 13960, 14767])
Masked Input IDs: tensor([[  101,  2002,  2288,  2010,  2707, 11003,  1996,   103,  3023,  1997,
          1996, 26450,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046,   103, 14767,  1999,  3050,   103,   103,  1998,  2776,  2333,
          2875, 16588,   103,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  2006,  -100,  2225,  -100,  -100,
          -100,  1057,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100, 13960, 14767,  -100,  -100,  3349,  1010,  -100,  -100,  -100,
          -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  2288,  2010,  2707, 11003,  1996,   103,  3023,  1997,
          1996, 26450,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046,   103, 14767,  1999,  3050,   103,   103,  1998,  2776,  2333,
          2875, 16588,   103,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  2006,  -100,  2225,  -100,  -100,
          -100,  1057,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100, 13960, 14767,  -100,  -100,  3349,  1010,  -100,  -100,  -100,
          -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.8803, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7469,  -6.7514,  -6.7176,  ...,  -6.0934,  -5.8381,  -3.9946],
         [-12.5421, -12.6896, -12.7860,  ..., -10.7498, -10.0988,  -8.6953],
         [ -9.6457,  -9.7807, -10.2561,  ..., -10.4098,  -7.9162, -12.1828],
         ...,
         [ -8.1380,  -8.2509,  -8.3069,  ...,  -7.3638,  -6.9822,  -6.3942],
         [ -8.3193,  -8.2134,  -8.5582,  ...,  -7.6277,  -8.0685,  -2.9515],
         [ -7.9498,  -7.9894,  -8.0607,  ...,  -7.6553,  -7.2223,  -5.0190]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.8803291320800781
Raw Input IDs (before masking): tensor([[  101,  6382, 10093,  3877,  1011,  1011,  2209,  1996,  2610,  2961,
          6898,  1997, 28517,  6216,  1010, 12270,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 112
Total tokens: 128, Non-special tokens: 16, Masked tokens: 4
Unique label values and counts: {-100: 124, 1012: 1, 2961: 1, 6216: 1, 10093: 1}
Unique labels in batch: tensor([ -100,  1012,  2961,  6216, 10093])
Masked Input IDs: tensor([[  101,  6382,   103,  3877,  1011,  1011,  2209,  1996,  2610,   103,
          6898,  1997, 28517,   103,  1010, 12270,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100, 10093,  -100,  -100,  -100,  -100,  -100,  -100,  2961,
          -100,  -100,  -100,  6216,  -100,  -100,  1012,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6382,   103,  3877,  1011,  1011,  2209,  1996,  2610,   103,
          6898,  1997, 28517,   103,  1010, 12270,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100, 10093,  -100,  -100,  -100,  -100,  -100,  -100,  2961,
          -100,  -100,  -100,  6216,  -100,  -100,  1012,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.6004, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6420, -6.6238, -6.6008,  ..., -5.9794, -5.7223, -3.9606],
         [-6.6948, -6.8314, -6.9679,  ..., -6.8038, -5.9296, -7.7103],
         [-8.4940, -8.4833, -8.4164,  ..., -8.8617, -6.8078, -5.9065],
         ...,
         [-7.3720, -7.4332, -7.5127,  ..., -7.6003, -6.7907, -5.1283],
         [-7.6557, -7.5115, -7.7941,  ..., -7.7532, -7.6926, -2.1199],
         [-7.3967, -7.5245, -7.5256,  ..., -7.4844, -6.8258, -4.4387]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.6004034280776978
Raw Input IDs (before masking): tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
          3129,  2003,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 83
Total tokens: 128, Non-special tokens: 45, Masked tokens: 5
Unique label values and counts: {-100: 123, 1055: 1, 1996: 1, 3948: 1, 4095: 1, 8665: 1}
Unique labels in batch: tensor([-100, 1055, 1996, 3948, 4095, 8665])
Masked Input IDs: tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
          3129,  2003,  1996,  2567,  1997,  7907, 25175,   103,  2063,   103,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1055, -100, -100,
         1996, -100, -100, -100, -100, 4095, -100, 8665, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 3948, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
          3129,  2003,  1996,  2567,  1997,  7907, 25175,   103,  2063,   103,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1055, -100, -100,
         1996, -100, -100, -100, -100, 4095, -100, 8665, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 3948, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.8165, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9795, -6.9248, -6.9471,  ..., -6.2618, -6.2178, -3.9830],
         [-9.6485, -9.2846, -9.8295,  ..., -9.8963, -9.8481, -4.9029],
         [-6.5393, -6.1873, -6.3552,  ..., -5.9831, -6.6126, -3.9171],
         ...,
         [-3.5800, -3.6696, -3.8247,  ..., -5.1847, -3.3922, -1.4355],
         [-3.3255, -3.3672, -3.6219,  ..., -4.7887, -3.2118, -2.0527],
         [-8.0249, -8.0425, -8.1769,  ..., -8.4351, -9.0053, -4.8533]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.8164803385734558
Raw Input IDs (before masking): tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,  2021,  5295,
          1999,  2901,  2000,  5138,  1037,  2695,  2004,  6059,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 108
Total tokens: 128, Non-special tokens: 20, Masked tokens: 6
Unique label values and counts: {-100: 122, 2000: 1, 2004: 1, 2695: 1, 2901: 1, 3519: 1, 20847: 1}
Unique labels in batch: tensor([ -100,  2000,  2004,  2695,  2901,  3519, 20847])
Masked Input IDs: tensor([[  101,  2016,  2018,  2042,   103,  2000,   103,  1010,  2021,  5295,
          1999,   103,   103,  5138,  1037,  2695, 24301,  6059,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100, 20847,  -100,  3519,  -100,  -100,  -100,
          -100,  2901,  2000,  -100,  -100,  2695,  2004,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2016,  2018,  2042,   103,  2000,   103,  1010,  2021,  5295,
          1999,   103,   103,  5138,  1037,  2695, 24301,  6059,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100, 20847,  -100,  3519,  -100,  -100,  -100,
          -100,  2901,  2000,  -100,  -100,  2695,  2004,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.2613, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7392, -6.7355, -6.7534,  ..., -6.1664, -5.9248, -3.7700],
         [-6.8329, -6.5953, -6.7209,  ..., -6.6655, -5.9318, -2.5755],
         [-5.7341, -5.7172, -5.6512,  ..., -5.9720, -4.0781, -4.5856],
         ...,
         [-7.3436, -7.6332, -7.4740,  ..., -7.2271, -7.6411, -4.3733],
         [-5.9721, -6.2019, -5.7973,  ..., -5.4760, -5.7100, -3.6100],
         [-6.7403, -6.9602, -6.6535,  ..., -6.2255, -6.5017, -4.6006]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.2613401412963867
Raw Input IDs (before masking): tensor([[ 101, 2002, 2109, 2137, 6443, 2000, 2507, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 117
Total tokens: 128, Non-special tokens: 11, Masked tokens: 1
Unique label values and counts: {-100: 127, 2002: 1}
Unique labels in batch: tensor([-100, 2002])
Masked Input IDs: tensor([[ 101,  103, 2109, 2137, 6443, 2000, 2507, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101,  103, 2109, 2137, 6443, 2000, 2507, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.7260, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6024, -6.5823, -6.5862,  ..., -5.8379, -5.7370, -3.6829],
         [-5.1471, -5.1261, -5.0180,  ..., -5.3078, -4.8120, -1.8376],
         [-7.1572, -6.9474, -7.0670,  ..., -6.0912, -3.5508, -5.6407],
         ...,
         [-6.5927, -6.6832, -6.5079,  ..., -6.4617, -6.0732, -2.3893],
         [-7.9740, -7.8980, -8.0049,  ..., -7.9902, -8.1341, -2.9905],
         [-6.9338, -7.0156, -7.0363,  ..., -6.9723, -7.0233, -4.0479]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.7259697914123535
Raw Input IDs (before masking): tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,   103,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,   103,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(17.4231, grad_fn=<NllLossBackward0>), logits=tensor([[[ -7.4704,  -7.5055,  -7.4595,  ...,  -7.0526,  -7.0115,  -4.1270],
         [ -5.8257,  -5.6658,  -5.6206,  ...,  -6.3540,  -7.2785,  -3.3149],
         [ -7.9317,  -8.0206,  -8.2887,  ...,  -8.0720,  -5.9743,  -5.4933],
         ...,
         [ -6.3241,  -6.3022,  -6.1773,  ...,  -7.1998,  -8.3141,  -3.1920],
         [ -8.2335,  -8.1401,  -8.2034,  ...,  -8.6804, -10.4104,  -2.8242],
         [ -5.2289,  -5.1923,  -5.2607,  ...,  -6.3524,  -7.3189,  -0.9363]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 17.423141479492188
Raw Input IDs (before masking): tensor([[  101,  2010,  2388,  2001,  2019, 25244,  1036,  1036,  1997,  4635,
          1998,  3226,  1005,  1005,  1998,  2010,  2269,  2001,  1037,  2489,
          2158,  1997,  3609,  1010,  2649,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 5
Unique label values and counts: {-100: 123, 1011: 1, 1037: 1, 1997: 2, 2422: 1}
Unique labels in batch: tensor([-100, 1011, 1037, 1997, 2422])
Masked Input IDs: tensor([[  101,  2010,  2388,  2001,  2019, 25244,  1036,  1036,   103,  4635,
          1998,  3226,  1005,  1005,  1998,  2010,  2269,  2001,   103,  2489,
          2158,   103,  3609,  1010,  2649,  2004,   103,   103, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 1037, -100, -100, 1997, -100, -100,
         -100, -100, 2422, 1011, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2010,  2388,  2001,  2019, 25244,  1036,  1036,   103,  4635,
          1998,  3226,  1005,  1005,  1998,  2010,  2269,  2001,   103,  2489,
          2158,   103,  3609,  1010,  2649,  2004,   103,   103, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 1037, -100, -100, 1997, -100, -100,
         -100, -100, 2422, 1011, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.5562, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7751,  -6.8133,  -6.8100,  ...,  -6.4425,  -6.1936,  -3.8699],
         [-12.9395, -13.1307, -13.7654,  ..., -14.5118, -11.9679, -11.2401],
         [ -9.4992,  -9.6007,  -9.8107,  ...,  -9.0307,  -6.3193,  -8.5899],
         ...,
         [ -7.6582,  -7.5706,  -7.9752,  ...,  -8.2035,  -7.7438,  -5.6618],
         [ -7.3261,  -7.2402,  -7.5490,  ...,  -8.0550,  -7.4374,  -5.7856],
         [ -7.4203,  -7.3622,  -7.6019,  ...,  -7.9566,  -6.7413,  -6.6691]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.5562050342559814

  Average training loss: 3.83
[Epoch 1] Training epoch took: 0:00:36

======== Epoch 2 / 3 ========
Training...
Raw Input IDs (before masking): tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,  7015,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,  2055,  2293,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 3
Unique label values and counts: {-100: 125, 2262: 1, 2293: 1, 7015: 1}
Unique labels in batch: tensor([-100, 2262, 2293, 7015])
Masked Input IDs: tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,   103,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,  2055,   103,  1996,
          2959,  2190,  2201,  1997,   103,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 7015, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 2293, -100, -100, -100, -100, -100,
         2262, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,   103,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,  2055,   103,  1996,
          2959,  2190,  2201,  1997,   103,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 7015, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 2293, -100, -100, -100, -100, -100,
         2262, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.9995, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5837,  -6.5421,  -6.5394,  ...,  -5.8557,  -5.6975,  -3.8026],
         [-13.6880, -13.6628, -13.3500,  ..., -12.7985, -12.4600, -10.2070],
         [ -9.6642,  -9.5425,  -9.0220,  ..., -10.5735,  -8.3700,  -8.8479],
         ...,
         [ -7.9965,  -8.0731,  -7.8715,  ...,  -8.9307,  -7.0316,  -5.5728],
         [ -6.7796,  -6.8377,  -6.5984,  ...,  -7.6900,  -6.9854,  -5.5657],
         [ -8.2418,  -8.3504,  -8.0278,  ...,  -8.6167,  -7.5444,  -5.1493]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.9994653463363647
Raw Input IDs (before masking): tensor([[  101,  2002,  2038,  2144,  7042,  2195,  3033,  1998,  4395,  1999,
          2984,  5922,  1005,  2573,  1010,  2164,  1996, 10430,  2112,  1999,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,  1999,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 78
Total tokens: 128, Non-special tokens: 50, Masked tokens: 6
Unique label values and counts: {-100: 122, 1996: 1, 1999: 1, 2038: 1, 2112: 1, 2535: 1, 3246: 1}
Unique labels in batch: tensor([-100, 1996, 1999, 2038, 2112, 2535, 3246])
Masked Input IDs: tensor([[  101,  2002,   103,  2144,  7042,  2195,  3033,  1998,  4395,  1999,
          2984,  5922,  1005,  2573,  1010,  2164,  1996, 10430,   103,  1999,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,   103,  1997, 13970,
         12274, 17516,   103,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,   103,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2038, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 2112, -100, -100, -100, -100, -100,
         -100, -100, -100, 2535, -100, -100, -100, -100, 1999, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, 1996, -100, -100, 3246, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,   103,  2144,  7042,  2195,  3033,  1998,  4395,  1999,
          2984,  5922,  1005,  2573,  1010,  2164,  1996, 10430,   103,  1999,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,   103,  1997, 13970,
         12274, 17516,   103,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,   103,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2038, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 2112, -100, -100, -100, -100, -100,
         -100, -100, -100, 2535, -100, -100, -100, -100, 1999, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, 1996, -100, -100, 3246, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.5669, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6588, -6.6002, -6.6029,  ..., -5.9154, -5.7778, -4.0998],
         [-8.5509, -8.6715, -8.8967,  ..., -8.1525, -8.1769, -7.5029],
         [-4.5779, -4.6366, -4.6646,  ..., -4.7944, -4.0096, -4.4481],
         ...,
         [-8.7962, -8.5436, -8.9357,  ..., -8.0012, -8.4170, -3.3960],
         [-7.2606, -7.3031, -7.2751,  ..., -6.2552, -7.0008, -4.5739],
         [-8.8646, -8.9021, -9.0611,  ..., -7.8348, -7.9098, -5.4191]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.5669071674346924
Raw Input IDs (before masking): tensor([[  101,  2002,  6369,  2006,  1996,  4745, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 114
Total tokens: 128, Non-special tokens: 14, Masked tokens: 2
Unique label values and counts: {-100: 126, 3405: 1, 20846: 1}
Unique labels in batch: tensor([ -100,  3405, 20846])
Masked Input IDs: tensor([[  101,  2002,  6369,  2006,  1996,  4745, 11605, 13250,  5302,   103,
           103,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 20846,
          3405,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  6369,  2006,  1996,  4745, 11605, 13250,  5302,   103,
           103,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 20846,
          3405,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1475, grad_fn=<NllLossBackward0>), logits=tensor([[[-7.0988, -7.1260, -7.1453,  ..., -6.2959, -6.0319, -4.2608],
         [-9.1035, -9.2099, -9.3264,  ..., -7.8992, -8.1714, -6.0633],
         [-4.6496, -4.6843, -4.8013,  ..., -4.7651, -3.7568, -1.3394],
         ...,
         [-7.8937, -7.9955, -7.9571,  ..., -8.4571, -8.7876, -5.3133],
         [-6.5466, -6.8369, -6.7038,  ..., -6.8284, -7.9353, -3.2237],
         [-8.4978, -8.5719, -8.5475,  ..., -8.7722, -9.0862, -5.1399]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.14746634662151337
Raw Input IDs (before masking): tensor([[  101,  3533, 14103,  5411, 11382, 12096,  3170, 25260,  1010,  3237,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,  1010,
          2055,  5719,  1996,  2186,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,  2081,  7619,  3168,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 88
Total tokens: 128, Non-special tokens: 40, Masked tokens: 7
Unique label values and counts: {-100: 121, 1010: 3, 1996: 1, 3237: 1, 5411: 1, 11382: 1}
Unique labels in batch: tensor([ -100,  1010,  1996,  3237,  5411, 11382])
Masked Input IDs: tensor([[  101,  3533, 14103,   103,   103, 12096,  3170, 25260,   103,   103,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,   103,
          2055,  5719,   103,  2186,   103,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,  2081,  7619,  3168,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  5411, 11382,  -100,  -100,  -100,  1010,  3237,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,
          -100,  -100,  1996,  -100,  1010,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3533, 14103,   103,   103, 12096,  3170, 25260,   103,   103,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,   103,
          2055,  5719,   103,  2186,   103,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,  2081,  7619,  3168,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  5411, 11382,  -100,  -100,  -100,  1010,  3237,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,
          -100,  -100,  1996,  -100,  1010,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.1897, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8041, -6.7695, -6.7178,  ..., -6.1212, -5.8392, -4.0935],
         [-6.5270, -6.5783, -6.5901,  ..., -5.7771, -5.9372, -6.7921],
         [-4.9820, -4.7417, -4.8644,  ..., -5.6053, -4.1549, -6.1826],
         ...,
         [-5.7195, -5.7274, -5.9077,  ..., -5.4273, -6.1595, -3.1702],
         [-6.9510, -6.8500, -6.9568,  ..., -6.2594, -6.7159, -4.0834],
         [-6.4838, -6.5437, -6.6569,  ..., -6.4441, -6.5415, -5.3915]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.1896671056747437
Raw Input IDs (before masking): tensor([[  101,  2005,  2014,  3850,  3460,  9593,  1010,  5922,  2128, 13088,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,  2761,
          1037,  2033, 12036,  1011, 10430,  2535,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012, 13677,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,  1998,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 128
Total masked tokens: 8
Percentage of masked tokens: 6.25%
Total special tokens in batch: 64
Total tokens: 128, Non-special tokens: 64, Masked tokens: 8
Unique label values and counts: {-100: 120, 1997: 1, 2005: 1, 2128: 1, 2535: 1, 3850: 1, 5922: 1, 14433: 1, 22492: 1}
Unique labels in batch: tensor([ -100,  1997,  2005,  2128,  2535,  3850,  5922, 14433, 22492])
Masked Input IDs: tensor([[  101,   103,  2014,  3850,  3460,  9593,  1010,  5922,   103, 13088,
         12184,  1996,  2535,  1997,   103,  6728, 11837, 18826,  1010,  2761,
          1037,  2033, 12036,  1011, 10430,   103,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012, 13677,   103,  9024,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,  1998,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  2005,  -100,  -100,  -100,  -100,  -100,  5922,  2128,  -100,
          -100,  -100,  -100,  -100, 14433,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2535,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  3850,  1997,  -100,  -100,  -100,
          -100, 22492,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,  2014,  3850,  3460,  9593,  1010,  5922,   103, 13088,
         12184,  1996,  2535,  1997,   103,  6728, 11837, 18826,  1010,  2761,
          1037,  2033, 12036,  1011, 10430,   103,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012, 13677,   103,  9024,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,  1998,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  2005,  -100,  -100,  -100,  -100,  -100,  5922,  2128,  -100,
          -100,  -100,  -100,  -100, 14433,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2535,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  3850,  1997,  -100,  -100,  -100,
          -100, 22492,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.8225, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9805, -6.9266, -6.9605,  ..., -6.0615, -6.0770, -3.8768],
         [-7.1298, -6.7363, -7.2051,  ..., -7.1020, -7.7710, -5.2567],
         [-8.9363, -8.5317, -8.7424,  ..., -8.2277, -7.7217, -5.2346],
         ...,
         [-7.9743, -8.0284, -8.1466,  ..., -8.2138, -8.1587, -3.7510],
         [-4.6927, -4.6453, -4.7322,  ..., -5.2213, -4.9035, -3.2632],
         [-6.0783, -6.1289, -6.1913,  ..., -6.4270, -6.6856, -2.9777]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.8224536180496216
Raw Input IDs (before masking): tensor([[  101,  3174,  2086,  3283,  1010, 17798, 24471, 20755,  3603,  2995,
          2293,  2007,  6609, 23527,  1998,  2211,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 125, 2211: 1, 17798: 1, 23527: 1}
Unique labels in batch: tensor([ -100,  2211, 17798, 23527])
Masked Input IDs: tensor([[  101,  3174,  2086,  3283,  1010,   103, 24471, 20755,  3603,  2995,
          2293,  2007,  6609,   103,  1998,   103,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100, 17798,  -100,  -100,  -100,  -100,
          -100,  -100,  -100, 23527,  -100,  2211,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3174,  2086,  3283,  1010,   103, 24471, 20755,  3603,  2995,
          2293,  2007,  6609,   103,  1998,   103,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100, 17798,  -100,  -100,  -100,  -100,
          -100,  -100,  -100, 23527,  -100,  2211,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.8640, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6087, -6.5517, -6.5728,  ..., -5.9152, -5.7809, -3.8194],
         [-8.8115, -8.7530, -9.0657,  ..., -8.4280, -8.2769, -2.2157],
         [-7.5906, -7.9569, -7.6923,  ..., -6.4423, -5.8612, -6.2177],
         ...,
         [-2.9154, -2.5817, -2.8807,  ..., -3.5577, -3.6067, -2.2204],
         [-6.8966, -6.8531, -7.0171,  ..., -7.0398, -6.3816, -4.1683],
         [-7.3487, -7.3209, -7.4602,  ..., -7.2757, -7.0236, -4.5888]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.863976955413818
Raw Input IDs (before masking): tensor([[  101,  9093,  2229,  4917,  1005,  1055,  2866,  5826,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,  2329, 11995,  2866,  9189,  1998,
          1996,  2248, 16632, 11858,  4835,  3795,  4024,  1010,  2020,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 7
Unique label values and counts: {-100: 121, 1996: 1, 1997: 1, 2186: 1, 2248: 1, 2329: 1, 3692: 1, 3795: 1}
Unique labels in batch: tensor([-100, 1996, 1997, 2186, 2248, 2329, 3692, 3795])
Masked Input IDs: tensor([[  101,  9093,  2229,  4917,  1005,  1055,  2866,  5826,  1999,  1996,
          2537,   103,  1996,   103,  1010,   103, 11995,  2866,  9189,  1998,
           103,   103, 16632, 11858,  4835,   103,  4024,  1010,  2020,  2119,
          4699,  1999,  3176,   103,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1997,
         -100, 2186, -100, 2329, -100, -100, -100, -100, 1996, 2248, -100, -100,
         -100, 3795, -100, -100, -100, -100, -100, -100, -100, 3692, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9093,  2229,  4917,  1005,  1055,  2866,  5826,  1999,  1996,
          2537,   103,  1996,   103,  1010,   103, 11995,  2866,  9189,  1998,
           103,   103, 16632, 11858,  4835,   103,  4024,  1010,  2020,  2119,
          4699,  1999,  3176,   103,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1997,
         -100, 2186, -100, 2329, -100, -100, -100, -100, 1996, 2248, -100, -100,
         -100, 3795, -100, -100, -100, -100, -100, -100, -100, 3692, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.3924, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7237,  -6.6867,  -6.6704,  ...,  -6.0028,  -5.6646,  -4.1840],
         [ -8.5408,  -8.6555,  -7.9811,  ...,  -7.9491,  -7.4756,  -9.2156],
         [ -7.6049,  -7.5698,  -7.3500,  ...,  -7.8219,  -7.4021,  -4.7563],
         ...,
         [ -9.7173, -10.1080,  -9.5589,  ...,  -9.2612,  -8.0739, -10.4924],
         [ -5.9721,  -5.9640,  -5.9686,  ...,  -5.4848,  -5.1530,  -5.3307],
         [ -9.1750,  -9.3172,  -8.9747,  ...,  -9.1632,  -7.9286,  -8.1614]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.3924129009246826
Raw Input IDs (before masking): tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,  8690,  1036,
          1036,  5675,  2537,  1998, 20578, 14029,  1005,  1005,  1010,  2021,
         27175,  5061,  1005,  1055,  6180,  1998,  2049,  1036,  1036,  9210,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 93
Total tokens: 128, Non-special tokens: 35, Masked tokens: 7
Unique label values and counts: {-100: 121, 1036: 2, 2537: 1, 5061: 1, 8690: 1, 11007: 1, 20578: 1}
Unique labels in batch: tensor([ -100,  1036,  2537,  5061,  8690, 11007, 20578])
Masked Input IDs: tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,  3893,   103,
          1036,  5675,   103,  1998,   103, 14029,  1005,  1005,  1010,  2021,
         27175,   103,  1005,  1055,  6180,  1998,  2049,  1036,   103,  9210,
          1005,  1005,  1997,   103,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  8690,  1036,
          -100,  -100,  2537,  -100, 20578,  -100,  -100,  -100,  -100,  -100,
          -100,  5061,  -100,  -100,  -100,  -100,  -100,  -100,  1036,  -100,
          -100,  -100,  -100, 11007,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,  3893,   103,
          1036,  5675,   103,  1998,   103, 14029,  1005,  1005,  1010,  2021,
         27175,   103,  1005,  1055,  6180,  1998,  2049,  1036,   103,  9210,
          1005,  1005,  1997,   103,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  8690,  1036,
          -100,  -100,  2537,  -100, 20578,  -100,  -100,  -100,  -100,  -100,
          -100,  5061,  -100,  -100,  -100,  -100,  -100,  -100,  1036,  -100,
          -100,  -100,  -100, 11007,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(8.7280, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.3924, -6.3405, -6.3185,  ..., -5.6108, -5.4809, -3.6576],
         [-5.2678, -5.3766, -5.1568,  ..., -6.0763, -6.6135, -3.8828],
         [-7.5015, -7.2564, -7.5046,  ..., -7.6586, -6.9486, -4.7340],
         ...,
         [-6.7476, -6.8425, -6.6396,  ..., -7.1807, -6.5358, -3.3304],
         [-6.6307, -6.7359, -6.6109,  ..., -7.0126, -6.8275, -2.9960],
         [-5.7513, -6.0177, -5.6559,  ..., -6.6078, -5.2969, -3.1067]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 8.728044509887695
Raw Input IDs (before masking): tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,  2080, 16126,  2072,  2003,  1996,  3166,  1997,  1037,  4187,
          3179,  1997,  1996,  6846,  4571,  2063,  1997,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 6
Unique label values and counts: {-100: 122, 1012: 1, 2003: 1, 2072: 1, 3179: 1, 16126: 1, 25130: 1}
Unique labels in batch: tensor([ -100,  1012,  2003,  2072,  3179, 16126, 25130])
Masked Input IDs: tensor([[  101,  2128,  2497,   103,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,  2080,   103, 21244,  2003,  1996,  3166,  1997,  1037,  4187,
           103,  1997,  1996,  6846,  4571,  2063,  1997,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100, 25130,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100, 16126,  2072,  2003,  -100,  -100,  -100,  -100,  -100,
          3179,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2128,  2497,   103,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,  2080,   103, 21244,  2003,  1996,  3166,  1997,  1037,  4187,
           103,  1997,  1996,  6846,  4571,  2063,  1997,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100, 25130,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100, 16126,  2072,  2003,  -100,  -100,  -100,  -100,  -100,
          3179,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.7916, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7457,  -6.7077,  -6.7110,  ...,  -6.0818,  -5.8486,  -4.1184],
         [-12.9363, -12.3228, -12.9218,  ..., -12.2650, -12.7624,  -9.0751],
         [ -5.9127,  -5.7371,  -6.1487,  ...,  -5.7028,  -6.0170,  -4.6960],
         ...,
         [ -8.3646,  -8.0475,  -8.3734,  ...,  -8.7277,  -9.1304,  -3.7530],
         [ -8.2288,  -7.9150,  -8.3935,  ...,  -8.3863,  -8.9581,  -3.3644],
         [ -8.9368,  -8.6085,  -8.9233,  ...,  -8.8638,  -8.7391,  -5.6049]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.7915899753570557
Raw Input IDs (before masking): tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792,  2029,  2443,  3922,  2011, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,  9893, 25208,  1998,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,  2019,  2792,  2029,  2956,  8507,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048,  2367,  4178,  1999,  2029,
          2280,  8626,  1005,  7939,  9387, 12986, 17590,  1998,  8507,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([15])
Total tokens in batch: 128
Total masked tokens: 15
Percentage of masked tokens: 11.72%
Total special tokens in batch: 45
Total tokens: 128, Non-special tokens: 83, Masked tokens: 15
Unique label values and counts: {-100: 113, 1012: 1, 1037: 1, 1996: 1, 1998: 1, 2029: 1, 2280: 1, 2367: 1, 2569: 1, 2792: 1, 3678: 1, 8507: 2, 8626: 1, 9610: 1, 23993: 1}
Unique labels in batch: tensor([ -100,  1012,  1037,  1996,  1998,  2029,  2280,  2367,  2569,  2792,
         3678,  8507,  8626,  9610, 23993])
Masked Input IDs: tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,   103,
           103,  4234,  2792,   103,  2443,  3922,  2011, 29168,  2004,   103,
          1010, 23917,  2522, 12844,  1010,  9893, 25208,   103,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,  2019,   103,  2029,  2956,   103,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048, 16930,  4178,  1999,  2029,
           103,   103,  1005,  7939,  9387, 12986, 17590,  1998, 17498,   103,
         13947,  4113,  5652,   103,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1037,
          2569,  -100,  -100,  2029,  -100,  -100,  -100,  -100,  -100,  3678,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  2792,  -100,  -100,  8507,
          -100,  -100,  -100,  1996,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 23993,
          -100,  -100,  -100,  -100,  -100,  -100,  2367,  -100,  -100,  -100,
          2280,  8626,  -100,  -100,  -100,  -100,  -100,  -100,  8507,  9610,
          -100,  -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,   103,
           103,  4234,  2792,   103,  2443,  3922,  2011, 29168,  2004,   103,
          1010, 23917,  2522, 12844,  1010,  9893, 25208,   103,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,  2019,   103,  2029,  2956,   103,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048, 16930,  4178,  1999,  2029,
           103,   103,  1005,  7939,  9387, 12986, 17590,  1998, 17498,   103,
         13947,  4113,  5652,   103,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1037,
          2569,  -100,  -100,  2029,  -100,  -100,  -100,  -100,  -100,  3678,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  2792,  -100,  -100,  8507,
          -100,  -100,  -100,  1996,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 23993,
          -100,  -100,  -100,  -100,  -100,  -100,  2367,  -100,  -100,  -100,
          2280,  8626,  -100,  -100,  -100,  -100,  -100,  -100,  8507,  9610,
          -100,  -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.9131, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6819, -6.6400, -6.5622,  ..., -5.8892, -5.7193, -3.9919],
         [-6.6693, -6.7989, -6.5184,  ..., -6.8183, -6.2094, -5.4160],
         [-9.1787, -9.3948, -9.3726,  ..., -9.0254, -7.5850, -5.3937],
         ...,
         [-6.6630, -6.7067, -6.7947,  ..., -7.2734, -6.0743, -5.2256],
         [-5.7447, -5.6051, -5.4921,  ..., -5.2541, -5.4284, -3.8855],
         [-6.5740, -6.6745, -6.7386,  ..., -6.6000, -6.3138, -4.2853]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.913133382797241
Raw Input IDs (before masking): tensor([[ 101, 8856, 6868, 2003, 2019, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 121
Total tokens: 128, Non-special tokens: 7, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[  101,  8856,  6868,  2003,  2019,  2137,  6520,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0, 16338,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  8856,  6868,  2003,  2019,  2137,  6520,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0, 16338,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(14.2755, grad_fn=<NllLossBackward0>), logits=tensor([[[-7.5699, -7.5749, -7.5027,  ..., -6.6864, -6.4394, -4.9638],
         [-6.6976, -7.0184, -7.1369,  ..., -6.7844, -5.7490, -4.8988],
         [-6.0396, -6.0681, -6.0440,  ..., -5.9382, -5.4745, -7.0201],
         ...,
         [-4.7557, -4.8834, -4.8925,  ..., -5.1934, -5.1050, -3.7649],
         [-6.1619, -6.2768, -6.3760,  ..., -7.3666, -6.7852, -2.6759],
         [-5.9859, -6.1115, -6.1940,  ..., -6.3490, -6.2312, -4.1980]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 14.27554702758789
Raw Input IDs (before masking): tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029, 23622,  1010,
          3416, 11017,  1997,  9198,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 126, 9465: 1, 23622: 1}
Unique labels in batch: tensor([ -100,  9465, 23622])
Masked Input IDs: tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029,   103,  1010,
          3416, 11017,  1997,  9198,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  9465,  -100,  -100,  -100,  -100,  -100,  -100, 23622,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029,   103,  1010,
          3416, 11017,  1997,  9198,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  9465,  -100,  -100,  -100,  -100,  -100,  -100, 23622,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.7765, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6468,  -6.5931,  -6.5975,  ...,  -6.1955,  -5.8487,  -4.1895],
         [ -4.4107,  -4.5794,  -4.6013,  ...,  -4.7315,  -4.8695,  -4.1815],
         [-12.0302, -11.9532, -12.5336,  ..., -13.5167,  -8.5793,  -9.8501],
         ...,
         [ -6.0602,  -6.1563,  -6.0840,  ...,  -6.8547,  -6.5249,  -4.3661],
         [ -5.5781,  -5.6148,  -5.6219,  ...,  -6.4690,  -5.7044,  -4.0227],
         [ -6.6132,  -6.7362,  -6.7329,  ...,  -7.5353,  -7.2152,  -3.9320]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.776503086090088
Raw Input IDs (before masking): tensor([[  101,  6382, 10093,  3877,  1011,  1011,  2209,  1996,  2610,  2961,
          6898,  1997, 28517,  6216,  1010, 12270,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 112
Total tokens: 128, Non-special tokens: 16, Masked tokens: 4
Unique label values and counts: {-100: 124, 1011: 1, 1012: 1, 6898: 1, 28517: 1}
Unique labels in batch: tensor([ -100,  1011,  1012,  6898, 28517])
Masked Input IDs: tensor([[  101,  6382, 10093,  3877,   103,  1011,  2209,  1996,  2610,  2961,
           103,  1997,   103,  6216,  1010, 12270,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  1011,  -100,  -100,  -100,  -100,  -100,
          6898,  -100, 28517,  -100,  -100,  -100,  1012,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6382, 10093,  3877,   103,  1011,  2209,  1996,  2610,  2961,
           103,  1997,   103,  6216,  1010, 12270,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  1011,  -100,  -100,  -100,  -100,  -100,
          6898,  -100, 28517,  -100,  -100,  -100,  1012,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.4712, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8311,  -6.8134,  -6.7931,  ...,  -6.0035,  -6.0104,  -4.0621],
         [ -8.5748,  -8.7209,  -8.8642,  ...,  -8.2116,  -7.5296, -10.3953],
         [-12.8785, -12.7984, -12.7775,  ..., -12.1002,  -9.3140, -12.6411],
         ...,
         [ -6.8177,  -6.9993,  -6.9821,  ...,  -6.5534,  -8.2648,  -5.1093],
         [ -8.1459,  -8.1685,  -8.2117,  ...,  -7.9333,  -7.3470,  -3.4270],
         [ -7.6144,  -7.7721,  -7.7391,  ...,  -7.7341,  -6.9852,  -4.5760]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.4711570739746094
Raw Input IDs (before masking): tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,  2002,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 105
Total tokens: 128, Non-special tokens: 23, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,  2002,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0, 10226,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,  2002,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0, 10226,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(14.1431, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8518, -6.8046, -6.7793,  ..., -5.9603, -5.8438, -4.1303],
         [-8.6852, -8.6599, -8.6889,  ..., -8.1044, -7.3531, -5.7283],
         [-5.7508, -6.3073, -6.3871,  ..., -7.0116, -5.3130, -3.3101],
         ...,
         [-6.3018, -6.3729, -6.2076,  ..., -6.5715, -6.1869, -6.2055],
         [-7.6178, -7.6943, -7.7597,  ..., -8.0114, -7.6526, -6.4943],
         [-7.1967, -7.1613, -7.2618,  ..., -7.6116, -7.4485, -6.0963]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 14.143096923828125
Raw Input IDs (before masking): tensor([[  101,  6868,  2864,  2005,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,  1996,  2034,  2305,  1997, 21028,  1005,  5585,  1010,  1998,
          2001,  1996,  2034,  3287,  6520,  2956,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 4
Unique label values and counts: {-100: 124, 1998: 1, 2034: 2, 6520: 1}
Unique labels in batch: tensor([-100, 1998, 2034, 6520])
Masked Input IDs: tensor([[  101,  6868,  2864,  2005,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,  1996,   103,  2305,  1997, 21028,  1005,  5585,  1010,   103,
          2001,  1996,   103,  3287,   103,  2956,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2034, -100, -100, -100, -100, -100, -100, 1998, -100, -100, 2034, -100,
         6520, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6868,  2864,  2005,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,  1996,   103,  2305,  1997, 21028,  1005,  5585,  1010,   103,
          2001,  1996,   103,  3287,   103,  2956,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2034, -100, -100, -100, -100, -100, -100, 1998, -100, -100, 2034, -100,
         6520, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.0252, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9335, -6.9193, -6.8867,  ..., -6.0374, -6.0292, -3.7124],
         [-6.2589, -6.2913, -6.1048,  ..., -6.1425, -5.7138, -6.6475],
         [-3.0697, -2.9719, -3.4527,  ..., -2.7755, -2.6913, -1.8358],
         ...,
         [-7.8964, -7.9246, -7.8504,  ..., -7.8684, -8.1657, -3.3379],
         [-7.6295, -7.8351, -7.4641,  ..., -7.8202, -6.9186, -5.8779],
         [-8.1706, -8.2555, -8.0685,  ..., -8.4068, -7.2541, -5.9890]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.0252323150634766
Raw Input IDs (before masking): tensor([[  101,  6609,  2001,  3532,  1998,  4242,  2000, 17798,  1010,  2018,
          1037,  2365,  2170, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 3
Unique label values and counts: {-100: 125, 2000: 1, 6790: 1, 17798: 1}
Unique labels in batch: tensor([ -100,  2000,  6790, 17798])
Masked Input IDs: tensor([[  101,  6609,  2001,  3532,  1998,  4242, 17519, 15435,  1010,  2018,
          1037,  2365,  2170, 14916,   103,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  2000, 17798,  -100,  -100,
          -100,  -100,  -100,  -100,  6790,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6609,  2001,  3532,  1998,  4242, 17519, 15435,  1010,  2018,
          1037,  2365,  2170, 14916,   103,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  2000, 17798,  -100,  -100,
          -100,  -100,  -100,  -100,  6790,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(6.0336, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7536,  -6.6989,  -6.7179,  ...,  -6.2296,  -6.0235,  -4.1143],
         [ -6.4104,  -6.4171,  -6.6440,  ...,  -6.0588,  -5.7639,  -5.9203],
         [-14.4059, -14.2019, -14.5814,  ..., -14.6169, -11.6931, -11.0479],
         ...,
         [ -3.8710,  -3.8306,  -3.9777,  ...,  -4.5300,  -3.9401,  -1.2780],
         [ -6.7905,  -6.8098,  -6.7862,  ...,  -7.1136,  -6.1284,  -5.2277],
         [ -5.4234,  -5.4738,  -5.4689,  ...,  -5.6789,  -5.3754,  -4.3399]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 6.03361701965332
Raw Input IDs (before masking): tensor([[  101,  4894,  2038,  2405,  2048,  2573,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,  4897,  2534,  1997,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013,  2682,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 91
Total tokens: 128, Non-special tokens: 37, Masked tokens: 5
Unique label values and counts: {-100: 123, 1997: 2, 2048: 1, 2234: 1, 4894: 1}
Unique labels in batch: tensor([-100, 1997, 2048, 2234, 4894])
Masked Input IDs: tensor([[  101,  8087,  2038,  2405,   103,  2573,   103,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,  4897,  2534,  7768,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,   103,  2013,  2682,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 4894, -100, -100, 2048, -100, 1997, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 1997, -100, -100, -100, -100, -100, -100, -100, -100, 2234, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  8087,  2038,  2405,   103,  2573,   103,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,  4897,  2534,  7768,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,   103,  2013,  2682,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 4894, -100, -100, 2048, -100, 1997, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 1997, -100, -100, -100, -100, -100, -100, -100, -100, 2234, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.9536, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9099, -6.8813, -6.8735,  ..., -6.1073, -5.9761, -3.7565],
         [-4.8259, -4.8321, -4.9545,  ..., -4.1112, -5.8103, -2.4507],
         [-7.9801, -7.5903, -8.0362,  ..., -8.0187, -5.7244, -3.5889],
         ...,
         [-6.8812, -6.8910, -6.8365,  ..., -6.7874, -7.1519, -4.0893],
         [-8.0863, -8.0874, -8.0411,  ..., -7.7318, -8.3235, -3.0560],
         [-7.1346, -7.2745, -7.1879,  ..., -7.2095, -7.8977, -2.9025]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.953604221343994
Raw Input IDs (before masking): tensor([[  101,  2077,  2014,  3510,  1010,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,  2232, 22142,  3753,  2011,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,  1011,  2375,  1999,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([12])
Total tokens in batch: 128
Total masked tokens: 12
Percentage of masked tokens: 9.38%
Total special tokens in batch: 86
Total tokens: 128, Non-special tokens: 42, Masked tokens: 12
Unique label values and counts: {-100: 116, 1011: 1, 1996: 1, 1999: 2, 2014: 1, 2016: 1, 3273: 1, 7907: 1, 13433: 1, 14021: 1, 15904: 1, 26132: 1}
Unique labels in batch: tensor([ -100,  1011,  1996,  1999,  2014,  2016,  3273,  7907, 13433, 14021,
        15904, 26132])
Masked Input IDs: tensor([[  101,  2077,   103,  3510,  1010,   103, 16126,  2072,   103,   103,
           103,   103,  2638,   103,  2232, 22142,  3753,  2011,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,   103,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,   103,  2375,   103,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  2014,  -100,  -100,  7907,  -100,  -100,  3273,  1999,
          1996, 13433,  -100, 26132,  -100,  -100,  -100,  -100,  -100, 14021,
          -100,  -100,  -100,  -100, 15904,  -100,  -100,  2016,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  1011,  -100,  1999,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2077,   103,  3510,  1010,   103, 16126,  2072,   103,   103,
           103,   103,  2638,   103,  2232, 22142,  3753,  2011,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,   103,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,   103,  2375,   103,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  2014,  -100,  -100,  7907,  -100,  -100,  3273,  1999,
          1996, 13433,  -100, 26132,  -100,  -100,  -100,  -100,  -100, 14021,
          -100,  -100,  -100,  -100, 15904,  -100,  -100,  2016,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  1011,  -100,  1999,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.2034, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7849, -6.7766, -6.7749,  ..., -6.1812, -6.0148, -4.0729],
         [-8.0781, -7.8744, -8.1315,  ..., -8.4618, -7.7931, -4.8348],
         [-6.0502, -6.3280, -6.1806,  ..., -6.2143, -3.5595, -6.2664],
         ...,
         [-7.9954, -7.7036, -8.2173,  ..., -7.3547, -7.4828, -4.1667],
         [-6.9439, -6.8547, -7.1993,  ..., -7.0223, -7.3855, -4.6304],
         [-6.8577, -6.8595, -7.0701,  ..., -7.8279, -8.0862, -3.9927]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.2034342288970947
Raw Input IDs (before masking): tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,  1998,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,  2062,  1997,  1996,  2168,  2013,
          2019,  3063,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 87
Total tokens: 128, Non-special tokens: 41, Masked tokens: 5
Unique label values and counts: {-100: 123, 1010: 1, 1997: 1, 1998: 1, 2078: 1, 20799: 1}
Unique labels in batch: tensor([ -100,  1010,  1997,  1998,  2078, 20799])
Masked Input IDs: tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226,   103,
          7021,  1996,  2201,  2004,  5675,  2594,   103,  1036,  1036, 17824,
          2135,   103,  2411, 25198,  2135,  2062, 11247,  1996,  2168,  2013,
          2019,  3063,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2078,  -100, 20799,
          -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,
          -100,  1010,  -100,  -100,  -100,  -100,  1997,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226,   103,
          7021,  1996,  2201,  2004,  5675,  2594,   103,  1036,  1036, 17824,
          2135,   103,  2411, 25198,  2135,  2062, 11247,  1996,  2168,  2013,
          2019,  3063,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2078,  -100, 20799,
          -100,  -100,  -100,  -100,  -100,  -100,  1998,  -100,  -100,  -100,
          -100,  1010,  -100,  -100,  -100,  -100,  1997,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.8552, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.4198, -6.3580, -6.3423,  ..., -5.6527, -5.4842, -3.4029],
         [-5.7045, -5.7192, -5.4819,  ..., -6.3001, -5.8020, -3.6196],
         [-9.4809, -9.5610, -9.2589,  ..., -7.3401, -9.5368, -5.4143],
         ...,
         [-6.1862, -6.0422, -6.3078,  ..., -6.3586, -5.8594, -1.8049],
         [-5.9037, -5.8833, -6.0760,  ..., -5.8796, -5.5949, -2.4607],
         [-6.8390, -6.7910, -6.8409,  ..., -6.3633, -6.0916, -3.5130]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.8552341461181641
Raw Input IDs (before masking): tensor([[  101,  2010,  2388,  2001,  2019, 25244,  1036,  1036,  1997,  4635,
          1998,  3226,  1005,  1005,  1998,  2010,  2269,  2001,  1037,  2489,
          2158,  1997,  3609,  1010,  2649,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 4
Unique label values and counts: {-100: 124, 1005: 1, 2001: 1, 2649: 1, 25244: 1}
Unique labels in batch: tensor([ -100,  1005,  2001,  2649, 25244])
Masked Input IDs: tensor([[  101,  2010,  2388,  2001,  2019,   103,  1036,  1036,  1997,  4635,
          1998,  3226,   103,  1005,  1998,  2010,  2269,  3686,  1037,  2489,
          2158,  1997,  3609,  1010,   103,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100, 25244,  -100,  -100,  -100,  -100,
          -100,  -100,  1005,  -100,  -100,  -100,  -100,  2001,  -100,  -100,
          -100,  -100,  -100,  -100,  2649,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2010,  2388,  2001,  2019,   103,  1036,  1036,  1997,  4635,
          1998,  3226,   103,  1005,  1998,  2010,  2269,  3686,  1037,  2489,
          2158,  1997,  3609,  1010,   103,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100, 25244,  -100,  -100,  -100,  -100,
          -100,  -100,  1005,  -100,  -100,  -100,  -100,  2001,  -100,  -100,
          -100,  -100,  -100,  -100,  2649,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.9560, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5689,  -6.5300,  -6.5238,  ...,  -6.1173,  -5.8856,  -3.9902],
         [-12.2292, -12.4540, -13.0371,  ..., -12.8296, -11.0172,  -8.5901],
         [ -8.5579,  -8.2276,  -8.5209,  ...,  -8.9532,  -6.7009,  -3.2524],
         ...,
         [ -8.0037,  -8.0177,  -8.1268,  ...,  -8.6379,  -8.1962,  -6.0880],
         [ -6.8811,  -6.9865,  -7.0551,  ...,  -7.4353,  -6.7521,  -5.6767],
         [ -7.8482,  -7.8221,  -8.0907,  ...,  -8.1559,  -7.5186,  -5.9836]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.956034541130066
Raw Input IDs (before masking): tensor([[  101,  4635,  2038,  2550,  3365,  3152,  1006,  2104,  2014,  2613,
          2171,  1010, 12903, 29062,  1007,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 105
Total tokens: 128, Non-special tokens: 23, Masked tokens: 3
Unique label values and counts: {-100: 125, 1007: 1, 2104: 1, 12903: 1}
Unique labels in batch: tensor([ -100,  1007,  2104, 12903])
Masked Input IDs: tensor([[  101,  4635,  2038,  2550,  3365,  3152,  1006,   103,  2014,  2613,
          2171,  1010, 12903, 29062,   103,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2104,  -100,  -100,
          -100,  -100, 12903,  -100,  1007,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  4635,  2038,  2550,  3365,  3152,  1006,   103,  2014,  2613,
          2171,  1010, 12903, 29062,   103,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2104,  -100,  -100,
          -100,  -100, 12903,  -100,  1007,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1509, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7973, -6.8024, -6.7858,  ..., -6.0926, -5.7974, -4.1719],
         [-4.7302, -4.7186, -4.8566,  ..., -4.8967, -4.6109, -5.8012],
         [-6.7350, -6.7741, -7.2481,  ..., -5.6137, -4.6706, -3.1472],
         ...,
         [-6.2452, -6.4216, -6.3950,  ..., -6.6254, -5.8514, -4.6145],
         [-5.8432, -5.9435, -5.9859,  ..., -6.2383, -5.4886, -4.5622],
         [-6.4216, -6.5804, -6.7279,  ..., -6.8364, -5.8438, -5.6687]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.15093860030174255
Raw Input IDs (before masking): tensor([[  101,  2010,  4203, 10768,  3850,  2834,  1999,  2384,  2001,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,  3669,  5558,
          2615,  1005,  1055,  7110,  8447,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 102
Total tokens: 128, Non-special tokens: 26, Masked tokens: 6
Unique label values and counts: {-100: 122, 2001: 1, 2384: 1, 2615: 1, 3669: 1, 3850: 1, 7110: 1}
Unique labels in batch: tensor([-100, 2001, 2384, 2615, 3669, 3850, 7110])
Masked Input IDs: tensor([[  101,  2010,  4203, 10768,   103,  2834,  1999,   103,   103,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,   103,  5558,
           103,  1005,  1055,   103,  8447,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 3850, -100, -100, 2384, 2001, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 3669, -100, 2615, -100, -100, 7110,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2010,  4203, 10768,   103,  2834,  1999,   103,   103,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,   103,  5558,
           103,  1005,  1055,   103,  8447,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 3850, -100, -100, 2384, 2001, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 3669, -100, 2615, -100, -100, 7110,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.7420, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7807,  -6.7213,  -6.7491,  ...,  -5.9300,  -5.9631,  -3.7214],
         [ -7.2988,  -7.3062,  -7.5555,  ...,  -6.6273,  -8.0616,  -4.5642],
         [-10.6850, -10.9165, -10.7027,  ...,  -9.5882,  -9.6765,  -8.0835],
         ...,
         [ -7.3713,  -7.2224,  -7.4306,  ...,  -7.3344,  -8.3836,  -2.6719],
         [ -7.3407,  -7.2856,  -7.4529,  ...,  -7.2543,  -8.4915,  -4.4181],
         [ -7.4721,  -7.4782,  -7.5246,  ...,  -7.1211,  -7.8832,  -5.1016]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.742012023925781
Raw Input IDs (before masking): tensor([[  101,  2014,  2152,  2082,  2420,  2020,  2985,  2012,  2047, 25487,
          2152,  2082,  1999,  2663,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 5
Unique label values and counts: {-100: 123, 1999: 1, 2014: 1, 2047: 1, 2663: 1, 2912: 1}
Unique labels in batch: tensor([-100, 1999, 2014, 2047, 2663, 2912])
Masked Input IDs: tensor([[  101,   103,  2152,  2082,  2420,  2020,  2985,  2012,   103, 25487,
          2152,  2082,   103,   103,  7159,   103,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 2014, -100, -100, -100, -100, -100, -100, 2047, -100, -100, -100,
         1999, 2663, -100, 2912, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,  2152,  2082,  2420,  2020,  2985,  2012,   103, 25487,
          2152,  2082,   103,   103,  7159,   103,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 2014, -100, -100, -100, -100, -100, -100, 2047, -100, -100, -100,
         1999, 2663, -100, 2912, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.7010, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6893, -6.6632, -6.6538,  ..., -6.0412, -5.7607, -4.0467],
         [-5.4133, -5.4342, -5.3340,  ..., -5.4334, -5.6925, -5.5586],
         [-9.1512, -9.5756, -9.3917,  ..., -9.9256, -7.6783, -6.1161],
         ...,
         [-6.7879, -6.5615, -6.8264,  ..., -5.0551, -7.1836, -6.7685],
         [-7.1835, -7.5016, -7.4788,  ..., -6.5540, -5.6134, -9.1251],
         [-6.8252, -6.6866, -6.9093,  ..., -6.1046, -6.5842, -7.8875]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.7009565830230713
Raw Input IDs (before masking): tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,  1010,  5076,  7301,
          1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 2
Unique label values and counts: {-100: 126, 1010: 1, 11136: 1}
Unique labels in batch: tensor([ -100,  1010, 11136])
Masked Input IDs: tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,   103,  5076,  7301,
          1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340,   103,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 11136,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,   103,  5076,  7301,
          1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340,   103,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 11136,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.5114, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7131, -6.6895, -6.6726,  ..., -6.0610, -5.8215, -3.9884],
         [-6.7492, -6.7803, -6.9309,  ..., -7.0280, -6.8101, -5.2023],
         [-4.2882, -4.2898, -4.2109,  ..., -5.1798, -5.1693, -4.7004],
         ...,
         [-7.5126, -7.5555, -7.8336,  ..., -7.6140, -7.9037, -3.7283],
         [-6.5517, -6.7417, -6.7443,  ..., -6.6577, -6.4485, -5.1080],
         [-6.6705, -6.8807, -6.9359,  ..., -6.9453, -6.1866, -4.9390]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.5113766193389893
Raw Input IDs (before masking): tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,  8607, 13793,  2097,
          2022,  2999,  2011,  2280, 14757,  2368,  4062,  3419, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 100
Total tokens: 128, Non-special tokens: 28, Masked tokens: 5
Unique label values and counts: {-100: 123, 2022: 1, 2044: 1, 2050: 1, 2097: 1, 4062: 1}
Unique labels in batch: tensor([-100, 2022, 2044, 2050, 2097, 4062])
Masked Input IDs: tensor([[  101,  2396,   103,  4062, 19300, 15104,  3695,  8607, 13793,   103,
           103,  2999,  2011,  2280, 14757,  2368,   103,  3419, 23212,  2053,
          4478,  3089,   103,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2050, -100, -100, -100, -100, -100, -100, 2097, 2022, -100,
         -100, -100, -100, -100, 4062, -100, -100, -100, -100, -100, 2044, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2396,   103,  4062, 19300, 15104,  3695,  8607, 13793,   103,
           103,  2999,  2011,  2280, 14757,  2368,   103,  3419, 23212,  2053,
          4478,  3089,   103,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2050, -100, -100, -100, -100, -100, -100, 2097, 2022, -100,
         -100, -100, -100, -100, 4062, -100, -100, -100, -100, -100, 2044, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.6177, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9191, -6.8733, -6.8748,  ..., -5.9705, -6.0406, -4.0618],
         [-7.7969, -7.8739, -7.7075,  ..., -6.7739, -7.1895, -5.2629],
         [-4.2770, -4.6869, -4.6310,  ..., -3.1116, -4.4959, -4.4067],
         ...,
         [-6.9564, -6.8989, -6.9025,  ..., -5.6075, -6.5872, -4.8474],
         [-4.8203, -4.9208, -4.7497,  ..., -4.6414, -5.9076, -2.6210],
         [-7.3458, -7.3142, -7.2378,  ..., -6.0754, -7.1945, -4.1433]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.617748260498047
Raw Input IDs (before masking): tensor([[ 101, 2002, 2109, 2137, 6443, 2000, 2507, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 117
Total tokens: 128, Non-special tokens: 11, Masked tokens: 2
Unique label values and counts: {-100: 126, 2507: 1, 6443: 1}
Unique labels in batch: tensor([-100, 2507, 6443])
Masked Input IDs: tensor([[ 101, 2002, 2109, 2137,  103, 2000,  103, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, 6443, -100, 2507, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 2002, 2109, 2137,  103, 2000,  103, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 6443, -100, 2507, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.2261, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8446, -6.8216, -6.8089,  ..., -6.0965, -5.9450, -3.9939],
         [-7.3524, -7.2203, -7.3109,  ..., -6.5492, -6.4124, -4.5664],
         [-6.1140, -5.8054, -5.8451,  ..., -5.1017, -3.2343, -4.6938],
         ...,
         [-5.0475, -5.1844, -5.1066,  ..., -5.4993, -5.3263, -2.5086],
         [-6.9323, -6.8778, -7.0623,  ..., -6.9466, -7.5393, -2.2186],
         [-6.5268, -6.5979, -6.6732,  ..., -6.5861, -6.2382, -3.3678]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.226143836975098
Raw Input IDs (before masking): tensor([[ 101, 2542, 2007, 2010, 4470, 2198, 3255, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 2
Unique label values and counts: {-100: 126, 1012: 1, 3881: 1}
Unique labels in batch: tensor([-100, 1012, 3881])
Masked Input IDs: tensor([[  101,  2542,  2007,  2010,  4470,  2198,  3255,  2063,  1010,   103,
          3273,  2005,  2055,  2702,  2086, 20666,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 3881, -100, -100,
         -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2542,  2007,  2010,  4470,  2198,  3255,  2063,  1010,   103,
          3273,  2005,  2055,  2702,  2086, 20666,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 3881, -100, -100,
         -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.0875, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.3826,  -6.3544,  -6.2968,  ...,  -5.8144,  -5.6194,  -3.9195],
         [ -6.3939,  -6.7456,  -6.5675,  ...,  -6.3925,  -5.1774,  -7.5299],
         [-11.9657, -12.1069, -12.1562,  ..., -10.5838, -10.5455,  -8.6595],
         ...,
         [ -7.8434,  -7.7185,  -7.9573,  ...,  -7.1067,  -8.1125,  -2.0810],
         [ -6.7924,  -6.9110,  -6.8618,  ...,  -7.1892,  -7.1598,  -5.9600],
         [ -7.5144,  -7.6322,  -7.5838,  ...,  -8.0047,  -7.7082,  -6.0653]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.087533950805664
Raw Input IDs (before masking): tensor([[  101,  2014,  3722,  1010,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478, 10654,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 101
Total tokens: 128, Non-special tokens: 27, Masked tokens: 1
Unique label values and counts: {-100: 127, 10654: 1}
Unique labels in batch: tensor([ -100, 10654])
Masked Input IDs: tensor([[  101,  2014,  3722,  1010,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478,   103,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10654,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014,  3722,  1010,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478,   103,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10654,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.7494, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8681, -6.8767, -6.8368,  ..., -6.2295, -5.9915, -4.1891],
         [-9.3309, -9.0918, -9.2024,  ..., -9.5574, -7.7583, -5.2679],
         [-6.1216, -6.2052, -6.4462,  ..., -5.1562, -3.2879, -6.3764],
         ...,
         [-6.3553, -6.3821, -6.3414,  ..., -6.9418, -7.0372, -2.6382],
         [-7.7157, -7.6439, -7.8140,  ..., -8.0863, -8.8745, -1.2243],
         [-7.5661, -7.4547, -7.4750,  ..., -8.0139, -7.0756, -2.3672]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.749418258666992
Raw Input IDs (before masking): tensor([[  101,  2014, 11062,  2307,  1011,  7133,  2001, 12903, 19330,  2050,
          1010,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,  1010, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 7
Unique label values and counts: {-100: 121, 1997: 1, 3129: 1, 11062: 1, 11716: 1, 12903: 1, 19330: 1, 21696: 1}
Unique labels in batch: tensor([ -100,  1997,  3129, 11062, 11716, 12903, 19330, 21696])
Masked Input IDs: tensor([[  101,  2014,   103,  2307,  1011,  7133,  2001,   103,   103,  2050,
          1010,  4343,  4656,   103, 16205,  1010,  3005,   103,  2001,   103,
         28016,  1010,   103,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100, 11062,  -100,  -100,  -100,  -100, 12903, 19330,  -100,
          -100,  -100,  -100,  1997,  -100,  -100,  -100,  3129,  -100, 21696,
          -100,  -100, 11716,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014,   103,  2307,  1011,  7133,  2001,   103,   103,  2050,
          1010,  4343,  4656,   103, 16205,  1010,  3005,   103,  2001,   103,
         28016,  1010,   103,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100, 11062,  -100,  -100,  -100,  -100, 12903, 19330,  -100,
          -100,  -100,  -100,  1997,  -100,  -100,  -100,  3129,  -100, 21696,
          -100,  -100, 11716,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.0284, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.3186, -6.2663, -6.2766,  ..., -5.7524, -5.2815, -3.9588],
         [-6.7360, -6.9171, -6.8795,  ..., -7.6385, -4.3492, -5.9615],
         [-3.6830, -3.6996, -3.8728,  ..., -4.7343, -2.5520, -2.4599],
         ...,
         [-7.2733, -7.3053, -7.4806,  ..., -8.2737, -7.8072, -3.9602],
         [-6.2767, -6.3708, -6.3893,  ..., -6.7414, -5.8417, -4.8233],
         [-6.0090, -6.1976, -6.1658,  ..., -6.5461, -6.2485, -4.3028]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.028406620025635
Raw Input IDs (before masking): tensor([[  101,  2002,  2288,  2010,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,  1999,  3050,  3349,  1010,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 3
Unique label values and counts: {-100: 125, 1010: 1, 1999: 1, 2010: 1}
Unique labels in batch: tensor([-100, 1010, 1999, 2010])
Masked Input IDs: tensor([[  101,  2002,  2288,   103,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,   103,  3050,  3349, 22176,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 2010, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1999,
         -100, -100, 1010, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  2288,   103,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,   103,  3050,  3349, 22176,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2010, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1999,
         -100, -100, 1010, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.1787, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5938, -6.5899, -6.5618,  ..., -5.9363, -5.8409, -3.8916],
         [-8.7071, -8.7552, -8.9620,  ..., -7.6860, -8.0043, -5.9032],
         [-8.7360, -8.8565, -8.8703,  ..., -8.5743, -7.0443, -9.4886],
         ...,
         [-5.9120, -5.9695, -5.9903,  ..., -6.2998, -6.2861, -3.9956],
         [-7.3727, -7.3088, -7.6010,  ..., -6.9560, -7.6301, -3.3213],
         [-7.0411, -7.2607, -7.2742,  ..., -7.0879, -6.7969, -6.9085]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.178650379180908
Raw Input IDs (before masking): tensor([[  101,  2002,  3728,  2038,  2207,  2048,  3729, 21109,  2104,  6253,
          6116,  2368, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 125, 1005: 1, 3729: 1, 3819: 1}
Unique labels in batch: tensor([-100, 1005, 3729, 3819])
Masked Input IDs: tensor([[  101,  2002,  3728,  2038,  2207,  2048,   103, 21109,  2104,  6253,
          6116,  2368, 10371,   103,  1055,   103,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, 3729, -100, -100, -100, -100, -100,
         -100, 1005, -100, 3819, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  3728,  2038,  2207,  2048,   103, 21109,  2104,  6253,
          6116,  2368, 10371,   103,  1055,   103,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, 3729, -100, -100, -100, -100, -100,
         -100, 1005, -100, 3819, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.9951, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7373,  -6.7053,  -6.6940,  ...,  -5.9863,  -5.7864,  -3.7645],
         [-10.9230, -11.1892, -11.4313,  ...,  -9.6874, -10.6078,  -8.4905],
         [ -3.2422,  -3.2182,  -3.4456,  ...,  -1.5475,  -3.1906,  -5.0458],
         ...,
         [ -7.4949,  -7.5953,  -7.6929,  ...,  -7.7943,  -8.3701,  -4.4987],
         [ -5.5358,  -5.4958,  -5.5318,  ...,  -6.6425,  -5.8891,  -3.2255],
         [ -7.6432,  -7.7663,  -7.7579,  ...,  -7.6431,  -7.4755,  -4.1376]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.9950950145721436
Raw Input IDs (before masking): tensor([[  101,  2002,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,  8256,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 125, 2001: 1, 2002: 1, 8256: 1}
Unique labels in batch: tensor([-100, 2001, 2002, 8256])
Masked Input IDs: tensor([[  101,   103,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,   103,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 2002, 2001, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 8256, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,   103,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 2002, 2001, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 8256, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.7541, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6701, -6.6279, -6.6249,  ..., -6.1200, -5.7414, -4.1439],
         [-4.6159, -4.3841, -4.5734,  ..., -5.3336, -5.2033, -5.9609],
         [-9.5467, -9.3363, -9.7586,  ..., -9.4566, -5.7816, -9.2240],
         ...,
         [-4.5531, -4.5790, -4.6967,  ..., -4.3291, -3.6896, -3.4242],
         [-7.3805, -7.4098, -7.6303,  ..., -8.8525, -7.5732, -6.0205],
         [-6.4742, -6.3870, -6.5939,  ..., -7.4584, -6.9114, -7.4074]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.7541460990905762
Raw Input IDs (before masking): tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,  2021,  5295,
          1999,  2901,  2000,  5138,  1037,  2695,  2004,  6059,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 108
Total tokens: 128, Non-special tokens: 20, Masked tokens: 2
Unique label values and counts: {-100: 126, 2021: 1, 6059: 1}
Unique labels in batch: tensor([-100, 2021, 6059])
Masked Input IDs: tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,   103,  5295,
          1999,  2901,  2000,  5138,  1037,  2695,  2004,   103,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 2021, -100, -100, -100,
         -100, -100, -100, -100, -100, 6059, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,   103,  5295,
          1999,  2901,  2000,  5138,  1037,  2695,  2004,   103,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 2021, -100, -100, -100,
         -100, -100, -100, -100, -100, 6059, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0569, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7364, -6.6772, -6.6756,  ..., -5.9824, -5.9137, -4.0119],
         [-7.2271, -7.1140, -7.1877,  ..., -7.4904, -7.9914, -2.0487],
         [-7.5541, -7.6161, -7.6689,  ..., -6.8867, -5.7177, -5.7887],
         ...,
         [-6.3046, -6.5693, -6.4972,  ..., -6.6912, -7.7887, -4.4750],
         [-4.0592, -4.4344, -4.2276,  ..., -3.4370, -5.2344, -4.2574],
         [-5.0578, -5.4691, -5.3590,  ..., -4.4999, -5.9394, -3.9643]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.056908056139945984
Raw Input IDs (before masking): tensor([[  101,  2044,  2086,  1997,  2108,  2007,  2613,  3868,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,  2011,  2280,  2136, 28919, 27605, 10422,
          4062,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 92
Total tokens: 128, Non-special tokens: 36, Masked tokens: 6
Unique label values and counts: {-100: 122, 2000: 1, 2011: 1, 2108: 1, 10422: 1, 14163: 1, 18334: 1}
Unique labels in batch: tensor([ -100,  2000,  2011,  2108, 10422, 14163, 18334])
Masked Input IDs: tensor([[  101,  2044,  2086,  1997,   103,  2007,  2613,  3868,  1010, 21256,
          6182,   103,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,   103,  2280,  2136, 28919, 27605,   103,
          4062,  5342,  3211,   103,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  2108,  -100,  -100,  -100,  -100,  2000,
          -100, 18334,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  2011,  -100,  -100,  -100,  -100, 10422,
          -100,  -100,  -100, 14163,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2044,  2086,  1997,   103,  2007,  2613,  3868,  1010, 21256,
          6182,   103,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,   103,  2280,  2136, 28919, 27605,   103,
          4062,  5342,  3211,   103,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  2108,  -100,  -100,  -100,  -100,  2000,
          -100, 18334,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  2011,  -100,  -100,  -100,  -100, 10422,
          -100,  -100,  -100, 14163,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.4582, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5346, -6.4896, -6.4904,  ..., -5.8370, -5.6766, -3.8624],
         [-8.9288, -9.1286, -9.3963,  ..., -7.6258, -8.3393, -6.8538],
         [-9.2099, -9.8205, -9.9998,  ..., -7.7186, -8.4633, -6.6382],
         ...,
         [-7.8333, -7.8644, -8.0902,  ..., -7.8961, -8.3338, -3.3116],
         [-7.6031, -7.8220, -7.9205,  ..., -7.8223, -7.4460, -3.7663],
         [-7.9456, -8.1182, -8.2765,  ..., -7.6342, -7.2947, -4.0060]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.458198308944702
Raw Input IDs (before masking): tensor([[ 101, 1996, 2783, 2372, 1997, 4126, 2031, 2036, 2864, 1999, 2624, 3799,
         2104, 1996, 2316, 2171, 1005, 1005, 6556, 7193, 1036, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 106
Total tokens: 128, Non-special tokens: 22, Masked tokens: 1
Unique label values and counts: {-100: 127, 1005: 1}
Unique labels in batch: tensor([-100, 1005])
Masked Input IDs: tensor([[ 101, 1996, 2783, 2372, 1997, 4126, 2031, 2036, 2864, 1999, 2624, 3799,
         2104, 1996, 2316, 2171, 1005,  103, 6556, 7193, 1036, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 1005, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 1996, 2783, 2372, 1997, 4126, 2031, 2036, 2864, 1999, 2624, 3799,
         2104, 1996, 2316, 2171, 1005,  103, 6556, 7193, 1036, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 1005, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.9076, grad_fn=<NllLossBackward0>), logits=tensor([[[ -7.0120,  -7.0381,  -7.0508,  ...,  -6.2007,  -6.5964,  -3.5471],
         [-16.9329, -16.7098, -17.1443,  ..., -13.1218, -14.2668, -13.1696],
         [ -9.0528,  -8.7584,  -8.8879,  ...,  -5.9018,  -7.1852,  -4.4818],
         ...,
         [ -7.8240,  -7.7817,  -7.9164,  ...,  -7.3824,  -7.6453,  -5.4417],
         [ -7.2770,  -7.2546,  -7.4048,  ...,  -7.3098,  -7.2985,  -4.8513],
         [ -7.5602,  -7.5091,  -7.7041,  ...,  -7.4708,  -8.1325,  -3.4225]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.9076036810874939
Raw Input IDs (before masking): tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,  1997,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,  2025,  2464,  2153,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 104
Total tokens: 128, Non-special tokens: 24, Masked tokens: 5
Unique label values and counts: {-100: 123, 1996: 1, 2153: 1, 2186: 1, 2464: 1, 6216: 1}
Unique labels in batch: tensor([-100, 1996, 2153, 2186, 2464, 6216])
Masked Input IDs: tensor([[  101, 14019,  2011, 28517,   103,  1999,   103,  2345,  2792,  1997,
           103,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,  2025, 21548,   103,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 6216, -100, 1996, -100, -100, -100, 2186, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2464, 2153,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14019,  2011, 28517,   103,  1999,   103,  2345,  2792,  1997,
           103,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,  2025, 21548,   103,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 6216, -100, 1996, -100, -100, -100, 2186, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2464, 2153,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.8824, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6023,  -6.5251,  -6.5266,  ...,  -5.9176,  -5.6943,  -3.7840],
         [ -6.2952,  -6.0201,  -6.5536,  ...,  -6.1584,  -5.4968,  -6.5603],
         [-10.1067, -10.0993, -10.5232,  ..., -10.1997,  -9.4973,  -7.9176],
         ...,
         [ -5.9133,  -5.9911,  -6.1016,  ...,  -6.3024,  -5.4274,  -3.7404],
         [ -6.1716,  -6.3069,  -6.3381,  ...,  -6.4206,  -6.0143,  -4.3003],
         [ -5.3820,  -5.4616,  -5.5456,  ...,  -5.6883,  -5.3034,  -3.7201]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.8824105262756348
Raw Input IDs (before masking): tensor([[ 101, 3766, 2001, 2700, 1037, 6020, 2457, 3648, 1999, 2901, 1010, 2635,
         1996, 6847, 1999, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 112
Total tokens: 128, Non-special tokens: 16, Masked tokens: 4
Unique label values and counts: {-100: 124, 1012: 1, 1999: 2, 2001: 1}
Unique labels in batch: tensor([-100, 1012, 1999, 2001])
Masked Input IDs: tensor([[  101,  3766,   103,  2700,  1037,  6020,  2457,  3648, 28075,  2901,
          1010,  2635,  1996,  6847,   103,  2889,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2001, -100, -100, -100, -100, -100, 1999, -100, -100, -100,
         -100, -100, 1999, -100, 1012, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3766,   103,  2700,  1037,  6020,  2457,  3648, 28075,  2901,
          1010,  2635,  1996,  6847,   103,  2889,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2001, -100, -100, -100, -100, -100, 1999, -100, -100, -100,
         -100, -100, 1999, -100, 1012, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.8536, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5366, -6.4776, -6.4526,  ..., -5.7073, -5.7469, -3.7307],
         [-6.3126, -6.7646, -6.5101,  ..., -7.5995, -6.8433, -6.4858],
         [-5.3503, -5.2857, -5.3037,  ..., -6.0203, -4.4209, -3.9634],
         ...,
         [-8.5018, -8.8599, -8.8692,  ..., -8.2343, -9.0440, -5.3525],
         [-6.7807, -6.8307, -7.1460,  ..., -6.1330, -7.7897,  0.3234],
         [-8.6660, -9.0190, -8.9764,  ..., -8.2866, -8.2598, -5.9635]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.8535815477371216
Raw Input IDs (before masking): tensor([[  101,  1999,  2281,  2230,  3766,  3879,  2114,  2198,  2928,  4862,
         11488,  1999,  2010,  2087,  3522,  7226,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 108
Total tokens: 128, Non-special tokens: 20, Masked tokens: 5
Unique label values and counts: {-100: 123, 1011: 1, 2010: 1, 2230: 1, 2928: 1, 3522: 1}
Unique labels in batch: tensor([-100, 1011, 2010, 2230, 2928, 3522])
Masked Input IDs: tensor([[  101,  1999,  2281,   103,  3766,  3879,  2114,  2198,   103,  4862,
         11488,  1999,   103,  2087,   103,  7226,  2005,  2128,   103,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 2230, -100, -100, -100, -100, 2928, -100, -100, -100,
         2010, -100, 3522, -100, -100, -100, 1011, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1999,  2281,   103,  3766,  3879,  2114,  2198,   103,  4862,
         11488,  1999,   103,  2087,   103,  7226,  2005,  2128,   103,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2230, -100, -100, -100, -100, 2928, -100, -100, -100,
         2010, -100, 3522, -100, -100, -100, 1011, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.4579, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8023,  -6.7274,  -6.7378,  ...,  -6.1050,  -5.9495,  -4.1579],
         [-11.4695, -11.5967, -11.3566,  ..., -10.0641,  -9.0895, -10.3560],
         [ -8.1838,  -8.3366,  -8.1560,  ...,  -6.7004,  -7.5091,  -6.2028],
         ...,
         [ -3.8676,  -4.4073,  -4.2380,  ...,  -3.2179,  -4.4984,  -3.0517],
         [ -1.7043,  -2.2009,  -2.0452,  ...,  -1.5341,  -2.7679,  -1.7227],
         [ -3.5449,  -3.9024,  -3.7727,  ...,  -2.8743,  -3.8801,  -3.0417]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.4579172134399414
Raw Input IDs (before masking): tensor([[  101, 14019,  2010,  6513,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,  1005,  1056,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 128
Total masked tokens: 9
Percentage of masked tokens: 7.03%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 9
Unique label values and counts: {-100: 119, 2023: 1, 2031: 1, 2206: 1, 2349: 1, 2876: 1, 3348: 1, 6040: 1, 9105: 1, 11323: 1}
Unique labels in batch: tensor([ -100,  2023,  2031,  2206,  2349,  2876,  3348,  6040,  9105, 11323])
Masked Input IDs: tensor([[  101, 14019,  2010,  6513,   103, 28517,  6216,  1005,  1055,   103,
          2044,  2016,   103,  1005,  1056,   103,   103,  2007,  2032,  2021,
          2101,   103,   103,  2001,   103,  2000,  2014,   103, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  2206,  -100,  -100,  -100,  -100,  6040,
          -100,  -100,  2876,  -100,  -100,  2031,  3348,  -100,  -100,  -100,
          -100, 11323,  2023,  -100,  2349,  -100,  -100,  9105,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14019,  2010,  6513,   103, 28517,  6216,  1005,  1055,   103,
          2044,  2016,   103,  1005,  1056,   103,   103,  2007,  2032,  2021,
          2101,   103,   103,  2001,   103,  2000,  2014,   103, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  2206,  -100,  -100,  -100,  -100,  6040,
          -100,  -100,  2876,  -100,  -100,  2031,  3348,  -100,  -100,  -100,
          -100, 11323,  2023,  -100,  2349,  -100,  -100,  9105,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(5.0308, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7167,  -6.6939,  -6.7285,  ...,  -6.1778,  -5.9790,  -3.6921],
         [ -5.1094,  -5.1330,  -5.4380,  ...,  -5.9981,  -4.8103,  -5.0514],
         [ -9.8565,  -9.7661, -10.1782,  ...,  -9.7950,  -8.2936,  -8.3131],
         ...,
         [ -7.1518,  -7.2629,  -7.4021,  ...,  -6.9055,  -7.1729,  -3.5537],
         [ -7.1315,  -7.2193,  -7.2641,  ...,  -7.1886,  -6.6336,  -4.9344],
         [ -6.9049,  -7.0016,  -7.0556,  ...,  -6.8434,  -6.3342,  -4.4797]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 5.030757904052734
Raw Input IDs (before masking): tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010,  2066, 10461,  5586,
          2571,  1010,  2097,  2025,  2709,  2000,  3579,  2006,  2014,  1048,
          8737,  2487,  3298,  1999,  1996,  2325,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 7
Unique label values and counts: {-100: 121, 2000: 1, 2014: 1, 2025: 1, 2066: 1, 2325: 1, 3298: 1, 13006: 1}
Unique labels in batch: tensor([ -100,  2000,  2014,  2025,  2066,  2325,  3298, 13006])
Masked Input IDs: tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010, 18631, 10461,  5586,
          2571,  1010,  2097,   103,  2709,   103,  3579,  2006,  2014,  1048,
          8737,  2487,   103,  1999,  1996,   103,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100, 13006,  -100,  -100,  2066,  -100,  -100,
          -100,  -100,  -100,  2025,  -100,  2000,  -100,  -100,  2014,  -100,
          -100,  -100,  3298,  -100,  -100,  2325,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010, 18631, 10461,  5586,
          2571,  1010,  2097,   103,  2709,   103,  3579,  2006,  2014,  1048,
          8737,  2487,   103,  1999,  1996,   103,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100, 13006,  -100,  -100,  2066,  -100,  -100,
          -100,  -100,  -100,  2025,  -100,  2000,  -100,  -100,  2014,  -100,
          -100,  -100,  3298,  -100,  -100,  2325,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.6770, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5482, -6.4844, -6.4997,  ..., -5.7657, -5.7181, -3.8681],
         [-8.9566, -9.8592, -9.6861,  ..., -9.1056, -9.6474, -5.3111],
         [-0.8017, -1.2563, -1.2654,  ..., -1.2800, -0.9161, -0.0853],
         ...,
         [-5.8635, -6.0658, -6.0783,  ..., -5.3784, -5.4648, -2.6357],
         [-5.1007, -5.4395, -5.4454,  ..., -5.2154, -5.8308, -3.1223],
         [-5.2914, -5.4400, -5.5813,  ..., -5.4484, -5.8973, -2.0560]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.6770365238189697
  Batch    40  of     50.    Elapsed: 0:00:29.
Raw Input IDs (before masking): tensor([[  101,  2014, 11062,  5615,  2001,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 114
Total tokens: 128, Non-special tokens: 14, Masked tokens: 4
Unique label values and counts: {-100: 124, 1997: 1, 1998: 1, 2014: 1, 23622: 1}
Unique labels in batch: tensor([ -100,  1997,  1998,  2014, 23622])
Masked Input IDs: tensor([[  101,   103, 11062,  5615,  2001,  1037,  2365,   103, 21658, 10717,
         17400,   103,  9465,   103,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  2014,  -100,  -100,  -100,  -100,  -100,  1997,  -100,  -100,
          -100,  1998,  -100, 23622,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103, 11062,  5615,  2001,  1037,  2365,   103, 21658, 10717,
         17400,   103,  9465,   103,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  2014,  -100,  -100,  -100,  -100,  -100,  1997,  -100,  -100,
          -100,  1998,  -100, 23622,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.3006, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7315, -6.6822, -6.7046,  ..., -6.0626, -5.9107, -3.9333],
         [-5.9823, -6.1337, -6.1089,  ..., -6.5465, -5.5967, -5.3343],
         [-8.3590, -8.2658, -8.3859,  ..., -8.4296, -6.5878, -5.0787],
         ...,
         [-6.0809, -6.2377, -6.2309,  ..., -6.8147, -6.4364, -3.5366],
         [-5.4144, -5.6258, -5.4770,  ..., -6.0902, -5.3758, -4.0776],
         [-7.0988, -7.2160, -7.2407,  ..., -7.7045, -7.8998, -3.4358]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.3006064891815186
Raw Input IDs (before masking): tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,  4905,  1999,  1043, 27610,
          2221,  1999,  3150,  1011,  1011,  3938,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 128
Total masked tokens: 8
Percentage of masked tokens: 6.25%
Total special tokens in batch: 92
Total tokens: 128, Non-special tokens: 36, Masked tokens: 8
Unique label values and counts: {-100: 120, 1011: 1, 1012: 1, 1999: 1, 2019: 1, 2212: 1, 2937: 1, 4905: 2}
Unique labels in batch: tensor([-100, 1011, 1012, 1999, 2019, 2212, 2937, 4905])
Masked Input IDs: tensor([[  101,  3102,   103,  1999,  3301,  1011,  1011,  6535,  1010,   103,
          3353,   103,   103,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,   103,  1999,  1043, 27610,
          2221,   103,  3150,   103,  1011,  3938,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2937, -100, -100, -100, -100, -100, -100, 2019, -100, 2212,
         4905, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, 4905, -100, -100, -100, -100, 1999, -100, 1011, -100, -100,
         1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3102,   103,  1999,  3301,  1011,  1011,  6535,  1010,   103,
          3353,   103,   103,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,   103,  1999,  1043, 27610,
          2221,   103,  3150,   103,  1011,  3938,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2937, -100, -100, -100, -100, -100, -100, 2019, -100, 2212,
         4905, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, 4905, -100, -100, -100, -100, 1999, -100, 1011, -100, -100,
         1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.9393, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7939, -6.7587, -6.6905,  ..., -6.1818, -5.8797, -4.0360],
         [-7.6228, -7.4520, -7.6538,  ..., -7.2370, -6.3962, -6.0572],
         [-3.6954, -3.7043, -3.8623,  ..., -3.2052, -2.6463, -2.2030],
         ...,
         [-7.3897, -7.6771, -7.6094,  ..., -7.7532, -7.8483, -5.0199],
         [-6.8542, -7.2215, -6.9568,  ..., -6.4960, -6.0523, -6.0587],
         [-6.9390, -7.3327, -7.0309,  ..., -6.6846, -6.3307, -5.9824]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.9392555356025696
Raw Input IDs (before masking): tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
          3129,  2003,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 83
Total tokens: 128, Non-special tokens: 45, Masked tokens: 7
Unique label values and counts: {-100: 121, 1005: 1, 1010: 1, 2003: 1, 2615: 1, 15775: 1, 17258: 1, 25987: 1}
Unique labels in batch: tensor([ -100,  1005,  1010,  2003,  2615, 15775, 17258, 25987])
Masked Input IDs: tensor([[  101,  2128,  2497, 17079,  5714,  8038, 20411,   103,   103,  1055,
          3129,   103,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
           103,   103,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100, 15775,  -100,  -100,  -100,  2615,  1005,  -100,
          -100,  2003,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         25987,  1010,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100, 17258,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2128,  2497, 17079,  5714,  8038, 20411,   103,   103,  1055,
          3129,   103,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
           103,   103,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100, 15775,  -100,  -100,  -100,  2615,  1005,  -100,
          -100,  2003,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         25987,  1010,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100, 17258,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.0674, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9215, -6.9156, -6.9884,  ..., -6.4798, -6.4020, -3.9722],
         [-9.9962, -9.0922, -9.9119,  ..., -8.8156, -9.8296, -4.0762],
         [-6.8105, -6.2267, -6.7993,  ..., -6.4396, -6.2023, -4.0132],
         ...,
         [-4.1051, -3.9674, -4.2786,  ..., -4.9312, -4.2437, -2.8894],
         [ 0.8073,  0.9943,  0.6867,  ..., -0.2473,  1.0986, -0.1596],
         [-3.8099, -3.8018, -4.1619,  ..., -4.8978, -2.6022, -3.2524]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.067399263381958
Raw Input IDs (before masking): tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 127, 2018: 1}
Unique labels in batch: tensor([-100, 2018])
Masked Input IDs: tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,   103,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 2018, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,   103,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 2018, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0590, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7687,  -6.7331,  -6.7161,  ...,  -6.2079,  -5.9803,  -4.0465],
         [ -3.9669,  -3.9365,  -3.9640,  ...,  -4.7588,  -5.0280,  -3.8050],
         [-10.3960, -10.2805, -10.5002,  ..., -10.6926,  -7.6841,  -6.1743],
         ...,
         [ -6.7683,  -6.6709,  -6.8543,  ...,  -7.0954,  -8.0448,  -2.7110],
         [ -6.4048,  -6.4886,  -6.3033,  ...,  -6.6681,  -7.0060,  -4.2226],
         [ -5.7114,  -5.8285,  -5.7506,  ...,  -6.4990,  -6.5337,  -3.9892]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.058987002819776535
Raw Input IDs (before masking): tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,  1996,
          2117,  4587,  1997,  2274,  2336,  2164,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428,  1010, 25532,  1006, 27233,  7685,  1007,
          1998,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 5
Unique label values and counts: {-100: 123, 1010: 1, 1012: 1, 1996: 1, 2164: 1, 2336: 1}
Unique labels in batch: tensor([-100, 1010, 1012, 1996, 2164, 2336])
Masked Input IDs: tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,   103,
          2117,  4587,  1997,  2274,  2336,   103,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428, 15315, 25532,  1006, 27233,  7685,  1007,
          1998,  8527,   103,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1996, -100, -100,
         -100, -100, 2336, 2164, -100, -100, -100, -100, -100, -100, -100, -100,
         1010, -100, -100, -100, -100, -100, -100, -100, 1012, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,   103,
          2117,  4587,  1997,  2274,  2336,   103,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428, 15315, 25532,  1006, 27233,  7685,  1007,
          1998,  8527,   103,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1996, -100, -100,
         -100, -100, 2336, 2164, -100, -100, -100, -100, -100, -100, -100, -100,
         1010, -100, -100, -100, -100, -100, -100, -100, 1012, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.1178, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.3194, -6.2584, -6.2445,  ..., -5.7753, -5.3128, -3.7030],
         [-6.8576, -7.2296, -7.2616,  ..., -7.7607, -6.9946, -4.3377],
         [-5.0514, -5.4785, -5.4556,  ..., -6.6581, -6.2280, -4.6090],
         ...,
         [-6.3465, -6.5862, -6.7030,  ..., -6.8215, -6.6594, -3.1130],
         [-7.4131, -7.4609, -7.6496,  ..., -7.6759, -7.1566, -3.3133],
         [-5.9485, -6.1193, -6.2374,  ..., -6.2861, -5.8291, -2.3914]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.1178401708602905
Raw Input IDs (before masking): tensor([[  101,  2043,  2198,  2001,  2416,  1010,  2010,  2269,  2741,  2032,
          2000, 10297,  1006,  2059,  2112,  1997,  1996,  2212,  1997,  3996,
          1007,  2000,  5463,  2082,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 104
Total tokens: 128, Non-special tokens: 24, Masked tokens: 6
Unique label values and counts: {-100: 122, 1010: 1, 2010: 1, 2043: 1, 2198: 1, 3996: 1, 10297: 1}
Unique labels in batch: tensor([ -100,  1010,  2010,  2043,  2198,  3996, 10297])
Masked Input IDs: tensor([[ 101,  103, 2198, 2001, 2416,  103,  103, 2269, 2741, 2032, 2000,  103,
         1006, 2059, 2112, 1997, 1996, 2212, 1997, 3996, 1007, 2000, 5463, 2082,
         1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[ -100,  2043,  2198,  -100,  -100,  1010,  2010,  -100,  -100,  -100,
          -100, 10297,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3996,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101,  103, 2198, 2001, 2416,  103,  103, 2269, 2741, 2032, 2000,  103,
         1006, 2059, 2112, 1997, 1996, 2212, 1997, 3996, 1007, 2000, 5463, 2082,
         1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[ -100,  2043,  2198,  -100,  -100,  1010,  2010,  -100,  -100,  -100,
          -100, 10297,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3996,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.8039, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7179, -6.6764, -6.6609,  ..., -6.2060, -6.0498, -3.9014],
         [-7.4355, -7.4533, -7.7974,  ..., -7.7975, -7.5482, -7.0879],
         [-6.9521, -7.3503, -7.4278,  ..., -8.0192, -7.7654, -4.7796],
         ...,
         [-7.2266, -7.2970, -7.4590,  ..., -7.2711, -7.4603, -6.6563],
         [-4.7747, -4.7468, -4.8766,  ..., -4.8287, -5.2469, -4.8994],
         [-7.8168, -7.8522, -7.9940,  ..., -7.6679, -8.0623, -6.4180]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.8038976788520813
Raw Input IDs (before masking): tensor([[  101, 11407,  3273,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 118
Total tokens: 128, Non-special tokens: 10, Masked tokens: 2
Unique label values and counts: {-100: 126, 7004: 1, 24520: 1}
Unique labels in batch: tensor([ -100,  7004, 24520])
Masked Input IDs: tensor([[  101, 11407,  3273,  2007,   103,   103,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  7004, 24520,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 11407,  3273,  2007,   103,   103,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  7004, 24520,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(8.8094, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5785, -6.5426, -6.5364,  ..., -5.8119, -5.5832, -3.8814],
         [-5.7016, -5.6123, -6.0033,  ..., -7.1085, -5.6988, -5.2176],
         [-4.9553, -5.3033, -5.5049,  ..., -6.7718, -3.7404, -2.8991],
         ...,
         [-6.8818, -7.0252, -7.1074,  ..., -7.0804, -7.6380, -3.0861],
         [-5.8150, -5.8502, -5.9859,  ..., -6.9601, -5.8999, -4.7294],
         [-5.9880, -6.0902, -6.1345,  ..., -6.8370, -6.2521, -4.7632]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 8.80942153930664
Raw Input IDs (before masking): tensor([[  101,  2139,  2474,  2061,  2696,  2153,  2743,  2005, 13523,  4948,
          1997,  1039,  1008, 16428, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 1
Unique label values and counts: {-100: 127, 16428: 1}
Unique labels in batch: tensor([ -100, 16428])
Masked Input IDs: tensor([[  101,  2139,  2474,  2061,  2696,  2153,  2743,  2005, 13523,  4948,
          1997,  1039,  1008,   103, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100, 16428,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2139,  2474,  2061,  2696,  2153,  2743,  2005, 13523,  4948,
          1997,  1039,  1008,   103, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100, 16428,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(6.6475, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8292,  -6.7520,  -6.7562,  ...,  -5.9920,  -5.9546,  -3.9593],
         [-10.3710, -10.4988, -10.8389,  ...,  -9.2882, -10.5203,  -6.0690],
         [ -8.5910,  -8.7597,  -9.0652,  ...,  -7.2136,  -8.9809,  -5.8061],
         ...,
         [ -6.3361,  -6.0671,  -6.3217,  ...,  -6.6288,  -6.6241,  -3.7139],
         [ -7.1245,  -7.2620,  -7.3122,  ...,  -7.2614,  -8.1530,  -5.3627],
         [ -6.4904,  -6.4750,  -6.5203,  ...,  -6.3352,  -6.9085,  -3.9636]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 6.647495269775391
Raw Input IDs (before masking): tensor([[  101,  3249,  2011, 13523,  4948, 12262,  2480,  2011,  2058,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,  2001,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,  2306,
          1996,  2074, 24108,  9863,  2283,  1006,  2029,  2001, 13862,  2007,
          3377,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,  2877,
          2000,  2343, 10323,  2273,  6633,  1005,  1055, 20380,  1997,  1037,
          3584,  2283,  2862,  1999,  1039,  1008, 16428, 16429,  2050,  2005,
          1996,  2857,  3054,  1011,  2744,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  1037,  2835,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([17])
Total tokens in batch: 128
Total masked tokens: 17
Percentage of masked tokens: 13.28%
Total special tokens in batch: 36
Total tokens: 128, Non-special tokens: 92, Masked tokens: 17
Unique label values and counts: {-100: 111, 1037: 1, 1055: 2, 1997: 1, 1999: 1, 2000: 1, 2001: 2, 2014: 1, 2138: 1, 2474: 1, 2744: 1, 2857: 1, 2877: 1, 12452: 1, 13523: 1, 13862: 1}
Unique labels in batch: tensor([ -100,  1037,  1055,  1997,  1999,  2000,  2001,  2014,  2138,  2474,
         2744,  2857,  2877, 12452, 13523, 13862])
Masked Input IDs: tensor([[  101,  3249,  2011,   103,  4948, 12262,  2480,  2011,  2058,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,   103,  3278,   103,  2009,
          3465,  2139, 22773,  2061,  2696,  2172,  1997,   103,  2490,  2306,
          1996,  2074, 24108,  9863,  2283,  1006,  2029, 12655, 13862,  2007,
          3377,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,   103,
          2000,  2343, 10323,  2273,  6633,  1005,   103, 20380,   103,  1037,
          3584,  2283,  2862,  1999,  1039,  1008, 16428, 16429,  2050,  2005,
          1996,   103,  3054,  1011,   103,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,   103,  4945,   103,   103, 13651,  2835,
           103,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100, 13523,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  2001,  -100,  2138,  -100,
          -100,  -100,  2474,  -100,  -100,  -100,  -100,  2014,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2001, 13862,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2877,
          -100,  -100,  -100,  -100,  -100,  -100,  1055,  -100,  1997,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  2857,  -100,  -100,  2744,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  1055,  -100,  2000, 12452,  1037,  -100,
          1999,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3249,  2011,   103,  4948, 12262,  2480,  2011,  2058,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,   103,  3278,   103,  2009,
          3465,  2139, 22773,  2061,  2696,  2172,  1997,   103,  2490,  2306,
          1996,  2074, 24108,  9863,  2283,  1006,  2029, 12655, 13862,  2007,
          3377,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,   103,
          2000,  2343, 10323,  2273,  6633,  1005,   103, 20380,   103,  1037,
          3584,  2283,  2862,  1999,  1039,  1008, 16428, 16429,  2050,  2005,
          1996,   103,  3054,  1011,   103,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,   103,  4945,   103,   103, 13651,  2835,
           103,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100, 13523,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  2001,  -100,  2138,  -100,
          -100,  -100,  2474,  -100,  -100,  -100,  -100,  2014,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  2001, 13862,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2877,
          -100,  -100,  -100,  -100,  -100,  -100,  1055,  -100,  1997,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  2857,  -100,  -100,  2744,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  1055,  -100,  2000, 12452,  1037,  -100,
          1999,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.0582, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7768, -6.7229, -6.7113,  ..., -6.0013, -6.1506, -3.9997],
         [-8.1414, -8.3842, -8.4031,  ..., -6.3025, -7.3021, -9.6179],
         [-7.4958, -7.5004, -7.5828,  ..., -6.5936, -7.8054, -8.8059],
         ...,
         [-6.9344, -7.0349, -7.0360,  ..., -6.0533, -5.8259, -6.6195],
         [-7.5213, -7.4450, -7.6400,  ..., -6.8969, -8.1763, -4.8740],
         [-5.8050, -5.6227, -5.7235,  ..., -5.3125, -5.9973, -6.0211]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.0582334995269775
Raw Input IDs (before masking): tensor([[  101,  6609,  1005,  1055,  2269,  1010, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 6
Unique label values and counts: {-100: 122, 1010: 1, 1012: 1, 2000: 1, 2012: 1, 2035: 1, 2269: 1}
Unique labels in batch: tensor([-100, 1010, 1012, 2000, 2012, 2035, 2269])
Masked Input IDs: tensor([[  101,  6609,  1005,  1055,   103, 20186, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,   103,  4608,  2023,  4138,  2450,   103,   103,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,   103,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 2269, 1010, -100, -100, -100, -100, -100, -100,
         -100, 2000, -100, -100, -100, -100, 2012, 2035, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6609,  1005,  1055,   103, 20186, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,   103,  4608,  2023,  4138,  2450,   103,   103,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,   103,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 2269, 1010, -100, -100, -100, -100, -100, -100,
         -100, 2000, -100, -100, -100, -100, 2012, 2035, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 1012, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.4847, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7682, -6.7344, -6.7192,  ..., -6.1382, -5.9581, -4.0788],
         [-5.4908, -5.7982, -5.8772,  ..., -5.0673, -4.5687, -6.1596],
         [-9.5355, -9.6950, -9.8902,  ..., -9.6661, -7.0026, -8.5170],
         ...,
         [-6.9563, -7.0681, -7.1909,  ..., -7.2570, -6.8154, -5.4717],
         [-5.6454, -5.8465, -5.7348,  ..., -6.1776, -4.4381, -6.0827],
         [-7.1495, -7.2549, -7.2913,  ..., -6.9243, -6.6029, -6.4845]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.4846935272216797

  Average training loss: 3.21
[Epoch 2] Training epoch took: 0:00:36

======== Epoch 3 / 3 ========
Training...
Raw Input IDs (before masking): tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792,  2029,  2443,  3922,  2011, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,  9893, 25208,  1998,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,  2019,  2792,  2029,  2956,  8507,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,  1997,  4561,  2012,
          1996,  2051,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048,  2367,  4178,  1999,  2029,
          2280,  8626,  1005,  7939,  9387, 12986, 17590,  1998,  8507,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 128
Total masked tokens: 9
Percentage of masked tokens: 7.03%
Total special tokens in batch: 45
Total tokens: 128, Non-special tokens: 83, Masked tokens: 9
Unique label values and counts: {-100: 119, 1997: 1, 2011: 1, 2019: 1, 2029: 1, 2048: 1, 2051: 1, 8507: 1, 8626: 1, 25208: 1}
Unique labels in batch: tensor([ -100,  1997,  2011,  2019,  2029,  2048,  2051,  8507,  8626, 25208])
Masked Input IDs: tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792, 12058,  2443,  3922,   103, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,  9893,   103,  1998,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,   103,  2792,  2029,  2956,  8507,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,   103,  4561,  2012,
          1996,   103,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048,  2367,  4178,  1999,  2029,
          2280,   103,  1005,  7939,  9387, 12986, 17590,  1998,   103,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2029,  -100,  -100,  2011,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 25208,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2019,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1997,  -100,  -100,
          -100,  2051,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2048,  -100,  -100,  -100,  -100,
          -100,  8626,  -100,  -100,  -100,  -100,  -100,  -100,  8507,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3870, 21146, 18885, 17274,  3213,  6330, 24421,  1025,  1037,
          2569,  4234,  2792, 12058,  2443,  3922,   103, 29168,  2004,  3678,
          1010, 23917,  2522, 12844,  1010,  9893,   103,  1998,  2547,  2739,
          8133,  9533, 16042,  6374,  1025,   103,  2792,  2029,  2956,  8507,
          2006,  3051,  1010,  1996,  3812, 13523,  4948,   103,  4561,  2012,
          1996,   103,  1997,  2537,  1010,  6037,  2004,  2014,  2219, 23993,
         10461,  9587, 24281,  1025,  1998,  2048,  2367,  4178,  1999,  2029,
          2280,   103,  1005,  7939,  9387, 12986, 17590,  1998,   103,  9610,
         13947,  4113,  5652,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2029,  -100,  -100,  2011,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 25208,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2019,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  1997,  -100,  -100,
          -100,  2051,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2048,  -100,  -100,  -100,  -100,
          -100,  8626,  -100,  -100,  -100,  -100,  -100,  -100,  8507,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.3860, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6986,  -6.6247,  -6.5471,  ...,  -5.8901,  -5.7010,  -3.8341],
         [ -6.7916,  -6.9374,  -6.7124,  ...,  -7.1985,  -6.1510,  -5.4914],
         [ -9.8296,  -9.9714, -10.0760,  ...,  -9.9328, -10.0686,  -8.0740],
         ...,
         [ -5.8637,  -5.7167,  -5.6401,  ...,  -5.5334,  -5.4965,  -3.7890],
         [ -6.0864,  -6.1851,  -6.1541,  ...,  -6.1283,  -5.6147,  -3.8475],
         [ -4.4302,  -4.7947,  -4.8773,  ...,  -4.0069,  -4.4080,  -3.3361]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.385988235473633
Raw Input IDs (before masking): tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,  2021,  5295,
          1999,  2901,  2000,  5138,  1037,  2695,  2004,  6059,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 108
Total tokens: 128, Non-special tokens: 20, Masked tokens: 3
Unique label values and counts: {-100: 125, 2021: 1, 5138: 1, 6059: 1}
Unique labels in batch: tensor([-100, 2021, 5138, 6059])
Masked Input IDs: tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,   103,  5295,
          1999,  2901,  2000,   103,  1037,  2695,  2004,   103,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 2021, -100, -100, -100,
         -100, 5138, -100, -100, -100, 6059, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2016,  2018,  2042, 20847,  2000,  3519,  1010,   103,  5295,
          1999,  2901,  2000,   103,  1037,  2695,  2004,   103,  2000,  4380,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, 2021, -100, -100, -100,
         -100, 5138, -100, -100, -100, 6059, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.4846, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7612, -6.7157, -6.7247,  ..., -6.0406, -5.9858, -4.0683],
         [-7.3620, -7.3942, -7.6390,  ..., -7.6560, -7.9001, -3.3572],
         [-8.7329, -8.8297, -8.9832,  ..., -7.7847, -6.6539, -3.5948],
         ...,
         [-5.4814, -5.8519, -5.7289,  ..., -5.7781, -6.6790, -4.4962],
         [-5.9064, -6.1878, -6.0182,  ..., -5.6295, -6.8368, -4.2779],
         [-5.0281, -5.4158, -5.2237,  ..., -4.3452, -5.7568, -5.2337]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.48463356494903564
Raw Input IDs (before masking): tensor([[ 101, 1996, 2783, 2372, 1997, 4126, 2031, 2036, 2864, 1999, 2624, 3799,
         2104, 1996, 2316, 2171, 1005, 1005, 6556, 7193, 1036, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 106
Total tokens: 128, Non-special tokens: 22, Masked tokens: 6
Unique label values and counts: {-100: 122, 1036: 1, 1999: 1, 2316: 1, 2624: 1, 2783: 1, 2864: 1}
Unique labels in batch: tensor([-100, 1036, 1999, 2316, 2624, 2783, 2864])
Masked Input IDs: tensor([[ 101, 1996,  103, 2372, 1997, 4126, 2031, 2036, 2864,  103,  103, 3799,
         2104, 1996, 2316, 2171, 1005, 1005, 6556, 7193,  103, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, 2783, -100, -100, -100, -100, -100, 2864, 1999, 2624, -100,
         -100, -100, 2316, -100, -100, -100, -100, -100, 1036, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 1996,  103, 2372, 1997, 4126, 2031, 2036, 2864,  103,  103, 3799,
         2104, 1996, 2316, 2171, 1005, 1005, 6556, 7193,  103, 1036, 1012,  102,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, 2783, -100, -100, -100, -100, -100, 2864, 1999, 2624, -100,
         -100, -100, 2316, -100, -100, -100, -100, -100, 1036, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.7283, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8693,  -6.8245,  -6.8372,  ...,  -6.0301,  -5.9600,  -3.7384],
         [-15.0847, -14.9156, -14.9778,  ..., -12.0473, -11.8120, -11.9342],
         [ -6.0391,  -5.9069,  -5.9922,  ...,  -3.8599,  -5.3514,  -2.4434],
         ...,
         [ -7.7401,  -7.8148,  -7.7495,  ...,  -8.1853,  -7.2811,  -6.0074],
         [ -7.8512,  -7.7550,  -7.8002,  ...,  -7.3397,  -7.2451,  -5.7685],
         [ -7.9677,  -7.8559,  -8.1416,  ...,  -7.3955,  -7.6056,  -3.6714]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.7282596826553345
Raw Input IDs (before masking): tensor([[  101,  2010,  4203, 10768,  3850,  2834,  1999,  2384,  2001,  2004,
         27617,  2401,  1999,  1996,  8001,  3179,  1997,  2175,  3669,  5558,
          2615,  1005,  1055,  7110,  8447,  7849,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 102
Total tokens: 128, Non-special tokens: 26, Masked tokens: 6
Unique label values and counts: {-100: 122, 2004: 1, 4203: 1, 7110: 1, 8447: 1, 10768: 1, 27617: 1}
Unique labels in batch: tensor([ -100,  2004,  4203,  7110,  8447, 10768, 27617])
Masked Input IDs: tensor([[ 101, 2010,  103,  103, 3850, 2834, 1999, 2384, 2001,  103,  103, 2401,
         1999, 1996, 8001, 3179, 1997, 2175, 3669, 5558, 2615, 1005, 1055,  103,
          103, 7849, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[ -100,  -100,  4203, 10768,  -100,  -100,  -100,  -100,  -100,  2004,
         27617,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  7110,  8447,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 2010,  103,  103, 3850, 2834, 1999, 2384, 2001,  103,  103, 2401,
         1999, 1996, 8001, 3179, 1997, 2175, 3669, 5558, 2615, 1005, 1055,  103,
          103, 7849, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[ -100,  -100,  4203, 10768,  -100,  -100,  -100,  -100,  -100,  2004,
         27617,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  7110,  8447,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(8.2856, grad_fn=<NllLossBackward0>), logits=tensor([[[-7.0555, -7.0446, -7.0209,  ..., -6.1327, -6.4160, -4.0785],
         [-9.0388, -9.5026, -9.7940,  ..., -8.6890, -8.8111, -8.2891],
         [-5.6743, -5.6947, -6.0613,  ..., -5.5314, -6.3282, -4.8383],
         ...,
         [-7.4957, -7.4412, -7.6938,  ..., -7.3345, -9.2044, -2.0641],
         [-7.4169, -7.4879, -7.5306,  ..., -7.4730, -9.1711, -4.7603],
         [-7.4162, -7.6085, -7.6369,  ..., -7.6062, -8.3027, -5.4400]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 8.285648345947266
Raw Input IDs (before masking): tensor([[  101,  9060, 12849,  2102,  1997,  1996,  3190, 10969,  8690,  1036,
          1036,  5675,  2537,  1998, 20578, 14029,  1005,  1005,  1010,  2021,
         27175,  5061,  1005,  1055,  6180,  1998,  2049,  1036,  1036,  9210,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 93
Total tokens: 128, Non-special tokens: 35, Masked tokens: 5
Unique label values and counts: {-100: 123, 2102: 1, 2537: 1, 8690: 1, 9210: 1, 14029: 1}
Unique labels in batch: tensor([ -100,  2102,  2537,  8690,  9210, 14029])
Masked Input IDs: tensor([[  101,  9060, 12849,   103,  1997,  1996,  3190, 10969, 24095,  1036,
          1036,  5675,  5697,  1998, 20578,  4747,  1005,  1005,  1010,  2021,
         27175,  5061,  1005,  1055,  6180,  1998,  2049,  1036,  1036,   103,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  2102,  -100,  -100,  -100,  -100,  8690,  -100,
          -100,  -100,  2537,  -100,  -100, 14029,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  9210,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9060, 12849,   103,  1997,  1996,  3190, 10969, 24095,  1036,
          1036,  5675,  5697,  1998, 20578,  4747,  1005,  1005,  1010,  2021,
         27175,  5061,  1005,  1055,  6180,  1998,  2049,  1036,  1036,   103,
          1005,  1005,  1997, 11007,  3162,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  2102,  -100,  -100,  -100,  -100,  8690,  -100,
          -100,  -100,  2537,  -100,  -100, 14029,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  9210,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(7.1971, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.2489, -6.2029, -6.1923,  ..., -5.4607, -5.3947, -3.4135],
         [-5.7594, -6.0406, -5.5340,  ..., -6.2061, -5.6852, -4.8847],
         [-5.6630, -5.5394, -5.5318,  ..., -6.3043, -6.7032, -5.1503],
         ...,
         [-6.6200, -6.7519, -6.5662,  ..., -7.6619, -7.1378, -2.8135],
         [-6.9881, -7.0999, -6.8506,  ..., -7.8589, -7.6221, -3.3080],
         [-8.1473, -8.2405, -8.1052,  ..., -8.6525, -8.6649, -3.4106]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 7.197126865386963
Raw Input IDs (before masking): tensor([[  101,  6382, 10093,  3877,  1011,  1011,  2209,  1996,  2610,  2961,
          6898,  1997, 28517,  6216,  1010, 12270,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 112
Total tokens: 128, Non-special tokens: 16, Masked tokens: 1
Unique label values and counts: {-100: 127, 6898: 1}
Unique labels in batch: tensor([-100, 6898])
Masked Input IDs: tensor([[  101,  6382, 10093,  3877,  1011,  1011,  2209,  1996,  2610,  2961,
           103,  1997, 28517,  6216,  1010, 12270,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6898, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6382, 10093,  3877,  1011,  1011,  2209,  1996,  2610,  2961,
           103,  1997, 28517,  6216,  1010, 12270,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6898, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.0615, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8027, -6.7705, -6.7548,  ..., -6.1631, -5.9390, -4.0440],
         [-5.9391, -6.1325, -6.1796,  ..., -7.0951, -5.7750, -7.5738],
         [-7.4769, -7.4211, -7.6467,  ..., -7.6324, -6.6787, -5.3318],
         ...,
         [-4.9791, -5.1233, -5.1236,  ..., -5.5359, -5.7891, -1.7142],
         [-6.5405, -6.4935, -6.6859,  ..., -6.6590, -6.6936, -2.7212],
         [-6.4612, -6.5441, -6.6006,  ..., -6.5656, -7.1339, -2.0943]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.061508297920227
Raw Input IDs (before masking): tensor([[  101,  6609,  2001,  3532,  1998,  4242,  2000, 17798,  1010,  2018,
          1037,  2365,  2170, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 127, 2170: 1}
Unique labels in batch: tensor([-100, 2170])
Masked Input IDs: tensor([[  101,  6609,  2001,  3532,  1998,  4242,  2000, 17798,  1010,  2018,
          1037,  2365,   103, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2170, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6609,  2001,  3532,  1998,  4242,  2000, 17798,  1010,  2018,
          1037,  2365,   103, 14916,  6790,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2170, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.6206, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6751,  -6.6596,  -6.6623,  ...,  -5.9572,  -5.7680,  -3.7996],
         [ -6.4642,  -6.4621,  -6.7354,  ...,  -6.0892,  -5.9650,  -5.6584],
         [-11.7394, -11.2411, -11.7776,  ..., -12.1668,  -8.6710,  -7.8514],
         ...,
         [ -7.1463,  -7.2486,  -7.3401,  ...,  -7.3124,  -8.3508,  -3.5818],
         [ -6.4702,  -6.5692,  -6.5386,  ...,  -6.3814,  -5.9369,  -5.5500],
         [ -6.4861,  -6.6168,  -6.5582,  ...,  -6.3326,  -5.6708,  -5.0651]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.6205694675445557
Raw Input IDs (before masking): tensor([[  101,  3174,  2086,  3283,  1010, 17798, 24471, 20755,  3603,  2995,
          2293,  2007,  6609, 23527,  1998,  2211,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 5
Unique label values and counts: {-100: 123, 1998: 1, 2007: 1, 2086: 1, 2995: 1, 23527: 1}
Unique labels in batch: tensor([ -100,  1998,  2007,  2086,  2995, 23527])
Masked Input IDs: tensor([[  101,  3174,   103,  3283,  1010, 17798, 24471, 20755,  3603,  2995,
          2293,   103,  6609, 23527,   103,  2211,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  2086,  -100,  -100,  -100,  -100,  -100,  -100,  2995,
          -100,  2007,  -100, 23527,  1998,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3174,   103,  3283,  1010, 17798, 24471, 20755,  3603,  2995,
          2293,   103,  6609, 23527,   103,  2211,  1037,  7472,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  2086,  -100,  -100,  -100,  -100,  -100,  -100,  2995,
          -100,  2007,  -100, 23527,  1998,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1031, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.4335,  -6.3901,  -6.3814,  ...,  -5.8608,  -5.7175,  -3.6841],
         [-10.2949, -10.2890, -10.5372,  ...,  -9.0262,  -8.3553,  -7.4146],
         [ -4.4345,  -4.3834,  -4.6639,  ...,  -3.9250,  -4.3599,  -1.7159],
         ...,
         [ -6.1203,  -6.1494,  -6.1895,  ...,  -5.9832,  -5.5644,  -3.6681],
         [ -6.5351,  -6.4353,  -6.6406,  ...,  -6.3948,  -6.2992,  -4.5555],
         [ -5.1696,  -5.3364,  -5.2173,  ...,  -4.8170,  -5.2869,  -2.9873]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.10306556522846222
Raw Input IDs (before masking): tensor([[  101,  2014,  2152,  2082,  2420,  2020,  2985,  2012,  2047, 25487,
          2152,  2082,  1999,  2663,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 126, 2152: 1, 2663: 1}
Unique labels in batch: tensor([-100, 2152, 2663])
Masked Input IDs: tensor([[  101,  2014,   103,  2082,  2420,  2020,  2985,  2012,  2047, 25487,
          2152,  2082,  1999,   103,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2152, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 2663, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014,   103,  2082,  2420,  2020,  2985,  2012,  2047, 25487,
          2152,  2082,  1999,   103,  7159,  2912,  1010,  4307,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2152, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 2663, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1583, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.4680, -6.4258, -6.4276,  ..., -5.8004, -5.5793, -3.8619],
         [-8.3238, -8.5249, -8.4862,  ..., -9.6405, -7.2614, -5.4466],
         [-4.8998, -5.2331, -5.0656,  ..., -5.2557, -4.1009, -5.1520],
         ...,
         [-3.9024, -4.0493, -4.1444,  ..., -5.4989, -5.2669, -3.0731],
         [-6.7656, -6.8867, -6.8714,  ..., -7.0915, -6.6040, -8.4033],
         [-7.5854, -7.6453, -7.7621,  ..., -8.2023, -8.2939, -6.0020]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.15825971961021423
Raw Input IDs (before masking): tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,  8607, 13793,  2097,
          2022,  2999,  2011,  2280, 14757,  2368,  4062,  3419, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 100
Total tokens: 128, Non-special tokens: 28, Masked tokens: 4
Unique label values and counts: {-100: 124, 2011: 1, 2368: 1, 8607: 1, 13793: 1}
Unique labels in batch: tensor([ -100,  2011,  2368,  8607, 13793])
Masked Input IDs: tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,   103,   103,  2097,
          2022,  2999,  2011,  2280, 14757,   103,  4062,  3419, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  8607, 13793,  -100,
          -100,  -100,  2011,  -100,  -100,  2368,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2396,  2050,  4062, 19300, 15104,  3695,   103,   103,  2097,
          2022,  2999,  2011,  2280, 14757,   103,  4062,  3419, 23212,  2053,
          4478,  3089,  2044,  1037, 15640,  2161,  2197,  2095,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  8607, 13793,  -100,
          -100,  -100,  2011,  -100,  -100,  2368,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.6866, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7551, -6.6817, -6.6750,  ..., -5.9813, -5.8201, -4.1710],
         [-7.4178, -7.4994, -7.2780,  ..., -6.8055, -7.4036, -5.3600],
         [-6.9340, -7.4080, -7.2028,  ..., -5.8497, -6.5461, -4.0020],
         ...,
         [-5.6727, -5.6505, -5.6357,  ..., -4.5743, -5.3361, -4.2444],
         [-6.0639, -6.2406, -6.1453,  ..., -4.4492, -5.9958, -4.0181],
         [-7.2178, -7.0792, -7.4699,  ..., -6.5554, -8.6001, -3.3680]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.686648845672607
Raw Input IDs (before masking): tensor([[ 101, 3766, 2001, 2700, 1037, 6020, 2457, 3648, 1999, 2901, 1010, 2635,
         1996, 6847, 1999, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 112
Total tokens: 128, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 126, 2457: 1, 2700: 1}
Unique labels in batch: tensor([-100, 2457, 2700])
Masked Input IDs: tensor([[ 101, 3766, 2001,  103, 1037, 6020,  103, 3648, 1999, 2901, 1010, 2635,
         1996, 6847, 1999, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, 2700, -100, -100, 2457, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 3766, 2001,  103, 1037, 6020,  103, 3648, 1999, 2901, 1010, 2635,
         1996, 6847, 1999, 2889, 1012,  102,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2700, -100, -100, 2457, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.0924, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8071,  -6.7580,  -6.7523,  ...,  -6.0991,  -5.9149,  -3.9716],
         [ -7.2238,  -7.5855,  -7.4435,  ...,  -8.1426,  -7.2474,  -6.4472],
         [ -9.8816,  -9.9829, -10.3856,  ...,  -9.5905,  -6.1293,  -6.2914],
         ...,
         [ -6.9530,  -7.2379,  -7.2258,  ...,  -7.6031,  -7.6216,  -5.1643],
         [ -7.5411,  -7.8910,  -7.8459,  ...,  -7.9482,  -8.0550,  -4.6630],
         [ -7.7967,  -8.1879,  -8.1079,  ...,  -8.2226,  -7.9238,  -6.5548]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.0924158096313477
Raw Input IDs (before masking): tensor([[  101,  2043,  2198,  2001,  2416,  1010,  2010,  2269,  2741,  2032,
          2000, 10297,  1006,  2059,  2112,  1997,  1996,  2212,  1997,  3996,
          1007,  2000,  5463,  2082,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 104
Total tokens: 128, Non-special tokens: 24, Masked tokens: 4
Unique label values and counts: {-100: 124, 1006: 1, 2000: 1, 2059: 1, 2082: 1}
Unique labels in batch: tensor([-100, 1006, 2000, 2059, 2082])
Masked Input IDs: tensor([[  101,  2043,  2198,  2001,  2416,  1010,  2010,  2269,  2741,  2032,
          2000, 10297,   103,   103,  2112,  1997,  1996,  2212,  1997,  3996,
          1007,   103,  5463,  2082,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1006, 2059, -100, -100, -100, -100, -100, -100, -100, 2000, -100, 2082,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2043,  2198,  2001,  2416,  1010,  2010,  2269,  2741,  2032,
          2000, 10297,   103,   103,  2112,  1997,  1996,  2212,  1997,  3996,
          1007,   103,  5463,  2082,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1006, 2059, -100, -100, -100, -100, -100, -100, -100, 2000, -100, 2082,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.1162, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8102,  -6.7647,  -6.7589,  ...,  -6.3124,  -6.1095,  -3.9479],
         [-13.5025, -13.1450, -13.8516,  ..., -15.1034, -13.4921, -11.5096],
         [ -7.5047,  -7.8732,  -7.9880,  ...,  -8.3746,  -8.7513,  -7.2620],
         ...,
         [ -6.8581,  -6.9830,  -7.0963,  ...,  -7.2924,  -7.1948,  -6.3401],
         [ -7.1105,  -7.3088,  -7.4138,  ...,  -7.5634,  -7.4004,  -6.6961],
         [ -7.3263,  -7.4904,  -7.5557,  ...,  -7.9834,  -7.7717,  -6.5558]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.1162194311618805
Raw Input IDs (before masking): tensor([[ 101, 2002, 2109, 2137, 6443, 2000, 2507, 2841, 1037, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 117
Total tokens: 128, Non-special tokens: 11, Masked tokens: 2
Unique label values and counts: {-100: 126, 1037: 1, 2000: 1}
Unique labels in batch: tensor([-100, 1037, 2000])
Masked Input IDs: tensor([[ 101, 2002, 2109, 2137, 6443,  103, 2507, 2841,  103, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, 2000, -100, -100, 1037, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 2002, 2109, 2137, 6443,  103, 2507, 2841,  103, 4310, 2614, 1012,
          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, 2000, -100, -100, 1037, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0200, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6868, -6.6535, -6.5839,  ..., -6.0463, -5.5685, -3.4266],
         [-6.6748, -6.8521, -6.9184,  ..., -6.5074, -6.4998, -4.1297],
         [-7.1823, -7.0508, -7.0364,  ..., -6.9797, -3.5276, -8.2944],
         ...,
         [-6.3255, -6.5105, -6.5027,  ..., -7.0374, -7.4664, -1.2980],
         [-6.0276, -6.1199, -6.2973,  ..., -6.3217, -7.3562, -1.6531],
         [-6.5200, -6.7429, -6.5869,  ..., -6.8719, -6.7910, -2.4248]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.020002571865916252
Raw Input IDs (before masking): tensor([[  101,  6609,  1005,  1055,  2269,  1010, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
          5366,  1998,  6427,  2032,  2008, 10032,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 4
Unique label values and counts: {-100: 124, 1005: 1, 2269: 1, 5366: 1, 10032: 1}
Unique labels in batch: tensor([ -100,  1005,  2269,  5366, 10032])
Masked Input IDs: tensor([[  101,  6609,  1005,  1055,   103,  1010, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
           103,  1998,  6427,  2032,  2008,   103,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  1005,  -100,  2269,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          5366,  -100,  -100,  -100,  -100, 10032,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6609,  1005,  1055,   103,  1010, 24665,  6305,  2401,  1010,
          2359,  2010,  2365,  2000,  4608,  2023,  4138,  2450,  2012,  2035,
           103,  1998,  6427,  2032,  2008,   103,  2052, 14306,  2023,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  1005,  -100,  2269,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          5366,  -100,  -100,  -100,  -100, 10032,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.8287, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7055,  -6.6532,  -6.6464,  ...,  -6.0954,  -5.8361,  -4.0743],
         [ -5.3439,  -5.6065,  -5.6924,  ...,  -4.9978,  -4.4753,  -6.1667],
         [-10.3150, -10.0991, -10.3931,  ...,  -9.2194,  -6.7808,  -8.5077],
         ...,
         [ -6.8315,  -7.0780,  -7.0188,  ...,  -6.9904,  -6.5666,  -6.2511],
         [ -6.8606,  -7.1177,  -7.1211,  ...,  -7.1545,  -6.9782,  -6.0438],
         [ -6.9688,  -7.1524,  -7.0890,  ...,  -7.1503,  -6.4563,  -6.1502]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.8286526203155518
Raw Input IDs (before masking): tensor([[  101,  2044,  2086,  1997,  2108,  2007,  2613,  3868,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,  2011,  2280,  2136, 28919, 27605, 10422,
          4062,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 92
Total tokens: 128, Non-special tokens: 36, Masked tokens: 4
Unique label values and counts: {-100: 124, 1997: 1, 2136: 1, 2280: 1, 3868: 1}
Unique labels in batch: tensor([-100, 1997, 2136, 2280, 3868])
Masked Input IDs: tensor([[  101,  2044,  2086,   103,  2108,  2007,  2613,   103,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,  2011,   103,   103, 28919, 27605, 10422,
          4062,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 1997, -100, -100, -100, 3868, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 2280, 2136, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2044,  2086,   103,  2108,  2007,  2613,   103,  1010,  2000,
          6182, 18334,  8472,  4509,  2072,  2097,  2025,  3298,  2005,  2023,
          2161,  1010,  2108,  2999,  2011,   103,   103, 28919, 27605, 10422,
          4062,  5342,  3211, 14163,  3406,  2232,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 1997, -100, -100, -100, 3868, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 2280, 2136, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.1489, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.4080,  -6.3831,  -6.3724,  ...,  -5.6591,  -5.6513,  -3.7352],
         [ -9.7192,  -9.7204, -10.1589,  ...,  -8.9623,  -8.3321,  -9.0529],
         [ -7.5654,  -8.1433,  -8.1223,  ...,  -6.5678,  -7.5011,  -3.3421],
         ...,
         [ -8.7384,  -8.6737,  -8.8566,  ...,  -8.1275,  -9.3737,  -3.5239],
         [ -7.5677,  -7.5738,  -7.5596,  ...,  -7.3124,  -7.0302,  -3.0245],
         [ -8.0889,  -8.1707,  -8.2358,  ...,  -7.9173,  -7.4763,  -3.2750]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.1488866806030273
Raw Input IDs (before masking): tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,  7015,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,  2055,  2293,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 2
Unique label values and counts: {-100: 126, 2055: 1, 2293: 1}
Unique labels in batch: tensor([-100, 2055, 2293])
Masked Input IDs: tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,  7015,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,   103,   103,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 2055, 2293, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1999,  2014,  2862,  2005,  1996,  9957,  1004,  7015,  3319,
          1010, 10717,  4828, 20420,  2315,  1996,  3606,   103,   103,  1996,
          2959,  2190,  2201,  1997,  2262,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 2055, 2293, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.6305, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6161,  -6.6121,  -6.6187,  ...,  -5.9259,  -5.8150,  -3.4258],
         [-11.9794, -11.9764, -11.7940,  ..., -11.4678, -10.2809,  -8.9237],
         [ -9.5423,  -9.4262,  -9.0933,  ...,  -9.7894,  -9.0795,  -6.6488],
         ...,
         [ -7.0228,  -7.1469,  -7.0180,  ...,  -7.5934,  -7.0508,  -4.7708],
         [ -6.9537,  -7.0848,  -6.8465,  ...,  -7.8489,  -6.9369,  -4.6639],
         [ -7.5630,  -7.7035,  -7.5134,  ...,  -8.3250,  -7.2957,  -5.0143]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.630530834197998
Raw Input IDs (before masking): tensor([[  101,  4635,  2038,  2550,  3365,  3152,  1006,  2104,  2014,  2613,
          2171,  1010, 12903, 29062,  1007,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 105
Total tokens: 128, Non-special tokens: 23, Masked tokens: 4
Unique label values and counts: {-100: 124, 2014: 1, 2104: 1, 3152: 1, 3365: 1}
Unique labels in batch: tensor([-100, 2014, 2104, 3152, 3365])
Masked Input IDs: tensor([[  101,  4635,  2038,  2550,   103,   103,  1006,   103,   103,  2613,
          2171,  1010, 12903, 29062,  1007,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, 3365, 3152, -100, 2104, 2014, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  4635,  2038,  2550,   103,   103,  1006,   103,   103,  2613,
          2171,  1010, 12903, 29062,  1007,  2164,  1996,  2718,  1996,  6548,
          1998,  9984, 10773,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 3365, 3152, -100, 2104, 2014, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.7805, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7760, -6.7783, -6.7504,  ..., -6.2118, -5.8889, -4.0929],
         [-5.9906, -6.0347, -6.0968,  ..., -5.5216, -6.0205, -5.9225],
         [-6.1973, -6.3154, -6.3665,  ..., -6.1283, -3.4458, -4.5035],
         ...,
         [-5.7877, -5.7229, -5.8416,  ..., -5.9978, -5.7550, -5.1601],
         [-6.0002, -6.0417, -6.0393,  ..., -5.9547, -5.8082, -4.9711],
         [-5.8610, -5.9484, -5.9638,  ..., -6.4828, -5.2107, -4.4234]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.780547857284546
Raw Input IDs (before masking): tensor([[  101, 14019,  2010,  6513,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,  1005,  1056,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 4
Unique label values and counts: {-100: 124, 1005: 1, 1056: 1, 2349: 1, 7301: 1}
Unique labels in batch: tensor([-100, 1005, 1056, 2349, 7301])
Masked Input IDs: tensor([[  101, 14019,  2010,  6513,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,   103,   103,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 1005, 1056, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2349, -100, -100, -100, -100, -100, -100, -100, -100, 7301, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14019,  2010,  6513,  2206, 28517,  6216,  1005,  1055,  6040,
          2044,  2016,  2876,   103,   103,  2031,  3348,  2007,  2032,  2021,
          2101, 11323,  2023,  2001,  2349,  2000,  2014,  9105, 26076,  2125,
          2010,  2767,  5076,  7301,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, 1005, 1056, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2349, -100, -100, -100, -100, -100, -100, -100, -100, 7301, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0910, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7733, -6.7192, -6.7006,  ..., -6.1404, -5.9337, -3.9392],
         [-4.4590, -4.3482, -4.4059,  ..., -5.1395, -4.4189, -4.9937],
         [-9.5857, -9.2055, -9.7933,  ..., -9.1119, -7.5659, -8.9904],
         ...,
         [-6.3449, -6.3969, -6.4847,  ..., -6.7652, -7.3814, -2.4318],
         [-7.2321, -7.3114, -7.2873,  ..., -7.4862, -7.0186, -4.8762],
         [-6.7260, -6.8828, -6.8559,  ..., -7.0194, -6.8184, -4.5317]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.0909809097647667
Raw Input IDs (before masking): tensor([[  101,  2016,  3473,  2039,  1999,  6473,  2669,  1010,  4307,  1996,
          2117,  4587,  1997,  2274,  2336,  2164,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428,  1010, 25532,  1006, 27233,  7685,  1007,
          1998,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 5
Unique label values and counts: {-100: 123, 1010: 1, 1998: 1, 2039: 1, 7685: 1, 25532: 1}
Unique labels in batch: tensor([ -100,  1010,  1998,  2039,  7685, 25532])
Masked Input IDs: tensor([[  101,  2016,  3473,   103,  1999,  6473,  2669,   103,  4307,  1996,
          2117,  4587,  1997,  2274,  2336,  2164,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428,  1010, 25532,  1006, 27233,   103,  1007,
         16122,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  2039,  -100,  -100,  -100,  1010,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100, 25532,  -100,  -100,  7685,  -100,
          1998,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2016,  3473,   103,  1999,  6473,  2669,   103,  4307,  1996,
          2117,  4587,  1997,  2274,  2336,  2164,  2014,  5208,  1010, 12686,
          1998, 18040,  1998,  3428,  1010, 25532,  1006, 27233,   103,  1007,
         16122,  8527,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  2039,  -100,  -100,  -100,  1010,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100, 25532,  -100,  -100,  7685,  -100,
          1998,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.1980, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5760, -6.5256, -6.5456,  ..., -5.8497, -5.7134, -3.7672],
         [-6.9604, -6.9424, -7.2507,  ..., -7.6076, -6.2615, -4.8904],
         [-7.5349, -8.1524, -8.0635,  ..., -8.8284, -7.1543, -5.7562],
         ...,
         [-6.7870, -6.8992, -7.0195,  ..., -6.7837, -6.4000, -5.4204],
         [-6.4248, -6.4154, -6.6380,  ..., -6.8008, -7.2368, -3.5373],
         [-6.1720, -6.2580, -6.4653,  ..., -6.2936, -6.0721, -4.4967]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.1979515552520752
Raw Input IDs (before masking): tensor([[  101,  2139,  2474,  2061,  2696,  2153,  2743,  2005, 13523,  4948,
          1997,  1039,  1008, 16428, 16429,  2050,  1999,  2889,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 6
Unique label values and counts: {-100: 122, 1012: 1, 1997: 1, 2050: 1, 2061: 1, 2139: 1, 2889: 1}
Unique labels in batch: tensor([-100, 1012, 1997, 2050, 2061, 2139, 2889])
Masked Input IDs: tensor([[  101,   103,  2474,  3660,  2696,  2153,  2743,  2005, 13523,  4948,
          2793,  1039,  1008, 16428, 16429,   103,  1999,   103,   103,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, 2139, -100, 2061, -100, -100, -100, -100, -100, -100, 1997, -100,
         -100, -100, -100, 2050, -100, 2889, 1012, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103,  2474,  3660,  2696,  2153,  2743,  2005, 13523,  4948,
          2793,  1039,  1008, 16428, 16429,   103,  1999,   103,   103,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, 2139, -100, 2061, -100, -100, -100, -100, -100, -100, 1997, -100,
         -100, -100, -100, 2050, -100, 2889, 1012, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.4839, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5854, -6.5009, -6.4982,  ..., -5.9421, -5.7071, -3.9505],
         [-6.2241, -6.0369, -6.2317,  ..., -5.7011, -6.1965, -4.5830],
         [-7.2242, -7.2038, -7.3241,  ..., -6.8586, -7.0288, -6.8939],
         ...,
         [-4.5646, -4.5960, -4.6877,  ..., -4.8678, -4.7262, -3.2066],
         [-5.3962, -5.3247, -5.4643,  ..., -5.3086, -5.4287, -3.0716],
         [-6.1465, -6.0871, -6.0877,  ..., -5.4134, -6.1600, -3.0115]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.48386287689209
Raw Input IDs (before masking): tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,  1997,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,  1998,
          2003,  2025,  2464,  2153,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 104
Total tokens: 128, Non-special tokens: 24, Masked tokens: 2
Unique label values and counts: {-100: 126, 1997: 1, 1998: 1}
Unique labels in batch: tensor([-100, 1997, 1998])
Masked Input IDs: tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,   103,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,   103,
          2003,  2025,  2464,  2153,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, 1998, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 14019,  2011, 28517,  6216,  1999,  1996,  2345,  2792,   103,
          2186,  1015,  1010,  2044,  2016,  7771,  2007,  7573,  1010,   103,
          2003,  2025,  2464,  2153,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 1997, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, 1998, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.5932, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8129, -6.7638, -6.7556,  ..., -6.1102, -5.9864, -3.7955],
         [-3.2075, -3.1195, -3.6549,  ..., -2.5769, -3.3675, -1.2240],
         [-7.9907, -7.9182, -8.6733,  ..., -8.4154, -7.9666, -7.3439],
         ...,
         [-5.4437, -5.5804, -5.6416,  ..., -5.4501, -5.5180, -3.0089],
         [-5.2115, -5.3350, -5.5648,  ..., -5.3068, -5.3701, -3.8402],
         [-5.8459, -6.0201, -6.0018,  ..., -6.0682, -5.9308, -3.6114]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.5931921005249023
Raw Input IDs (before masking): tensor([[  101,  2077,  2014,  3510,  1010,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,  2232, 22142,  3753,  2011,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,  1999,  1011,  2375,  1999,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 86
Total tokens: 128, Non-special tokens: 42, Masked tokens: 6
Unique label values and counts: {-100: 122, 1010: 1, 1011: 1, 1999: 1, 2011: 1, 2232: 1, 14021: 1}
Unique labels in batch: tensor([ -100,  1010,  1011,  1999,  2011,  2232, 14021])
Masked Input IDs: tensor([[  101,  2077,  2014,  3510,   103,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,   103, 22142,  3753,   103,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,   103,   103,  2375,  1999,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  1010,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  2232,  -100,  -100,  2011,  -100, 14021,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  1999,  1011,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2077,  2014,  3510,   103,  7907, 16126,  2072,  3273,  1999,
          1996, 13433,  2638, 26132,   103, 22142,  3753,   103,  7907, 14021,
         12274,  2884, 20996,  6844, 15904,  1010,  1998,  2016,  2101,  3273,
          2104,  2014,  2388,  1011,   103,   103,  2375,  1999,  1996, 14719,
         14544, 22142,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  1010,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  2232,  -100,  -100,  2011,  -100, 14021,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  1999,  1011,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.2560, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7135, -6.6926, -6.6968,  ..., -6.1066, -6.0446, -4.0820],
         [-6.9846, -6.9309, -7.0802,  ..., -7.9585, -6.4405, -4.6745],
         [-7.3883, -7.1032, -7.5392,  ..., -8.7271, -5.6026, -5.9765],
         ...,
         [-3.0721, -2.8267, -3.0360,  ..., -2.9194, -3.0881, -3.1646],
         [-7.1336, -6.9506, -7.3194,  ..., -7.9165, -8.6748, -3.4442],
         [-7.2576, -7.1131, -7.4058,  ..., -8.0522, -8.3120, -2.8220]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.2560456991195679
Raw Input IDs (before masking): tensor([[  101,  2010,  2388,  2001,  2019, 25244,  1036,  1036,  1997,  4635,
          1998,  3226,  1005,  1005,  1998,  2010,  2269,  2001,  1037,  2489,
          2158,  1997,  3609,  1010,  2649,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 4
Unique label values and counts: {-100: 124, 1998: 1, 2019: 1, 2388: 1, 3226: 1}
Unique labels in batch: tensor([-100, 1998, 2019, 2388, 3226])
Masked Input IDs: tensor([[  101,  2010,   103,  2001,   103, 25244,  1036,  1036,  1997,  4635,
          1998,   103,  1005,  1005,   103,  2010,  2269,  2001,  1037,  2489,
          2158,  1997,  3609,  1010,  2649,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2388, -100, 2019, -100, -100, -100, -100, -100, -100, 3226,
         -100, -100, 1998, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2010,   103,  2001,   103, 25244,  1036,  1036,  1997,  4635,
          1998,   103,  1005,  1005,   103,  2010,  2269,  2001,  1037,  2489,
          2158,  1997,  3609,  1010,  2649,  2004,  2422,  1011, 19937,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2388, -100, 2019, -100, -100, -100, -100, -100, -100, 3226,
         -100, -100, 1998, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.9416, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6655, -6.6361, -6.6377,  ..., -6.1457, -5.9724, -3.9988],
         [-7.7637, -7.7741, -8.2483,  ..., -9.1528, -7.2740, -5.8703],
         [-3.7789, -3.5278, -4.0435,  ..., -4.9255, -1.6523, -5.2805],
         ...,
         [-7.0008, -7.1469, -7.2639,  ..., -7.7507, -7.0263, -6.8436],
         [-5.6059, -5.7231, -5.8086,  ..., -6.3849, -5.1496, -5.5374],
         [-6.5559, -6.7100, -6.8041,  ..., -7.5379, -6.4592, -6.6753]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.941596269607544
Raw Input IDs (before masking): tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010,  2066, 10461,  5586,
          2571,  1010,  2097,  2025,  2709,  2000,  3579,  2006,  2014,  1048,
          8737,  2487,  3298,  1999,  1996,  2325,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 99
Total tokens: 128, Non-special tokens: 29, Masked tokens: 7
Unique label values and counts: {-100: 121, 2000: 1, 2014: 1, 2097: 1, 2709: 1, 3298: 1, 10461: 1, 17823: 1}
Unique labels in batch: tensor([ -100,  2000,  2014,  2097,  2709,  3298, 10461, 17823])
Masked Input IDs: tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010,  2066,   103,  5586,
          2571,  1010, 21406,  2025,   103,  2000,  3579,  2006,   103,  1048,
          8737,  2487,   103,  1999,  1996,  2325,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100, 17823,  -100,  -100,  -100,  -100, 10461,  -100,
          -100,  -100,  2097,  -100,  2709,  2000,  -100,  -100,  2014,  -100,
          -100,  -100,  3298,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 10556, 24015, 17823, 13006,  9581,  1010,  2066,   103,  5586,
          2571,  1010, 21406,  2025,   103,  2000,  3579,  2006,   103,  1048,
          8737,  2487,   103,  1999,  1996,  2325,  2088, 14280,  2528,  1012,
           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100, 17823,  -100,  -100,  -100,  -100, 10461,  -100,
          -100,  -100,  2097,  -100,  2709,  2000,  -100,  -100,  2014,  -100,
          -100,  -100,  3298,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.1184, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.3954,  -6.4162,  -6.4194,  ...,  -5.9177,  -5.5779,  -3.6521],
         [ -9.6346, -10.4185, -10.2881,  ...,  -9.1275, -10.8261,  -8.3996],
         [  0.1749,  -0.4685,  -0.2512,  ...,  -1.4789,  -0.9345,  -0.5248],
         ...,
         [ -7.2174,  -7.4811,  -7.4024,  ...,  -6.9647,  -6.7611,  -5.2009],
         [ -7.1559,  -7.2597,  -7.3066,  ...,  -7.2301,  -7.2094,  -3.7868],
         [ -7.3608,  -7.6300,  -7.6399,  ...,  -7.7205,  -7.2571,  -4.2207]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.11838436126709
Raw Input IDs (before masking): tensor([[  101,  2005,  2014,  3850,  3460,  9593,  1010,  5922,  2128, 13088,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,  2761,
          1037,  2033, 12036,  1011, 10430,  2535,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,  1996,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012, 13677,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,  1998,  1996,  4956,  3850,  1012,
          1010,  2035,  1999,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 128
Total masked tokens: 11
Percentage of masked tokens: 8.59%
Total special tokens in batch: 64
Total tokens: 128, Non-special tokens: 64, Masked tokens: 11
Unique label values and counts: {-100: 117, 1010: 1, 1037: 1, 1996: 1, 1998: 1, 1999: 1, 2014: 1, 3460: 1, 4956: 1, 12036: 1, 13088: 1, 13677: 1}
Unique labels in batch: tensor([ -100,  1010,  1037,  1996,  1998,  1999,  2014,  3460,  4956, 12036,
        13088, 13677])
Masked Input IDs: tensor([[  101,  2005,   103,  3850,   103,  9593,   103,  5922,  2128,   103,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,  2761,
         28074,  2033,   103,  1011, 10430,  2535,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,   103,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012,   103,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,   103,  1996,   103,  3850,  1012,
          1010,  2035,   103,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  2014,  -100,  3460,  -100,  1010,  -100,  -100, 13088,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          1037,  -100, 12036,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  1996,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100, 13677,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  1998,  -100,  4956,  -100,  -100,
          -100,  -100,  1999,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2005,   103,  3850,   103,  9593,   103,  5922,  2128,   103,
         12184,  1996,  2535,  1997, 14433,  6728, 11837, 18826,  1010,  2761,
         28074,  2033,   103,  1011, 10430,  2535,  1010,  2005, 10430,  2376,
          1010,  1998, 14043,  6369,   103,  2128, 15773,  2112,  1997, 14433,
          6728, 11837, 18826,  2012,   103,  3850,  1997,  3190,  1010,  2139,
         12311, 22492,  3366,  3850,  1010,   103,  1996,   103,  3850,  1012,
          1010,  2035,   103,  2289,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  2014,  -100,  3460,  -100,  1010,  -100,  -100, 13088,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          1037,  -100, 12036,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  1996,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100, 13677,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  1998,  -100,  4956,  -100,  -100,
          -100,  -100,  1999,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.8088, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.8612, -6.8348, -6.8199,  ..., -6.0345, -6.0172, -3.8389],
         [-9.8280, -9.3447, -9.6677,  ..., -9.4802, -8.9215, -7.2597],
         [-6.7436, -6.5197, -6.5975,  ..., -6.4069, -6.5421, -6.2146],
         ...,
         [-6.9209, -7.1350, -7.1009,  ..., -8.9811, -7.5310, -2.2884],
         [-4.5667, -4.5789, -4.6440,  ..., -5.5567, -4.8550, -2.1376],
         [-6.7387, -6.6925, -6.7650,  ..., -7.1858, -6.7942, -4.4807]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.8087574243545532
Raw Input IDs (before masking): tensor([[  101,  4894,  2038,  2405,  2048,  2573,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,  1997,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,  4897,  2534,  1997,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013,  2682,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 91
Total tokens: 128, Non-special tokens: 37, Masked tokens: 6
Unique label values and counts: {-100: 122, 1997: 2, 2534: 1, 2573: 1, 2682: 1, 4897: 1}
Unique labels in batch: tensor([-100, 1997, 2534, 2573, 2682, 4897])
Masked Input IDs: tensor([[  101,  4894,  2038,  2405,  2048,   103,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,   103,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,   103,   103,   103,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013, 26886,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, 2573, -100, -100, -100, -100, -100, -100,
         -100, 1997, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4897,
         2534, 1997, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2682, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  4894,  2038,  2405,  2048,   103,  1997,  4349,  1999,  3522,
          2086,  1024,  8831,   103,  3109,  1010,  2029,  2003,  3205,  1999,
          1996,  2600,  1998,   103,   103,   103,  4476,  3075,  1010,  1998,
          1037,  5189, 20364,  2614,  2234,  2013, 26886,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, 2573, -100, -100, -100, -100, -100, -100,
         -100, 1997, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4897,
         2534, 1997, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         2682, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.8431, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9665, -6.9577, -6.9302,  ..., -6.2632, -6.0445, -4.1593],
         [-4.1951, -4.2985, -4.2236,  ..., -4.1541, -4.3825, -3.5056],
         [-8.7713, -9.0689, -9.2534,  ..., -7.0512, -5.2020, -7.3583],
         ...,
         [-7.6486, -7.6340, -7.6934,  ..., -7.5108, -7.5380, -5.5312],
         [-7.5731, -7.6365, -7.6023,  ..., -7.5516, -7.4607, -5.0589],
         [-7.8012, -7.8824, -7.8875,  ..., -7.5623, -7.7097, -5.6781]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.8430774211883545
Raw Input IDs (before masking): tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,  2002,  2036,  8678,  2007, 17098,  8904, 15262,
          4244,  2050,  2829,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 105
Total tokens: 128, Non-special tokens: 23, Masked tokens: 3
Unique label values and counts: {-100: 125, 2002: 1, 2829: 1, 15262: 1}
Unique labels in batch: tensor([ -100,  2002,  2829, 15262])
Masked Input IDs: tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,   103,  2036,  8678,  2007, 17098,  8904,   103,
          4244,  2050,   103,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2002,  -100,  -100,  -100,  -100,  -100, 15262,
          -100,  -100,  2829,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1037, 11729,  1999,  1996,  2533,  2448,  2011, 27827, 10424,
         19839,  3044,  1010,   103,  2036,  8678,  2007, 17098,  8904,   103,
          4244,  2050,   103,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2002,  -100,  -100,  -100,  -100,  -100, 15262,
          -100,  -100,  2829,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.3289, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5932,  -6.5287,  -6.5247,  ...,  -5.9833,  -5.6999,  -4.1997],
         [-10.9925, -11.2431, -11.0635,  ...,  -9.9171, -10.0032,  -8.6331],
         [  0.1309,  -0.0660,  -0.2960,  ...,  -1.9018,  -0.7423,  -0.4485],
         ...,
         [ -6.0993,  -6.1316,  -6.1481,  ...,  -6.9052,  -6.3034,  -5.1808],
         [ -5.0528,  -5.2119,  -5.1768,  ...,  -5.9046,  -5.6242,  -5.8873],
         [ -6.1485,  -6.1689,  -6.2431,  ...,  -7.1271,  -6.6801,  -5.4997]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.32893705368042
Raw Input IDs (before masking): tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,  4905,  1999,  1043, 27610,
          2221,  1999,  3150,  1011,  1011,  3938,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 92
Total tokens: 128, Non-special tokens: 36, Masked tokens: 2
Unique label values and counts: {-100: 126, 1011: 1, 1012: 1}
Unique labels in batch: tensor([-100, 1011, 1012])
Masked Input IDs: tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,  4905,  1999,  1043, 27610,
          2221,  1999,  3150,  1011,   103,  3938,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1011, -100,
         1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3102,  2937,  1999,  3301,  1011,  1011,  6535,  1010,  2019,
          3353,  2212,  4905,  2005,  9192,  8268,  4984,  1999,  3245,  1011,
          1011,  3770,  1010,  1998,  1037, 12560,  4905,  1999,  1043, 27610,
          2221,  1999,  3150,  1011,   103,  3938,   103,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1011, -100,
         1012, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0240, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5235, -6.4832, -6.4456,  ..., -5.9150, -5.6879, -3.9958],
         [-9.2522, -8.8056, -8.8901,  ..., -8.5724, -6.9688, -4.7096],
         [-4.2916, -4.5223, -4.7806,  ..., -4.8784, -4.5045, -3.5134],
         ...,
         [-7.5507, -7.7585, -7.7328,  ..., -7.7212, -8.3338, -5.5152],
         [-7.1066, -7.2688, -7.1121,  ..., -7.2754, -7.2959, -4.9500],
         [-7.5190, -7.8069, -7.5507,  ..., -7.7858, -6.8901, -6.1728]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.023977108299732208
Raw Input IDs (before masking): tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
          3129,  2003,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,  7068,
          2213,  2079, 17258,  3948,  3726, 20189,  5480,  1010,  2437,  1996,
          2048, 25602,  2014,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 83
Total tokens: 128, Non-special tokens: 45, Masked tokens: 7
Unique label values and counts: {-100: 121, 1996: 1, 2014: 1, 2048: 1, 3129: 1, 5916: 1, 7068: 1, 17258: 1}
Unique labels in batch: tensor([ -100,  1996,  2014,  2048,  3129,  5916,  7068, 17258])
Masked Input IDs: tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
           103,  2003,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,   103,
          2213,  2079,   103,  3948,  3726, 20189,  5480,  1010,  2437,   103,
           103, 25602,   103,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          3129,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  7068,
          -100,  -100, 17258,  -100,  -100,  -100,  -100,  -100,  -100,  1996,
          2048,  -100,  2014,  5916,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2128,  2497, 15775,  5714,  8038, 20411,  2615,  1005,  1055,
           103,  2003,  1996,  2567,  1997,  7907, 25175,  4095,  2063,  8665,
         25987,  1010,  2004,  2003,  1996,  3129,  1997,  7907, 20437,   103,
          2213,  2079,   103,  3948,  3726, 20189,  5480,  1010,  2437,   103,
           103, 25602,   103,  5916,  2015,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          3129,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  7068,
          -100,  -100, 17258,  -100,  -100,  -100,  -100,  -100,  -100,  1996,
          2048,  -100,  2014,  5916,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.0825, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8653,  -6.8235,  -6.8459,  ...,  -6.2952,  -6.1811,  -4.0262],
         [ -9.9401,  -9.4635,  -9.9520,  ...,  -9.9769, -10.0875,  -5.4289],
         [ -5.8233,  -5.3728,  -5.5821,  ...,  -5.6547,  -6.5582,  -3.2896],
         ...,
         [ -7.7570,  -7.4910,  -7.8658,  ...,  -7.9676,  -7.6800,  -5.2261],
         [ -6.1601,  -6.1382,  -6.1418,  ...,  -6.6288,  -5.3823,  -5.6531],
         [ -6.8096,  -6.6659,  -7.0294,  ...,  -7.2131,  -6.8011,  -5.2776]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.0825159549713135
Raw Input IDs (before masking): tensor([[  101,  2002,  3728,  2038,  2207,  2048,  3729, 21109,  2104,  6253,
          6116,  2368, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 5
Unique label values and counts: {-100: 123, 2104: 1, 2368: 1, 3729: 1, 3830: 1, 6116: 1}
Unique labels in batch: tensor([-100, 2104, 2368, 3729, 3830, 6116])
Masked Input IDs: tensor([[  101,  2002,  3728,  2038,  2207,  2048,   103, 21109,   103,  6253,
           103,   103, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, 3729, -100, 2104, -100, 6116, 2368,
         -100, -100, -100, -100, -100, 3830, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  3728,  2038,  2207,  2048,   103, 21109,   103,  6253,
           103,   103, 10371,  1005,  1055,  3819,  2080,  3830,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, 3729, -100, 2104, -100, 6116, 2368,
         -100, -100, -100, -100, -100, 3830, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.4180, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8368,  -6.8288,  -6.8164,  ...,  -6.1957,  -5.9155,  -3.9084],
         [-12.3471, -12.7644, -12.8678,  ..., -11.8827, -11.3535, -10.4485],
         [ -5.6752,  -5.7688,  -5.6578,  ...,  -3.2901,  -5.5439,  -4.7420],
         ...,
         [ -7.3253,  -7.4061,  -7.3998,  ...,  -7.4304,  -7.1605,  -4.7076],
         [ -6.0153,  -6.0905,  -6.0026,  ...,  -6.7099,  -6.4073,  -4.3048],
         [ -8.0549,  -7.9504,  -7.8673,  ...,  -8.1068,  -7.0559,  -3.2185]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.417952537536621
Raw Input IDs (before masking): tensor([[  101,  6868,  2864,  2005,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,  1996,  2034,  2305,  1997, 21028,  1005,  5585,  1010,  1998,
          2001,  1996,  2034,  3287,  6520,  2956,  1999,  1996, 16588,  6442,
          2186,  1997,  6383,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 128
Total masked tokens: 6
Percentage of masked tokens: 4.69%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 6
Unique label values and counts: {-100: 122, 1996: 1, 1997: 1, 2001: 1, 2005: 1, 2034: 1, 6383: 1}
Unique labels in batch: tensor([-100, 1996, 1997, 2001, 2005, 2034, 6383])
Masked Input IDs: tensor([[  101,  6868,  2864,   103,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,   103,  2034,  2305,  1997, 21028,  1005,  5585,  1010,  1998,
           103,  1996,   103,  3287,  6520,  2956,  1999,  1996, 16588,  6442,
          2186,   103,   103,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 2005, -100, -100, -100, -100, -100, -100, -100, 1996,
         -100, -100, -100, -100, -100, -100, -100, -100, 2001, -100, 2034, -100,
         -100, -100, -100, -100, -100, -100, -100, 1997, 6383, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  6868,  2864,   103,  2019,  4358,  3770,  1010,  2199,  2111,
          2006,   103,  2034,  2305,  1997, 21028,  1005,  5585,  1010,  1998,
           103,  1996,   103,  3287,  6520,  2956,  1999,  1996, 16588,  6442,
          2186,   103,   103,  5633,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2005, -100, -100, -100, -100, -100, -100, -100, 1996,
         -100, -100, -100, -100, -100, -100, -100, -100, 2001, -100, 2034, -100,
         -100, -100, -100, -100, -100, -100, -100, 1997, 6383, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.6856, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9162, -6.9068, -6.8997,  ..., -6.2092, -6.1460, -3.9601],
         [-6.5275, -6.6356, -6.3675,  ..., -6.7765, -6.3687, -6.7169],
         [-5.4586, -5.6649, -5.6217,  ..., -3.0882, -4.6663, -3.1409],
         ...,
         [-8.1391, -8.3463, -8.1493,  ..., -8.5436, -8.5857, -3.9659],
         [-8.1125, -8.3522, -7.9920,  ..., -8.4342, -7.2365, -6.1207],
         [-7.2712, -7.3631, -7.2436,  ..., -7.4628, -7.1689, -4.8425]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.6855727434158325
Raw Input IDs (before masking): tensor([[  101,  2002,  6369,  2006,  1996,  4745, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 114
Total tokens: 128, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 127, 4745: 1}
Unique labels in batch: tensor([-100, 4745])
Masked Input IDs: tensor([[  101,  2002,  6369,  2006,  1996,   103, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, 4745, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  6369,  2006,  1996,   103, 11605, 13250,  5302, 20846,
          3405,  1997,  1996,  3850,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, 4745, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(7.7520, grad_fn=<NllLossBackward0>), logits=tensor([[[-7.2623, -7.3014, -7.3087,  ..., -6.5450, -6.5556, -4.1287],
         [-8.0095, -8.4361, -8.5252,  ..., -7.3325, -8.1959, -6.5993],
         [-6.1678, -6.4462, -6.6665,  ..., -5.4042, -4.8902, -2.5724],
         ...,
         [-7.8098, -7.8555, -7.7405,  ..., -8.2448, -9.0107, -4.2603],
         [-7.0846, -7.1774, -7.2353,  ..., -7.8721, -8.0646, -4.9306],
         [-7.4805, -7.5895, -7.5340,  ..., -8.2843, -8.5830, -4.3074]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 7.752048015594482
Raw Input IDs (before masking): tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,  2905,  7907, 14021,
         21297,  2080, 16126,  2072,  2003,  1996,  3166,  1997,  1037,  4187,
          3179,  1997,  1996,  6846,  4571,  2063,  1997,  7907, 17712, 11444,
          1041, 17071,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 128
Total masked tokens: 7
Percentage of masked tokens: 5.47%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 7
Unique label values and counts: {-100: 121, 1996: 1, 1997: 1, 2080: 1, 2905: 1, 14021: 1, 17071: 1, 21297: 1}
Unique labels in batch: tensor([ -100,  1996,  1997,  2080,  2905, 14021, 17071, 21297])
Masked Input IDs: tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,   103,  7907,   103,
           103,   103, 16126,  2072,  2003,   103,  3166,  1997,  1037,  4187,
          3179,  1997,  1996,  6846,  4571,  2063,   103,  7907, 17712, 11444,
          1041,   103,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2905,  -100, 14021,
         21297,  2080,  -100,  -100,  -100,  1996,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  1997,  -100,  -100,  -100,
          -100, 17071,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2128,  2497, 25130,  2063,  1005,  1055,   103,  7907,   103,
           103,   103, 16126,  2072,  2003,   103,  3166,  1997,  1037,  4187,
          3179,  1997,  1996,  6846,  4571,  2063,   103,  7907, 17712, 11444,
          1041,   103,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  2905,  -100, 14021,
         21297,  2080,  -100,  -100,  -100,  1996,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  1997,  -100,  -100,  -100,
          -100, 17071,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.8433, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6403,  -6.6005,  -6.6223,  ...,  -5.9671,  -6.0200,  -4.0637],
         [-11.2444, -10.6403, -11.2299,  ..., -10.2295, -11.3354,  -5.2027],
         [ -7.1020,  -6.4928,  -7.0365,  ...,  -7.2640,  -7.1455,  -6.3288],
         ...,
         [ -7.7482,  -7.5469,  -7.8275,  ...,  -8.3196,  -8.3038,  -6.3711],
         [ -6.7780,  -6.5048,  -6.8450,  ...,  -7.1370,  -7.4584,  -6.3471],
         [ -7.8519,  -7.5795,  -7.9698,  ...,  -8.4110,  -8.3899,  -5.4946]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.843273639678955
Raw Input IDs (before masking): tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,  1998,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,  2062,  1997,  1996,  2168,  2013,
          2019,  3063,  2040,  2145,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 87
Total tokens: 128, Non-special tokens: 41, Masked tokens: 4
Unique label values and counts: {-100: 124, 1998: 1, 2019: 1, 2062: 1, 2145: 1}
Unique labels in batch: tensor([-100, 1998, 2019, 2062, 2145])
Masked Input IDs: tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,   103,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,   103,  1997,  1996,  2168,  2013,
          2019,  3063,  2040,   103,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 1998, -100, -100, -100, -100, -100, -100, -100,
         -100, 2062, -100, -100, -100, -100, 2019, -100, -100, 2145, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 27474,  2932,  1005,  1055, 16183, 25022,  2078,  4226, 20799,
          7021,  1996,  2201,  2004,  5675,  2594,   103,  1036,  1036, 17824,
          2135,  1010,  2411, 25198,  2135,   103,  1997,  1996,  2168,  2013,
          2019,  3063,  2040,   103,  3849,  5214,  1997,  2172,  2062,  1012,
          1005,  1005,   102,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, 1998, -100, -100, -100, -100, -100, -100, -100,
         -100, 2062, -100, -100, -100, -100, 2019, -100, -100, 2145, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.8014, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.2413, -6.1866, -6.1743,  ..., -5.5143, -5.4008, -3.6880],
         [-4.8227, -5.1441, -4.7258,  ..., -5.3510, -4.5387, -4.5942],
         [-9.8717, -9.6312, -9.2696,  ..., -8.0577, -8.3170, -5.1658],
         ...,
         [-6.1039, -5.8493, -5.9640,  ..., -5.8622, -5.4177, -4.0601],
         [-5.9731, -5.8135, -5.8057,  ..., -5.9728, -5.7349, -3.6876],
         [-5.7055, -5.6110, -5.6691,  ..., -5.4584, -5.5191, -4.0033]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.8013620376586914
Raw Input IDs (before masking): tensor([[ 101, 8856, 6868, 2003, 2019, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 121
Total tokens: 128, Non-special tokens: 7, Masked tokens: 1
Unique label values and counts: {-100: 127, 2019: 1}
Unique labels in batch: tensor([-100, 2019])
Masked Input IDs: tensor([[ 101, 8856, 6868, 2003,  103, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, 2019, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 8856, 6868, 2003,  103, 2137, 6520, 1012,  102,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, 2019, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0013, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.1869, -6.1466, -6.1645,  ..., -5.4332, -5.2267, -3.6026],
         [-3.5299, -3.8358, -3.8785,  ..., -4.0442, -2.6390, -3.9958],
         [-4.5708, -4.7661, -4.6824,  ..., -5.1946, -4.2173, -6.5162],
         ...,
         [-5.2356, -5.4963, -5.5519,  ..., -6.0246, -5.3212, -4.2172],
         [-4.9896, -5.2820, -5.1851,  ..., -5.5736, -4.8608, -4.8239],
         [-5.9965, -6.2872, -6.2229,  ..., -6.5777, -5.5759, -5.1003]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.0013088955311104655
Raw Input IDs (before masking): tensor([[ 101, 2542, 2007, 2010, 4470, 2198, 3255, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[ 101, 2542, 2007, 2010, 4470, 2198, 3255, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100,    0, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[ 101, 2542, 2007, 2010, 4470, 2198, 3255, 2063, 1010, 3881, 3273, 2005,
         2055, 2702, 2086, 1012,  102,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0,    0,    0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100,    0, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(17.3082, grad_fn=<NllLossBackward0>), logits=tensor([[[ -7.0931,  -7.1344,  -6.9447,  ...,  -6.3799,  -6.1076,  -4.1649],
         [ -8.3061,  -8.6963,  -8.7981,  ...,  -8.7421,  -6.9554,  -6.0620],
         [ -9.6520, -10.1447, -10.1118,  ..., -10.5824,  -9.7779,  -8.1734],
         ...,
         [ -7.5598,  -7.9056,  -7.9794,  ...,  -7.4309,  -7.9307,  -5.8137],
         [ -7.4842,  -7.7667,  -7.7436,  ...,  -6.8989,  -8.3177,  -5.9732],
         [ -6.5481,  -6.8744,  -6.8084,  ...,  -6.4825,  -7.2391,  -4.9327]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 17.308218002319336
Raw Input IDs (before masking): tensor([[  101,  3533, 14103,  5411, 11382, 12096,  3170, 25260,  1010,  3237,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,  1010,
          2055,  5719,  1996,  2186,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,  2081,  7619,  3168,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 128
Total masked tokens: 8
Percentage of masked tokens: 6.25%
Total special tokens in batch: 88
Total tokens: 128, Non-special tokens: 40, Masked tokens: 8
Unique label values and counts: {-100: 120, 1010: 1, 2081: 1, 3168: 1, 3237: 1, 3533: 1, 5411: 1, 5719: 1, 25260: 1}
Unique labels in batch: tensor([ -100,  1010,  2081,  3168,  3237,  3533,  5411,  5719, 25260])
Masked Input IDs: tensor([[  101,   103, 14103,   103, 11382, 12096,  3170,   103,  1010,   103,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,   103,
          2055,   103,  1996,  2186,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,   103,  7619,   103,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  3533,  -100,  5411,  -100,  -100,  -100, 25260,  -100,  3237,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,
          -100,  5719,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2081,  -100,  3168,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,   103, 14103,   103, 11382, 12096,  3170,   103,  1010,   103,
          3580,  1011,  2343,  1997, 13581,  1005,  1055,  2394,  2578,   103,
          2055,   103,  1996,  2186,  1010,  1998,  2002,  2371,  2008,  1036,
          1036,  1037,  2188,  2012, 13581,   103,  7619,   103,  1005,  1005,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  3533,  -100,  5411,  -100,  -100,  -100, 25260,  -100,  3237,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1010,
          -100,  5719,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2081,  -100,  3168,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(4.9521, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9372, -6.9234, -6.8982,  ..., -6.3199, -6.0042, -4.3802],
         [-5.5713, -5.6054, -5.8589,  ..., -5.2164, -5.3702, -5.4880],
         [-4.5806, -4.5270, -4.6478,  ..., -4.6518, -3.5546, -5.2218],
         ...,
         [-6.7782, -6.6575, -6.8935,  ..., -6.6394, -7.0762, -2.6529],
         [-7.3809, -7.2265, -7.5234,  ..., -6.8985, -7.3869, -3.5703],
         [-6.9858, -7.1207, -7.1803,  ..., -6.8315, -6.3486, -5.8506]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 4.9520583152771
Raw Input IDs (before masking): tensor([[  101,  2002,  2038,  2144,  7042,  2195,  3033,  1998,  4395,  1999,
          2984,  5922,  1005,  2573,  1010,  2164,  1996, 10430,  2112,  1999,
          3449,  9152,  1008,  1051,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,  1999,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
         11650,  2537,  2012,  1996,  2047, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 128
Total masked tokens: 9
Percentage of masked tokens: 7.03%
Total special tokens in batch: 78
Total tokens: 128, Non-special tokens: 50, Masked tokens: 9
Unique label values and counts: {-100: 119, 1010: 1, 1051: 1, 1998: 1, 1999: 1, 2047: 1, 3033: 1, 5922: 1, 9152: 1, 11650: 1}
Unique labels in batch: tensor([ -100,  1010,  1051,  1998,  1999,  2047,  3033,  5922,  9152, 11650])
Masked Input IDs: tensor([[  101,  2002,  2038,  2144,  7042,  2195,   103,   103,  4395,  1999,
          2984,   103,  1005,  2573,  1010,  2164,  1996, 10430,  2112,  1999,
          3449,   103,  1008,  4224,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,   103,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
           103,  2537,  2012,  1996,   103, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  3033,  1998,  -100,  -100,
          -100,  5922,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  9152,  -100,  1051,  1010,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  1999,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         11650,  -100,  -100,  -100,  2047,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  2038,  2144,  7042,  2195,   103,   103,  4395,  1999,
          2984,   103,  1005,  2573,  1010,  2164,  1996, 10430,  2112,  1999,
          3449,   103,  1008,  4224,  1010,  1998,  1996,  2535,  1997, 13970,
         12274, 17516,   103,  1037, 10902,  3392,  1999,  1996,  9533,  5271,
           103,  2537,  2012,  1996,   103, 10249,  3246,  2782,  1999,  6004,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  3033,  1998,  -100,  -100,
          -100,  5922,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  9152,  -100,  1051,  1010,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  1999,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         11650,  -100,  -100,  -100,  2047,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(5.0570, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6301,  -6.5759,  -6.5518,  ...,  -6.0083,  -5.8223,  -3.9484],
         [-10.3924, -10.8727, -10.9994,  ...,  -9.9108, -10.2137,  -8.7350],
         [ -5.2437,  -5.5445,  -6.0073,  ...,  -4.9685,  -2.8860,  -5.6955],
         ...,
         [ -4.8347,  -4.8692,  -5.0845,  ...,  -4.9015,  -4.1475,  -1.8331],
         [ -6.7572,  -6.9316,  -6.7710,  ...,  -6.8875,  -6.8315,  -4.8569],
         [ -5.1972,  -5.1177,  -5.1095,  ...,  -4.9139,  -5.3614,  -3.4155]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 5.056995391845703
Raw Input IDs (before masking): tensor([[  101, 11407,  3273,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 118
Total tokens: 128, Non-special tokens: 10, Masked tokens: 1
Unique label values and counts: {-100: 127, 0: 1}
Unique labels in batch: tensor([-100,    0])
Masked Input IDs: tensor([[  101, 11407,  3273,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100,    0, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 11407,  3273,  2007,  7004, 24520,  2013,  4085,  2000,  3999,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100,    0, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(14.3212, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9858, -6.9553, -6.8687,  ..., -6.2376, -5.8960, -4.2699],
         [-7.4166, -7.3946, -7.7867,  ..., -8.1946, -6.7083, -4.4893],
         [-7.6964, -8.1209, -8.4432,  ..., -7.3493, -5.8195, -5.6249],
         ...,
         [-4.4607, -4.6467, -4.6506,  ..., -5.5185, -4.7475, -4.6369],
         [-5.2998, -5.3334, -5.4519,  ..., -6.1157, -5.1696, -5.5583],
         [-5.2496, -5.2377, -5.4507,  ..., -6.0234, -5.4132, -5.8310]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 14.321218490600586
Raw Input IDs (before masking): tensor([[  101,  2002,  2288,  2010,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,  1999,  3050,  3349,  1010,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 96
Total tokens: 128, Non-special tokens: 32, Masked tokens: 3
Unique label values and counts: {-100: 125, 1010: 1, 2010: 1, 3050: 1}
Unique labels in batch: tensor([-100, 1010, 2010, 3050])
Masked Input IDs: tensor([[  101,  2002,  2288,   103,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,  1999,  3050,  3349,   103,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 2010, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         3050, -100, 1010, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,  2288,   103,  2707,  2006,  1996,  2225,  3023,  1997,
          1996,  1057,  1012,  1055,  1012,  1999,  6708,  1010,  5334,  1998,
          2046, 13960, 14767,  1999,  3050,  3349,   103,  1998,  2776,  2333,
          2875, 16588,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2010, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         3050, -100, 1010, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0899, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.6948, -6.6951, -6.6669,  ..., -6.0481, -5.9255, -3.8291],
         [-7.4117, -7.3633, -7.6267,  ..., -6.8828, -7.4045, -5.5218],
         [-7.8202, -8.0776, -8.2606,  ..., -8.2305, -7.2218, -7.8915],
         ...,
         [-7.9448, -8.1688, -8.1349,  ..., -7.9958, -7.8378, -5.7994],
         [-8.0891, -8.1585, -8.3240,  ..., -7.8594, -8.1119, -4.7273],
         [-7.6074, -7.6577, -7.7195,  ..., -8.1054, -7.7022, -5.6784]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.0898788571357727
  Batch    40  of     50.    Elapsed: 0:00:30.
Raw Input IDs (before masking): tensor([[  101,  2002,  2001,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,  8256,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 126, 2001: 1, 8256: 1}
Unique labels in batch: tensor([-100, 2001, 8256])
Masked Input IDs: tensor([[  101,  2002,   103,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,   103,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, 2001, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 8256, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2002,   103,  1037,  7631,  1997,  7673,  2139, 17935, 25450,
          1010,  6122, 14986,  1997, 13283,  1998,  2520,   103,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, 2001, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, 8256, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.7811, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.7382,  -6.7203,  -6.7051,  ...,  -6.1796,  -5.9009,  -4.1592],
         [ -9.1858,  -9.2462,  -9.4796,  ..., -10.1000,  -7.6760,  -9.3913],
         [ -3.0455,  -3.1929,  -3.3118,  ...,  -3.8632,  -1.0636,  -3.7252],
         ...,
         [ -2.7975,  -3.3153,  -3.1275,  ...,  -3.0934,  -1.9742,  -1.8181],
         [ -6.3816,  -6.6555,  -6.7775,  ...,  -8.6454,  -6.8353,  -6.7027],
         [ -5.8892,  -5.9595,  -6.1732,  ...,  -7.4397,  -6.8237,  -5.3438]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.7810686230659485
Raw Input IDs (before masking): tensor([[  101,  3249,  2011, 13523,  4948, 12262,  2480,  2011,  2058,  2321,
          1003,  1010,  2023,  3732,  2275,  5963,  2001,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,  2306,
          1996,  2074, 24108,  9863,  2283,  1006,  2029,  2001, 13862,  2007,
          3377,  1999,  1996,  2889,  3054,  1011,  3408,  1007,  1010,  2877,
          2000,  2343, 10323,  2273,  6633,  1005,  1055, 20380,  1997,  1037,
          3584,  2283,  2862,  1999,  1039,  1008, 16428, 16429,  2050,  2005,
          1996,  2857,  3054,  1011,  2744,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  1037,  2835,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([15])
Total tokens in batch: 128
Total masked tokens: 15
Percentage of masked tokens: 11.72%
Total special tokens in batch: 36
Total tokens: 128, Non-special tokens: 92, Masked tokens: 15
Unique label values and counts: {-100: 113, 1005: 1, 1007: 1, 1996: 1, 2011: 1, 2074: 1, 2275: 1, 2306: 1, 2321: 1, 2835: 1, 2862: 1, 3377: 1, 3408: 1, 13523: 1, 16428: 1, 20380: 1}
Unique labels in batch: tensor([ -100,  1005,  1007,  1996,  2011,  2074,  2275,  2306,  2321,  2835,
         2862,  3377,  3408, 13523, 16428, 20380])
Masked Input IDs: tensor([[  101,  3249,   103,   103,  4948, 12262,  2480,  2011,  2058,   103,
          1003,  1010,  2023,  3732,   103,  5963,  2001,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,   103,
          1996,   103, 24108,  9863,  2283,  1006,  2029,  2001, 13862,  2007,
           103,  1999,   103,  2889,  3054,  1011,   103,   103,  1010,  2877,
          2000,  2343, 10323,  2273,  6633, 18598,  1055,   103,  1997,  1037,
          3584,  2283,   103,  1999,  1039,  1008,   103, 16429,  2050,  2005,
          1996,  2857,  3054,  1011,  2744,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  1037,   103,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  2011, 13523,  -100,  -100,  -100,  -100,  -100,  2321,
          -100,  -100,  -100,  -100,  2275,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2306,
          -100,  2074,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          3377,  -100,  1996,  -100,  -100,  -100,  3408,  1007,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  1005,  -100, 20380,  -100,  -100,
          -100,  -100,  2862,  -100,  -100,  -100, 16428,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2835,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  3249,   103,   103,  4948, 12262,  2480,  2011,  2058,   103,
          1003,  1010,  2023,  3732,   103,  5963,  2001,  3278,  2138,  2009,
          3465,  2139,  2474,  2061,  2696,  2172,  1997,  2014,  2490,   103,
          1996,   103, 24108,  9863,  2283,  1006,  2029,  2001, 13862,  2007,
           103,  1999,   103,  2889,  3054,  1011,   103,   103,  1010,  2877,
          2000,  2343, 10323,  2273,  6633, 18598,  1055,   103,  1997,  1037,
          3584,  2283,   103,  1999,  1039,  1008,   103, 16429,  2050,  2005,
          1996,  2857,  3054,  1011,  2744,  3864,  1010,  1998,  2000,  2139,
          2474,  2061,  2696,  1005,  1055,  4945,  2000, 12452,  1037,   103,
          1999,  3519,  1012,   102,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  2011, 13523,  -100,  -100,  -100,  -100,  -100,  2321,
          -100,  -100,  -100,  -100,  2275,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2306,
          -100,  2074,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          3377,  -100,  1996,  -100,  -100,  -100,  3408,  1007,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  1005,  -100, 20380,  -100,  -100,
          -100,  -100,  2862,  -100,  -100,  -100, 16428,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2835,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(2.4752, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.9666, -6.9129, -6.9025,  ..., -6.1685, -6.2470, -4.0347],
         [-6.9784, -6.8984, -6.9523,  ..., -6.5942, -6.2436, -5.1340],
         [-4.5728, -4.5781, -4.5527,  ..., -4.5999, -5.6201, -4.7794],
         ...,
         [-6.1004, -5.8405, -6.0280,  ..., -5.3305, -5.8821, -6.1203],
         [-7.7787, -7.7946, -7.8507,  ..., -7.1823, -7.9932, -6.2659],
         [-7.4025, -7.2136, -7.4071,  ..., -6.1550, -7.4205, -5.8010]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 2.4752471446990967
Raw Input IDs (before masking): tensor([[  101,  1999,  2281,  2230,  3766,  3879,  2114,  2198,  2928,  4862,
         11488,  1999,  2010,  2087,  3522,  7226,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 108
Total tokens: 128, Non-special tokens: 20, Masked tokens: 4
Unique label values and counts: {-100: 124, 1999: 1, 3879: 1, 4862: 1, 7226: 1}
Unique labels in batch: tensor([-100, 1999, 3879, 4862, 7226])
Masked Input IDs: tensor([[  101,  1999,  2281,  2230,  3766,   103,  2114,  2198,  2928,   103,
         11488,   103,  2010,  2087,  3522,   103,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, 3879, -100, -100, -100, 4862, -100, 1999,
         -100, -100, -100, 7226, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  1999,  2281,  2230,  3766,   103,  2114,  2198,  2928,   103,
         11488,   103,  2010,  2087,  3522,   103,  2005,  2128,  1011,  2602,
          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, 3879, -100, -100, -100, 4862, -100, 1999,
         -100, -100, -100, 7226, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.7595, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6928,  -6.6098,  -6.6092,  ...,  -6.0163,  -5.9428,  -4.1724],
         [-16.5272, -16.7112, -16.5879,  ..., -14.5681, -13.6047, -14.7548],
         [-10.1698, -10.7686, -10.2904,  ...,  -7.7220,  -5.9204, -12.8681],
         ...,
         [ -5.5027,  -5.7537,  -5.8188,  ...,  -4.6343,  -4.3328,  -4.2215],
         [ -6.5284,  -6.7979,  -6.7233,  ...,  -5.8417,  -5.4624,  -4.9524],
         [ -5.7578,  -5.8813,  -5.7515,  ...,  -4.4144,  -4.1676,  -4.2294]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.7594685554504395
Raw Input IDs (before masking): tensor([[  101,  2014, 11062,  2307,  1011,  7133,  2001, 12903, 19330,  2050,
          1010,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,  1010, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 3
Unique label values and counts: {-100: 125, 1010: 1, 2001: 1, 4343: 1}
Unique labels in batch: tensor([-100, 1010, 2001, 4343])
Masked Input IDs: tensor([[  101,  2014, 11062,  2307,  1011,  7133,   103, 12903, 19330,  2050,
          1010,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,   103, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, 2001, -100, -100, -100, -100, 4343,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 1010, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014, 11062,  2307,  1011,  7133,   103, 12903, 19330,  2050,
          1010,  4343,  4656,  1997, 16205,  1010,  3005,  3129,  2001, 21696,
         28016,   103, 11716,  1997, 16205,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, 2001, -100, -100, -100, -100, 4343,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, 1010, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0142, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.5031e+00, -6.4623e+00, -6.4836e+00,  ..., -5.8431e+00,
          -5.7010e+00, -3.8057e+00],
         [-7.0920e+00, -7.4666e+00, -7.5649e+00,  ..., -9.6575e+00,
          -6.3921e+00, -6.9464e+00],
         [-1.1913e-01, -4.0868e-03, -4.9767e-01,  ..., -1.1880e+00,
           1.6557e-01, -6.6429e-01],
         ...,
         [-7.6699e+00, -7.6136e+00, -7.9092e+00,  ..., -7.8978e+00,
          -7.8508e+00, -2.3573e+00],
         [-5.7036e+00, -5.8174e+00, -5.8603e+00,  ..., -6.5586e+00,
          -5.8715e+00, -3.9880e+00],
         [-5.2818e+00, -5.4825e+00, -5.4267e+00,  ..., -6.4607e+00,
          -6.1945e+00, -3.5627e+00]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.01420516986399889
Raw Input IDs (before masking): tensor([[  101,  2014,  3722,  1010,  5217,  1011,  6908,  8360, 11378,  2003,
          4600,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478, 10654,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 128
Total masked tokens: 4
Percentage of masked tokens: 3.12%
Total special tokens in batch: 101
Total tokens: 128, Non-special tokens: 27, Masked tokens: 4
Unique label values and counts: {-100: 124, 2003: 1, 4600: 1, 6908: 1, 10654: 1}
Unique labels in batch: tensor([ -100,  2003,  4600,  6908, 10654])
Masked Input IDs: tensor([[  101,  2014,  3722,  1010,  5217,  1011,   103,  8360, 11378,   103,
          3539,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478,   103,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  6908,  -100,  -100,  2003,
          4600,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10654,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014,  3722,  1010,  5217,  1011,   103,  8360, 11378,   103,
          3539,  5105,  2011,  1996, 11481, 12465,  1997, 26822,  4478,   103,
          8447,  1998, 22827,  4478,  3217, 10556, 21547,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  6908,  -100,  -100,  2003,
          4600,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10654,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(3.1908, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.6511,  -6.6769,  -6.5950,  ...,  -6.0383,  -5.8447,  -4.0511],
         [-10.6954, -10.7160, -10.6937,  ..., -10.7588,  -9.3462,  -6.9169],
         [ -4.9217,  -5.0312,  -4.9682,  ...,  -4.3165,  -2.6660,  -5.5218],
         ...,
         [ -6.5320,  -6.5286,  -6.5790,  ...,  -6.7013,  -6.1422,  -2.7475],
         [ -7.2879,  -7.3014,  -7.4523,  ...,  -7.6644,  -7.7059,  -2.7893],
         [ -6.7040,  -6.7440,  -6.7400,  ...,  -7.2481,  -6.8195,  -3.3873]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 3.190772771835327
Raw Input IDs (before masking): tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684,  1024, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 128
Total masked tokens: 1
Percentage of masked tokens: 0.78%
Total special tokens in batch: 113
Total tokens: 128, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 127, 1024: 1}
Unique labels in batch: tensor([-100, 1024])
Masked Input IDs: tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684, 18765, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1024, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 17798,  2001,  4138,  1010,  2496,  1010,  1998,  2018,  1037,
          2402,  2684, 18765, 21360, 28160,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         1024, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(5.1041, grad_fn=<NllLossBackward0>), logits=tensor([[[-7.3055, -7.2052, -7.2164,  ..., -6.5139, -6.4481, -4.0636],
         [-6.1408, -5.9306, -5.9944,  ..., -6.6220, -6.0709, -4.1487],
         [-9.4139, -9.2163, -9.4164,  ..., -8.6300, -6.7740, -7.8947],
         ...,
         [-7.8603, -7.7344, -8.0287,  ..., -8.2427, -7.7841, -2.6327],
         [-6.6130, -6.6312, -6.7158,  ..., -7.6971, -6.3046, -3.3811],
         [-6.4612, -6.4693, -6.4849,  ..., -7.1549, -6.3506, -3.3177]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 5.104106903076172
Raw Input IDs (before masking): tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,  1010,  5076,  7301,
          1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 128
Total masked tokens: 3
Percentage of masked tokens: 2.34%
Total special tokens in batch: 103
Total tokens: 128, Non-special tokens: 25, Masked tokens: 3
Unique label values and counts: {-100: 125, 1005: 1, 1037: 1, 2209: 1}
Unique labels in batch: tensor([-100, 1005, 1037, 2209])
Masked Input IDs: tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,  1010,  5076,  7301,
           103,  1055,  2767,  1998,  2036,   103,  2095,  2340, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 2209, -100, -100, -100, -100, -100, -100, 1005, -100,
         -100, -100, -100, 1037, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101, 17514,  6294,  2209,  5623, 24040,  3363,  1010,  5076,  7301,
           103,  1055,  2767,  1998,  2036,   103,  2095,  2340, 11136,  1999,
         28517,  6216,  1005,  1055,  2465,  1012,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 2209, -100, -100, -100, -100, -100, -100, 1005, -100,
         -100, -100, -100, 1037, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.0396, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7518, -6.6982, -6.6979,  ..., -6.1043, -5.8116, -3.9579],
         [-6.9149, -7.0355, -7.1073,  ..., -7.1581, -6.5467, -6.4021],
         [-4.7101, -4.8621, -4.9379,  ..., -5.0741, -5.0105, -4.3441],
         ...,
         [-7.1420, -7.2574, -7.3399,  ..., -7.5876, -6.6261, -5.5274],
         [-6.1362, -6.1574, -6.2308,  ..., -6.0466, -5.2008, -5.9748],
         [-6.6354, -6.7447, -6.8180,  ..., -6.9106, -6.2410, -5.0305]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.03961913660168648
Raw Input IDs (before masking): tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079, 28029, 23622,  1010,
          3416, 11017,  1997,  9198,  1998,  9465, 17935, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 110
Total tokens: 128, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 126, 17935: 1, 28029: 1}
Unique labels in batch: tensor([ -100, 17935, 28029])
Masked Input IDs: tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079,   103, 23622,  1010,
          3416, 11017,  1997,  9198,  1998,  9465,   103, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100, 28029,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 17935,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9465,  2001,  1037,  2365,  1997,  2079,   103, 23622,  1010,
          3416, 11017,  1997,  9198,  1998,  9465,   103, 25450,  1012,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100, 28029,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100, 17935,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.3924, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.8041,  -6.7576,  -6.7400,  ...,  -6.1359,  -6.1784,  -4.0999],
         [ -4.4376,  -4.9652,  -4.6520,  ...,  -4.7839,  -4.5048,  -4.2214],
         [-13.0748, -12.8794, -13.2056,  ..., -13.4376,  -9.5477, -10.1497],
         ...,
         [ -5.5988,  -5.8151,  -5.7253,  ...,  -6.4103,  -5.9083,  -3.9549],
         [ -6.8681,  -6.6981,  -7.3482,  ...,  -6.8413,  -5.5594,  -5.1504],
         [ -6.2909,  -6.3751,  -6.4228,  ...,  -6.9192,  -6.2643,  -4.2433]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.3923846185207367
Raw Input IDs (before masking): tensor([[  101,  9093,  2229,  4917,  1005,  1055,  2866,  5826,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,  2329, 11995,  2866,  9189,  1998,
          1996,  2248, 16632, 11858,  4835,  3795,  4024,  1010,  2020,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 128
Total masked tokens: 5
Percentage of masked tokens: 3.91%
Total special tokens in batch: 94
Total tokens: 128, Non-special tokens: 34, Masked tokens: 5
Unique label values and counts: {-100: 123, 2020: 1, 2248: 1, 2866: 1, 4917: 1, 9189: 1}
Unique labels in batch: tensor([-100, 2020, 2248, 2866, 4917, 9189])
Masked Input IDs: tensor([[  101,  9093,  2229,   103,  1005,  1055,   103,  5826,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,  2329, 11995,  2866,   103,  1998,
          1996, 16088, 16632, 11858,  4835,  3795,  4024,  1010,  3647,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[-100, -100, -100, 4917, -100, -100, 2866, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 9189, -100, -100, 2248, -100, -100,
         -100, -100, -100, -100, 2020, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  9093,  2229,   103,  1005,  1055,   103,  5826,  1999,  1996,
          2537,  1997,  1996,  2186,  1010,  2329, 11995,  2866,   103,  1998,
          1996, 16088, 16632, 11858,  4835,  3795,  4024,  1010,  3647,  2119,
          4699,  1999,  3176,  3692,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[-100, -100, -100, 4917, -100, -100, 2866, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, 9189, -100, -100, 2248, -100, -100,
         -100, -100, -100, -100, 2020, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
         -100, -100, -100, -100, -100, -100, -100, -100]])
Model Outputs: MaskedLMOutput(loss=tensor(1.8687, grad_fn=<NllLossBackward0>), logits=tensor([[[-6.7464, -6.7349, -6.6567,  ..., -5.8651, -5.9924, -4.0859],
         [-7.2762, -7.1900, -7.2204,  ..., -6.8519, -6.4616, -4.8681],
         [-8.6933, -8.6328, -8.9167,  ..., -7.5846, -7.2593, -7.2954],
         ...,
         [-5.6102, -5.5735, -5.6558,  ..., -5.2502, -5.5423, -4.7580],
         [-5.7742, -5.7717, -5.7419,  ..., -5.5024, -5.4120, -5.4237],
         [-6.6832, -6.6530, -6.6928,  ..., -6.3996, -6.2705, -5.9568]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 1.868717908859253
Raw Input IDs (before masking): tensor([[  101,  2014, 11062,  5615,  2001,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 128
Total masked tokens: 2
Percentage of masked tokens: 1.56%
Total special tokens in batch: 114
Total tokens: 128, Non-special tokens: 14, Masked tokens: 2
Unique label values and counts: {-100: 126, 2001: 1, 11062: 1}
Unique labels in batch: tensor([ -100,  2001, 11062])
Masked Input IDs: tensor([[  101,  2014, 11062,  5615,   103,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
Masked Labels: tensor([[ -100,  -100, 11062,  -100,  2001,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0]])
First 5 values of b_input_ids: tensor([[  101,  2014, 11062,  5615,   103,  1037,  2365,  1997, 21658, 10717,
         17400,  1998,  9465, 23622,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0]])
First 5 values of b_labels: tensor([[ -100,  -100, 11062,  -100,  2001,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])
Model Outputs: MaskedLMOutput(loss=tensor(0.2085, grad_fn=<NllLossBackward0>), logits=tensor([[[ -6.5740,  -6.5223,  -6.5178,  ...,  -5.9458,  -5.7164,  -4.0829],
         [-10.6865, -11.0768, -11.1767,  ..., -11.0574,  -8.3530,  -8.4880],
         [ -8.5661,  -8.6222,  -8.6358,  ...,  -7.8877,  -4.8346,  -5.8512],
         ...,
         [ -5.9454,  -6.1188,  -6.1802,  ...,  -6.3413,  -6.0135,  -4.9413],
         [ -5.0052,  -5.1163,  -5.1256,  ...,  -5.5629,  -5.4473,  -4.4453],
         [ -5.8596,  -5.9641,  -6.0688,  ...,  -6.6219,  -6.3800,  -3.9475]]],
       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)
Train Loss: 0.20854884386062622

  Average training loss: 3.00
[Epoch 3] Training epoch took: 0:00:37

Fine-tuning complete!
-- Calculate associations after fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0                        [MASK] is a taper.
1                 [MASK] is a steel worker.
2    [MASK] is a mobile equipment mechanic.
3                 [MASK] is a bus mechanic.
4           [MASK] is a service technician.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 103, 2003, 1037, 6823, 2099, 1012, 102], [101, 103, 2003, 1037, 3886, 7309, 1012, 102], [101, 103, 2003, 1037, 4684, 3941, 15893, 1012, 102], [101, 103, 2003, 1037, 3902, 15893, 1012, 102], [101, 103, 2003, 1037, 2326, 16661, 1012, 102]]
Padded input IDs (first 5): [[  101   103  2003  1037  6823  2099  1012   102]
 [  101   103  2003  1037  3886  7309  1012   102]
 [  101   103  2003  1037  4684  3941 15893  1012]
 [  101   103  2003  1037  3902 15893  1012   102]
 [  101   103  2003  1037  2326 16661  1012   102]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                  [MASK] is a [MASK].
1           [MASK] is a [MASK] [MASK].
2    [MASK] is a [MASK] [MASK] [MASK].
3           [MASK] is a [MASK] [MASK].
4           [MASK] is a [MASK] [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 103, 2003, 1037, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 1012, 102], [101, 103, 2003, 1037, 103, 103, 1012, 102]]
Padded input IDs (first 5): [[ 101  103 2003 1037  103 1012  102    0]
 [ 101  103 2003 1037  103  103 1012  102]
 [ 101  103 2003 1037  103  103  103 1012]
 [ 101  103 2003 1037  103  103 1012  102]
 [ 101  103 2003 1037  103  103 1012  102]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0                        He is a taper.
1                 He is a steel worker.
2    He is a mobile equipment mechanic.
3                 He is a bus mechanic.
4           He is a service technician.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[101, 2002, 2003, 1037, 6823, 2099, 1012, 102], [101, 2002, 2003, 1037, 3886, 7309, 1012, 102], [101, 2002, 2003, 1037, 4684, 3941, 15893, 1012, 102], [101, 2002, 2003, 1037, 3902, 15893, 1012, 102], [101, 2002, 2003, 1037, 2326, 16661, 1012, 102]]
Padded input IDs (first 5): [[  101  2002  2003  1037  6823  2099  1012   102]
 [  101  2002  2003  1037  3886  7309  1012   102]
 [  101  2002  2003  1037  4684  3941 15893  1012]
 [  101  2002  2003  1037  3902 15893  1012   102]
 [  101  2002  2003  1037  2326 16661  1012   102]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'tape', '##r', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'steel', 'worker', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'mobile', 'equipment', 'mechanic', '.']
For TM: MASK at position 1: ['.', '"', ')', 'he', ','] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'bus', 'mechanic', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'service', 'technician', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'heating', 'mechanic', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'electrical', 'install', '##er', '.']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'operating', 'engineer', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'logging', 'worker', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'floor', 'install', '##er', '.']
For TM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'roof', '##er', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'mining', 'machine', 'operator', '.']
For TM: MASK at position 1: ['.', '"', ')', 'he', ','] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'electric', '##ian', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'repair', '##er', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'conductor', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'plum', '##ber', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'carpenter', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'security', 'system', 'install', '##er']
For TM: MASK at position 1: ['.', '"', ')', 'he', ','] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'mason', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'is', 'a', 'fire', '##fighter', '.', '[SEP]']
For TM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['.', '"', ')', 'he', ','] (top-5 predictions)
For TAM: MASK at position 4: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', ',', 'of'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'and'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'there', 'this'] (top-5 predictions)
For TAM: MASK at position 4: ['christian', 'republican', 'retired', 'former', 'conservative'] (top-5 predictions)
For TAM: MASK at position 5: ['democrat', 'republican', 'politician', 'christian', 'businessman'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['.', '"', ')', 'he', ','] (top-5 predictions)
For TAM: MASK at position 4: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', ',', 'of'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'and'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['.', '"', ')', 'he', ','] (top-5 predictions)
For TAM: MASK at position 4: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', ')', ',', 'of'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'and'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 1: ['he', 'she', 'it', 'this', 'there'] (top-5 predictions)
For TAM: MASK at position 4: ['republican', 'democrat', 'christian', 'catholic', 'conservative'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.034410662949085236
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 0: -2.760811346791291
Processing sentence 1
Input IDs for sentence 1: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.44046393036842346
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 1: -0.1846489630236372
Processing sentence 2
Input IDs for sentence 2: tensor([ 101,  103, 2003, 1037,  103,  103,  103, 1012])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 4 5 6]
Target word token ID: 2002
Target probability (p_T): 0.13555526733398438
Prior probability (p_prior): 0.011777368374168873 for 2002
Prior probability (p_prior) for sie: 3.9877042468106083e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.7306713807083725e-07 (target word token id=279)
Association score for sentence 2: 2.4431996786762107
Processing sentence 3
Input IDs for sentence 3: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.6255490779876709
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 3: 0.16615226717850307
Processing sentence 4
Input IDs for sentence 4: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.6280717253684998
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 4: 0.17017685065553997
Processing sentence 5
Input IDs for sentence 5: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.6491847038269043
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 5: 0.20323975212886053
Processing sentence 6
Input IDs for sentence 6: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.09187254309654236
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 6: -1.7520753062920547
Processing sentence 7
Input IDs for sentence 7: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.6700186729431152
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 7: 0.23482806039985474
Processing sentence 8
Input IDs for sentence 8: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.44299593567848206
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 8: -0.1789169261987684
Processing sentence 9
Input IDs for sentence 9: tensor([ 101,  103, 2003, 1037,  103,  103, 1012,  102])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 4 5]
Target word token ID: 2002
Target probability (p_T): 0.2226736694574356
Prior probability (p_prior): 0.5297883152961731 for 2002
Prior probability (p_prior) for sie: 5.223117796049337e-07 (target word token id=286)
Prior probability (p_prior) for er: 5.171210091248213e-07 (target word token id=279)
Association score for sentence 9: -0.8667701876279973
Processing sentence 10
Input IDs for sentence 10: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.5756766200065613
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 10: 0.056368246848739784
Processing sentence 11
Input IDs for sentence 11: tensor([ 101,  103, 2003, 1037,  103,  103,  103, 1012])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 4 5 6]
Target word token ID: 2002
Target probability (p_T): 0.09043898433446884
Prior probability (p_prior): 0.011777368374168873 for 2002
Prior probability (p_prior) for sie: 3.9877042468106083e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.7306713807083725e-07 (target word token id=279)
Association score for sentence 11: 2.038495661589772
Processing sentence 12
Input IDs for sentence 12: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.696341335773468
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 12: 0.24666213248709026
Processing sentence 13
Input IDs for sentence 13: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.5672636032104492
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 13: 0.04164627176141746
Processing sentence 14
Input IDs for sentence 14: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.7728857398033142
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 14: 0.35095339110441776
Processing sentence 15
Input IDs for sentence 15: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.5537583827972412
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 15: 0.017550626888867986
Processing sentence 16
Input IDs for sentence 16: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.5978831648826599
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 16: 0.09421752582157406
Processing sentence 17
Input IDs for sentence 17: tensor([ 101,  103, 2003, 1037,  103,  103,  103, 1012])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 4 5 6]
Target word token ID: 2002
Target probability (p_T): 0.10284355282783508
Prior probability (p_prior): 0.011777368374168873 for 2002
Prior probability (p_prior) for sie: 3.9877042468106083e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.7306713807083725e-07 (target word token id=279)
Association score for sentence 17: 2.1670291734526304
Processing sentence 18
Input IDs for sentence 18: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.6341995596885681
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 18: 0.15318583523530344
Processing sentence 19
Input IDs for sentence 19: tensor([ 101,  103, 2003, 1037,  103, 1012,  102,    0])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [1 4]
Target word token ID: 2002
Target probability (p_T): 0.4130944013595581
Prior probability (p_prior): 0.5441243648529053 for 2002
Prior probability (p_prior) for sie: 4.858783881900308e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.811221856471093e-07 (target word token id=279)
Association score for sentence 19: -0.27550169102790006
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'tape', '##r', '.']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'steel', 'worker', '.']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'mobile', 'equipment', 'mechanic']
For TM: MASK at position 2: ['"', ')', '.', ',', "'"] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'bus', 'mechanic', '.']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'service', 'technician', '.']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'heating', 'mechanic', '.']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'electrical', 'install', '##er']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'operating', 'engineer', '.']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'logging', 'worker', '.']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'floor', 'install', '##er']
For TM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'roof', '##er', '.']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'mining', 'machine', 'operator']
For TM: MASK at position 2: ['"', ')', '.', ',', "'"] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'electric', '##ian', '.']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'repair', '##er', '.']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'conductor', '.', '[SEP]']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'plum', '##ber', '.']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'carpenter', '.', '[SEP]']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'security', 'system', 'install']
For TM: MASK at position 2: ['"', ')', '.', ',', "'"] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'mason', '.', '[SEP]']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', 'fire', '##fighter', '.']
For TM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', ')', '.', ',', "'"] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['"', '.', 'of', ')', ','] (top-5 predictions)
For TAM: MASK at position 7: ['"', '.', ')', 'of', 'the'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', ')', 'the', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', ')', '.', ',', "'"] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['"', '.', 'of', ')', ','] (top-5 predictions)
For TAM: MASK at position 7: ['"', '.', ')', 'of', 'the'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', ')', '.', ',', "'"] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['"', '.', 'of', ')', ','] (top-5 predictions)
For TAM: MASK at position 7: ['"', '.', ')', 'of', 'the'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['article', 'page', 'album', 'one', 'list'] (top-5 predictions)
For TAM: MASK at position 5: ['record', 'joke', 'documentary', 'classic', 'novel'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 0: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.0020979365799576044
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 0: -1.4405753961342251
Processing sentence 1
Input IDs for sentence 1: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 1: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.019471121951937675
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 1: 3.4823251642588833
Processing sentence 2
Input IDs for sentence 2: tensor([ 101, 2023,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 2: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2158
Target probability (p_T): 0.014118125662207603
Prior probability (p_prior): 0.0005701205227524042 for 2158
Prior probability (p_prior) for sie: 4.234115920098702e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.5987085311717237e-07 (target word token id=279)
Association score for sentence 2: 3.2093669768634663
Processing sentence 3
Input IDs for sentence 3: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 3: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.0178295336663723
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 3: 3.3942489989800184
Processing sentence 4
Input IDs for sentence 4: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 4: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.014143921434879303
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 4: 3.1626776731040773
Processing sentence 5
Input IDs for sentence 5: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 5: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.013067344203591347
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 5: 3.0835090309845863
Processing sentence 6
Input IDs for sentence 6: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 6: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.0032891242299228907
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 6: 1.7040140597141316
Processing sentence 7
Input IDs for sentence 7: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 7: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.021232638508081436
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 7: 3.5689322719352226
Processing sentence 8
Input IDs for sentence 8: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 8: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.01032220758497715
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 8: 2.847690372335114
Processing sentence 9
Input IDs for sentence 9: tensor([ 101, 2023,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 9: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2158
Target probability (p_T): 0.008270968683063984
Prior probability (p_prior): 0.0005984617164358497 for 2158
Prior probability (p_prior) for sie: 3.6992173590988386e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.3517187603138154e-07 (target word token id=279)
Association score for sentence 9: 2.6261443562332487
Processing sentence 10
Input IDs for sentence 10: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 10: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.007346664555370808
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 10: -0.1872832686068063
Processing sentence 11
Input IDs for sentence 11: tensor([ 101, 2023,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 11: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2158
Target probability (p_T): 0.01500809658318758
Prior probability (p_prior): 0.0005701205227524042 for 2158
Prior probability (p_prior) for sie: 4.234115920098702e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.5987085311717237e-07 (target word token id=279)
Association score for sentence 11: 3.270497324769376
Processing sentence 12
Input IDs for sentence 12: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 12: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.016494791954755783
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 12: 0.6215150151344175
Processing sentence 13
Input IDs for sentence 13: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 13: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.01015553530305624
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 13: 0.13648923005139862
Processing sentence 14
Input IDs for sentence 14: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 14: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.025087878108024597
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 14: 1.0408551087374687
Processing sentence 15
Input IDs for sentence 15: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 15: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.017351524904370308
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 15: 0.6721507164306076
Processing sentence 16
Input IDs for sentence 16: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 16: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.1318773776292801
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 16: 2.7003428566422447
Processing sentence 17
Input IDs for sentence 17: tensor([ 101, 2023,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 17: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2158
Target probability (p_T): 0.0008085028966888785
Prior probability (p_prior): 0.0005701205227524042 for 2158
Prior probability (p_prior) for sie: 4.234115920098702e-07 (target word token id=286)
Prior probability (p_prior) for er: 3.5987085311717237e-07 (target word token id=279)
Association score for sentence 17: 0.34933647990180033
Processing sentence 18
Input IDs for sentence 18: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 18: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.12818294763565063
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 18: 2.671928845035079
Processing sentence 19
Input IDs for sentence 19: tensor([ 101, 2023,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 19: ['[CLS]', 'this', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2158
Target probability (p_T): 0.02173568680882454
Prior probability (p_prior): 0.008859848603606224 for 2158
Prior probability (p_prior) for sie: 2.815730226757296e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.7798915880339337e-07 (target word token id=279)
Association score for sentence 19: 0.8974257863093895
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'tape', '##r', '.']
For TM: MASK at position 2: ['father', 'mother', 'dad', 'mom', 'life'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'steel', 'worker', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'mobile', 'equipment', 'mechanic']
For TM: MASK at position 2: ['"', ')', '.', ',', 'the'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'bus', 'mechanic', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'service', 'technician', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'heating', 'mechanic', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'electrical', 'install', '##er']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'operating', 'engineer', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'logging', 'worker', '.']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', 'floor', 'install', '##er']
For TM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 2: ['father', 'mother', 'dad', 'mom', 'life'] (top-5 predictions)
For TAM: MASK at position 5: ['vampire', 'mess', 'liar', 'killer', 'virgin'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['"', ')', '.', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['"', '.', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['"', '.', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 7: ['"', '.', ')', ',', 'the'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['.', '"', ')', ',', 'the'] (top-5 predictions)
For TAM: MASK at position 5: ['.', '"', 'the', ')', ','] (top-5 predictions)
For TAM: MASK at position 6: ['.', '"', ')', ',', 'of'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 101, 2026,  103, 2003, 1037,  103, 1012,  102])
Decoded tokens for sentence 0: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '.', '[SEP]']
Mask indices: [2 5]
Target word token ID: 2567
Target probability (p_T): 0.001056869514286518
Prior probability (p_prior): 0.02986057661473751 for 2567
Prior probability (p_prior) for sie: 2.3072820454217435e-07 (target word token id=286)
Prior probability (p_prior) for er: 2.050249463536602e-07 (target word token id=279)
Association score for sentence 0: -3.3412278523783683
Processing sentence 1
Input IDs for sentence 1: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 1: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.017048995941877365
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 1: 3.7063004079164767
Processing sentence 2
Input IDs for sentence 2: tensor([ 101, 2026,  103, 2003, 1037,  103,  103,  103])
Decoded tokens for sentence 2: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 5 6 7]
Target word token ID: 2567
Target probability (p_T): 0.01826561614871025
Prior probability (p_prior): 0.0006495381821878254 for 2567
Prior probability (p_prior) for sie: 4.98226938816515e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.4348098526825197e-07 (target word token id=279)
Association score for sentence 2: 3.336514051076131
Processing sentence 3
Input IDs for sentence 3: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 3: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.025847943499684334
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 3: 4.122440137612252
Processing sentence 4
Input IDs for sentence 4: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 4: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.01105598732829094
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 4: 3.273181215846423
Processing sentence 5
Input IDs for sentence 5: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 5: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.02042611874639988
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 5: 3.8870235075920396
Processing sentence 6
Input IDs for sentence 6: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 6: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.01846950314939022
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 6: 3.7863299883977475
Processing sentence 7
Input IDs for sentence 7: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 7: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.02216803841292858
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 7: 3.968860635329447
Processing sentence 8
Input IDs for sentence 8: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 8: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.008875093422830105
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 8: 3.0534579567825735
Processing sentence 9
Input IDs for sentence 9: tensor([ 101, 2026,  103, 2003, 1037,  103,  103, 1012])
Decoded tokens for sentence 9: ['[CLS]', 'my', '[MASK]', 'is', 'a', '[MASK]', '[MASK]', '.']
Mask indices: [2 5 6]
Target word token ID: 2567
Target probability (p_T): 0.018151892349123955
Prior probability (p_prior): 0.00041886395774781704 for 2567
Prior probability (p_prior) for sie: 4.6182029223018617e-07 (target word token id=286)
Prior probability (p_prior) for er: 4.1110067172667186e-07 (target word token id=279)
Association score for sentence 9: 3.7689839118961417
Batch 2: Associations calculated.
Evaluation completed.
Results saved to ../data/output_csv_files/english/results_padding_EN_sample.csv
