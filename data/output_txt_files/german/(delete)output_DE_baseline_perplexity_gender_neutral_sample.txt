-- Prepare evaluation data --
Loaded first 50 rows of the dataset:
    Unnamed: 0  ...              Profession_EN
0            0  ...                      taper
1            1  ...               steel worker
2            2  ...  mobile equipment mechanic
3            3  ...               bus mechanic
4            4  ...         service technician
5            5  ...           heating mechanic
6            6  ...       electrical installer
7            7  ...         operating engineer
8            8  ...             logging worker
9            9  ...            floor installer
10          10  ...                     roofer
11          11  ...    mining machine operator
12          12  ...                electrician
13          13  ...                   repairer
14          14  ...                  conductor
15          15  ...                    plumber
16          16  ...                  carpenter
17          17  ...  security system installer
18          18  ...                      mason
19          19  ...                firefighter
20          20  ...                      taper
21          21  ...               steel worker
22          22  ...  mobile equipment mechanic
23          23  ...               bus mechanic
24          24  ...         service technician
25          25  ...           heating mechanic
26          26  ...       electrical installer
27          27  ...         operating engineer
28          28  ...             logging worker
29          29  ...            floor installer
30          30  ...                     roofer
31          31  ...    mining machine operator
32          32  ...                electrician
33          33  ...                   repairer
34          34  ...                  conductor
35          35  ...                    plumber
36          36  ...                  carpenter
37          37  ...  security system installer
38          38  ...                      mason
39          39  ...                firefighter
40          40  ...                      taper
41          41  ...               steel worker
42          42  ...  mobile equipment mechanic
43          43  ...               bus mechanic
44          44  ...         service technician
45          45  ...           heating mechanic
46          46  ...       electrical installer
47          47  ...         operating engineer
48          48  ...             logging worker
49          49  ...            floor installer

[50 rows x 11 columns]
No GPU available, using the CPU instead.
-- Import BERT model --
loading german bert
Tokenizer: BertTokenizerFast(name_or_path='bert-base-german-dbmdz-cased', vocab_size=31102, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)
Model loaded: bert-base-german-dbmdz-cased
-- Calculate associations before fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0          [MASK] ist Trockenbaumontagekraft.
1               [MASK] ist Stahlarbeitskraft.
2     [MASK] ist Fachkraft für mobile Geräte.
3    [MASK] ist Mechanik Fachkraft für Busse.
4     [MASK] ist Kfz-Servicetechnikfachkraft.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 104, 215, 4471, 9180, 3854, 566, 103], [102, 104, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 104, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 104, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   104   215 11861  6717  1738   621  3854]
 [  102   104   215  4471  9180  3854   566   103]
 [  102   104   215  2394  3854   231 22615  7612]
 [  102   104   215 11016   269  2394  3854   231]
 [  102   104   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                            [MASK] ist [MASK].
1                            [MASK] ist [MASK].
2    [MASK] ist [MASK] [MASK] [MASK] [MASK]   .
3       [MASK] ist [MASK] [MASK] [MASK] [MASK].
4                            [MASK] ist [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 566, 103]]
Padded input IDs (first 5): [[102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 566 103   0   0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0          Er ist Trockenbaumontagekraft.
1               Er ist Stahlarbeitskraft.
2     Er ist Fachkraft für mobile Geräte.
3    Er ist Mechanik Fachkraft für Busse.
4     Er ist Kfz-Servicetechnikfachkraft.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 279, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 279, 215, 4471, 9180, 3854, 566, 103], [102, 279, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 279, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 279, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   279   215 11861  6717  1738   621  3854]
 [  102   279   215  4471  9180  3854   566   103]
 [  102   279   215  2394  3854   231 22615  7612]
 [  102   279   215 11016   269  2394  3854   231]
 [  102   279   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age', '##kraft']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile', 'Geräte']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft', 'für']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik', '##fach']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung', '##stechnik']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz', '-']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach', '##kraft']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz', '-']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der', 'Eisenbahn']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*', 'in']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft', 'für']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', 'Maurer', '*', 'in', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr', '.']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.00493394723162055
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 0: -2.153390357854629
Processing sentence 1
Input IDs for sentence 1: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.3719188868999481
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 1: 2.1691461034730657
Processing sentence 2
Input IDs for sentence 2: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.03518925607204437
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 2: 4.1485194886989545
Processing sentence 3
Input IDs for sentence 3: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.027334919199347496
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 3: 3.895943654313016
Processing sentence 4
Input IDs for sentence 4: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.055306609719991684
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 4: 0.2633627451028608
Processing sentence 5
Input IDs for sentence 5: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.09376762807369232
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 5: 5.578113779897467
Processing sentence 6
Input IDs for sentence 6: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.36511144042015076
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 6: 2.150672942145849
Processing sentence 7
Input IDs for sentence 7: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.27757689356803894
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 7: 1.8765683076852118
Processing sentence 8
Input IDs for sentence 8: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.05619538947939873
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 8: 4.616613393712863
Processing sentence 9
Input IDs for sentence 9: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.25630664825439453
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 9: 1.7968448913229422
Processing sentence 10
Input IDs for sentence 10: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.1297028511762619
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 10: 1.1157163928788136
Processing sentence 11
Input IDs for sentence 11: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.04569850489497185
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 11: 0.07253590066213876
Processing sentence 12
Input IDs for sentence 12: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.36511144042015076
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 12: 2.150672942145849
Processing sentence 13
Input IDs for sentence 13: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.042197246104478836
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 13: 4.779649060661696
Processing sentence 14
Input IDs for sentence 14: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.19288021326065063
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 14: 5.849848017448844
Processing sentence 15
Input IDs for sentence 15: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.044659294188022614
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 15: 0.04953276111089245
Processing sentence 16
Input IDs for sentence 16: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.03061605803668499
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 16: -0.328005037302605
Processing sentence 17
Input IDs for sentence 17: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.10954511165618896
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 17: 5.733630542948851
Processing sentence 18
Input IDs for sentence 18: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.11595921218395233
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 18: 1.003708829113292
Processing sentence 19
Input IDs for sentence 19: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.05882784351706505
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 19: 5.111909371963639
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Maurer', '*', 'in', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0005613241228275001
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 0: -1.1740323377692126
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.03120463714003563
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 1: 2.8439911535327655
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.004085221327841282
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 2: -0.21660771143791668
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0019613865297287703
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 3: -0.9503319826336415
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00037170766154304147
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 4: -1.586223146244904
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.006404647137969732
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 5: -1.1079174747031582
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.08908378332853317
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 6: 3.8930017560982835
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.01297338493168354
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 7: 1.9663243893820619
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.005246969871222973
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 8: 0.03366712368795263
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.016033202409744263
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 9: 2.178086166175778
Processing sentence 10
Input IDs for sentence 10: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 10: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.013190035708248615
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 10: 1.9828861174247612
Processing sentence 11
Input IDs for sentence 11: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 11: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0024192442651838064
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 11: 0.28687964776061814
Processing sentence 12
Input IDs for sentence 12: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 12: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.08908378332853317
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 12: 3.8930017560982835
Processing sentence 13
Input IDs for sentence 13: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 13: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0046766665764153
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 13: -1.4223557307343442
Processing sentence 14
Input IDs for sentence 14: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 14: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.007527501787990332
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 14: 0.39457959999723263
Processing sentence 15
Input IDs for sentence 15: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 15: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.015233241952955723
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 15: 2.1269044539801065
Processing sentence 16
Input IDs for sentence 16: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 16: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0016700844280421734
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 16: -0.0837013756574294
Processing sentence 17
Input IDs for sentence 17: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 17: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0003455917176324874
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 17: -4.02743852440556
Processing sentence 18
Input IDs for sentence 18: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 18: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.041479840874671936
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 18: 3.128631990637873
Processing sentence 19
Input IDs for sentence 19: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 19: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.007738414220511913
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 19: -0.9187445313425489
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Name', 'Leben', 'Körper', 'Vater', 'Mann'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Leben', 'Körper', 'Vater', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'eine', 'nur', 'ohne', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'mit', 'in', 'nur'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '##d', 'hoch', '"'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.02419525384902954
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 0: 0.33186182955823096
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.11867287009954453
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 1: 1.9220760545082702
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.1089879497885704
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 2: 4.982599675692551
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.07035420835018158
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 3: 4.544904953931824
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.09969065338373184
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 4: 1.7477772627392747
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2093,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 3726
Target probability (p_T): 0.10874830186367035
Prior probability (p_prior): 0.01264924369752407 for 3726
Prior probability (p_prior) for sie: 6.22024672338739e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.00041363900527358055 (target word token id=279)
Association score for sentence 5: 2.1514386281709283
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.16742539405822754
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 6: 2.266243181004587
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.07366335391998291
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 7: 1.445210779997433
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.09443186223506927
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 8: 4.8392408918529455
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.032362520694732666
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 9: 0.6227063221148327
Batch 2: Associations calculated.
Evaluation completed.
-- Import fine-tuning data --
Loaded first 50 rows of the finetuning dataset:
Loaded first 50 rows of the validation dataset:
['2024 auf Seite zwei auf den Punkt gebracht.', '»27.08.24« stand in der Nachricht in goldener Schrift auf schwarzem Grund – in der Typo des Oasis-Bandlogos.', 'A23 – Unterirdische Bohrungen zur Stromkabelverlegung im Bereich Anschlussstelle Halstenbek-Krupunder, 2.', 'Dezember bis voraussichtlich 23.', 'Dezember 2024: Um die Kapazität des Stromnetzes im Kreis Pinneberg zu erweitern, werden neue Stromkabel verlegt.', 'Aachen hat sehr gut gespielt, und vor allem Nicole van de Vosse hat uns einige Probleme bereitet“, sagte Trainer Alexander Waibl über die überragenden Holländerin (29 Punkte).', 'Ab 10:30 Uhr gibt es an den Sonntagen Frühschoppen und Familienprogramm, Abendveranstaltungen mit Schlagergrößen wie Mickie Krause, Lorenz Büffel, Almklausi und Schürze stehen täglich von 17 Uhr an auf dem Programm.', 'Ab 14 Uhr gibt es im Garten Kaffeetrinken.', 'Ab 15.', 'November umgibt sie der Wiener Christkindlmarkt.', 'Ab 17 Uhr begrüßt Moderator und Kommentator Markus Götz die Zuschauer gemeinsam mit Sky Experte Mladen Petric und Raphael Honigstein zur Partie zwischen Brighton & Hove Albion und Tottenham Hotspur.', 'Ab 180 Grad Celsius kann sich Fett selbst entzünden.', 'Ab 2014 bot sich bei ihr dann die Chance, hauptberuflich im politischen Umfeld zu arbeiten.', 'Ab 2025 wird der Post- und Paketversand teurer – aber nicht teuer genug.', 'Ab 2026 nimmt die Deutsche Bahn eine Generalsanierung von 4000 Streckenkilometern in Bayern vor.', 'Ab 2026 will die Formel 1 mit komplett klimaneutralen Sprit fahren.', 'Ab 2027 will der Bundesrat das Budget um 3,6 Milliarden Franken entlasten.', 'Ab 2035 dürfen keine neuen Verbrenner mehr in der EU zugelassen werden - bislang.', 'Ab 20 Uhr legt DJ Kribz auf, es wird Musik aus den 80ern, den 90ern und von heute gespielt.', 'Ab 29.', 'April bis Mitte September zwischen der Hardstrasse in Basel und Pratteln.', 'Ab 30 empfiehlt die Gynäkologin einen HPV-Test – wer mit den Humanen Papillomaviren infiziert ist, sollte regelmäßig zur Vorsorgeuntersuchung gehen.', 'Abarth: Der vollelektrische Abarth 600e startet im Jänner.', 'Abdelmadjid Tebboune bleibt wohl Präsident von Algerien.', 'Ab dem 22.', 'April ist der Oberhausener Stadtteil Styrum um eine Baustelle reicher.', 'Ab dem 26.', 'März kann man in Kutná Hora / Kuttenberg ihren Charme bewundern.', 'Ab dem 3.', 'Mai beginnt dann das klassische Volksfest mit großen -Acts, Bühnen und einem großen Markt.', 'Ab dem frühen Nachmittag bekommen Sie wieder Aufwind.', 'Ab dem heutigen Dienstag wird zusätzlich für mehr Lohn gestreikt.', 'Ab dem kommenden Schuljahr werden 376 Kinder unterrichtet, und rund 240 wollen die Betreuung wahrnehmen.', 'Ab dem kommenden Sommer wird er Björn Klos als Trainer des SV Merchweiler ablösen.', 'Ab der kommenden Woche wird saniert.', 'Ab diesem Zeitpunkt wird die etwa 20 Jahre alte Schieneninfrastruktur im Bereich Herdentor und Schüsselkorb erneuert.', 'Ab Donnerstag wird es dann grau und nass – und es kann wieder schneien.', 'Ab dort führt er in den Wald, und es ist gutes Schuhwerk erforderlich.', 'Ab einem Core i5-8400 oder Ryzen 5 1600 sowie einer GeForce GTX 1070 oder RX Vega 56 soll der Spaß losgehen.', 'Ab einer Beteiligung von 30 Prozent wäre Unicredit verpflichtet, den übrigen Aktionären der Commerzbank eine öffentliche Übernahmeofferte vorzulegen.', 'Ab Ende April schlüpfen die Larven des Eichenprozessionssspinners.', 'Abenteuer an der Börse: Wie hoch kann Nvidia noch steigen?', 'Aber, Alexis hat sich schon was überlegt, kommt schon mit passenden Kleidern um die Ecke.', 'Aber alleine der Versuch, das sowjetische Imperium in Osteuropa wieder zu errichten, könnte für viel Schaden sorgen.', 'Aber alles existiert in Kontinuität.', 'Aber als Erwachsener an eine fiktive Figur aus einem Märchenbuch zu glauben ist“normal“?', 'Aber: „Als ich nach Leipzig kam, gab es keine DDR mehr.“', 'Aber als sie sofortige Friedensverhandlungen mit Putin verlangt, wird sie von Strack-Zimmermann mit einem Drei-Wort-Satz in die Knie gezwungen.', 'Aber: «Als Staat, der keinem Bündnis angehört und eher kleine Stückzahlen beschafft, geniesst die Schweiz keine Priorität bei Lieferanten im Ausland.»', '"Aber als Vater habe ich grenzenlose Liebe zu meinem Sohn, Vertrauen in ihn und Respekt für seine Stärke."']
Max sentence length in training set: 64
Max sentence length in validation set: 32
Input sequence (first 5): ['Karl Telford – spielte Darren, den Freund von Estelle, der Polizist ist.', 'Er wurde in der letzten Folge der ersten Staffel von Estelle abserviert, nachdem sie mit Denver geschlafen hatte, und taucht seitdem nicht mehr auf.', 'Irwin Susan spielte Lawrence Odell, Jefferys Freund und ebenfalls ein Schüler der 11.', 'Klasse in Estelles Klasse.', 'Er verließ seine Freundin auf Estelles Rat hin, nachdem sie keinen Sex mit ihm haben wollte, merkte aber später, dass dies daran lag, dass sie sich bei seinem Freund Jeffery Filzläuse eingefangen hatte.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 3533, 2723, 6365, 809, 2985, 1472, 786, 818, 190, 2557, 195, 8006, 2033, 818, 125, 19377, 215, 566, 103], [102, 279, 325, 153, 125, 1761, 2900, 125, 940, 7482, 195, 8006, 2033, 28220, 2532, 426, 818, 3927, 307, 212, 2025, 289, 26774, 638, 818, 136, 21243, 9957, 255, 484, 216, 566, 103], [102, 3069, 5179, 28894, 2985, 28516, 12795, 630, 818, 14888, 105, 1296, 2557, 136, 1966, 143, 3248, 125, 1111, 566, 103], [102, 5263, 153, 8006, 12832, 5263, 566, 103], [102, 279, 7240, 697, 8645, 216, 8006, 12832, 1006, 631, 818, 3927, 307, 2450, 6913, 212, 1020, 450, 2164, 818, 23567, 30881, 494, 1243, 818, 377, 513, 3373, 3768, 818, 377, 307, 251, 282, 1182, 2557, 14888, 105, 30933, 3294, 12246, 17540, 1194, 7294, 638, 566, 103]]
Padded input IDs (first 5): [[  102  3533  2723  6365   809  2985  1472   786   818   190  2557   195
   8006  2033   818   125 19377   215   566   103     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102   279   325   153   125  1761  2900   125   940  7482   195  8006
   2033 28220  2532   426   818  3927   307   212  2025   289 26774   638
    818   136 21243  9957   255   484   216   566   103     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102  3069  5179 28894  2985 28516 12795   630   818 14888   105  1296
   2557   136  1966   143  3248   125  1111   566   103     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102  5263   153  8006 12832  5263   566   103     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102   279  7240   697  8645   216  8006 12832  1006   631   818  3927
    307  2450  6913   212  1020   450  2164   818 23567 30881   494  1243
    818   377   513  3373  3768   818   377   307   251   282  1182  2557
  14888   105 30933  3294 12246 17540  1194  7294   638   566   103     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
Input sequence (first 5): ['2024 auf Seite zwei auf den Punkt gebracht.', '»27.08.24« stand in der Nachricht in goldener Schrift auf schwarzem Grund – in der Typo des Oasis-Bandlogos.', 'A23 – Unterirdische Bohrungen zur Stromkabelverlegung im Bereich Anschlussstelle Halstenbek-Krupunder, 2.', 'Dezember bis voraussichtlich 23.', 'Dezember 2024: Um die Kapazität des Stromnetzes im Kreis Pinneberg zu erweitern, werden neue Stromkabel verlegt.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 27079, 30943, 216, 1640, 510, 216, 190, 4115, 3981, 566, 103], [102, 1665, 2533, 566, 3167, 566, 1955, 2529, 2680, 153, 125, 8132, 153, 23766, 30884, 4000, 216, 12890, 30895, 926, 809, 153, 125, 3763, 30892, 222, 257, 27106, 232, 2342, 14127, 317, 566, 103], [102, 131, 2485, 809, 633, 9238, 366, 15548, 271, 356, 4621, 26863, 3565, 314, 132, 223, 1373, 5450, 2138, 11001, 155, 3366, 232, 20611, 5361, 2411, 818, 197, 566, 103], [102, 1466, 378, 14994, 1910, 566, 103], [102, 1466, 27079, 30943, 853, 607, 128, 13826, 222, 4621, 26568, 223, 2235, 12347, 382, 851, 205, 16573, 818, 338, 1133, 4621, 26863, 10053, 566, 103]]
Padded input IDs (first 5): [[  102 27079 30943   216  1640   510   216   190  4115  3981   566   103
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  102  1665  2533   566  3167   566  1955  2529  2680   153   125  8132
    153 23766 30884  4000   216 12890 30895   926   809   153   125  3763
  30892   222   257 27106   232  2342 14127   317]
 [  102   131  2485   809   633  9238   366 15548   271   356  4621 26863
   3565   314   132   223  1373  5450  2138 11001   155  3366   232 20611
   5361  2411   818   197   566   103     0     0]
 [  102  1466   378 14994  1910   566   103     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  102  1466 27079 30943   853   607   128 13826   222  4621 26568   223
   2235 12347   382   851   205 16573   818   338  1133  4621 26863 10053
    566   103     0     0     0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0]])
val_tokens shape before masking: torch.Size([50, 32])
Total masked tokens per sequence: tensor([2, 3, 8, 2, 3, 3, 5, 1, 1, 1, 6, 2, 4, 2, 1, 1, 1, 2, 6, 1, 3, 3, 5, 1,
        3, 1, 1, 1, 1, 6, 2, 3, 2, 3, 5, 1, 1, 2, 3, 4, 3, 1, 1, 5, 1, 3, 1, 4,
        4, 3])
Total tokens in batch: 1600
Total masked tokens: 132
Percentage of masked tokens: 8.25%
Total special tokens in batch: 714
Total tokens: 1600, Non-special tokens: 886, Masked tokens: 132
Unique label values and counts: {-100: 1468, 0: 3, 110: 1, 125: 2, 128: 3, 136: 1, 153: 2, 180: 1, 190: 2, 195: 1, 197: 1, 199: 1, 205: 1, 208: 1, 231: 2, 232: 3, 249: 1, 251: 1, 255: 1, 260: 1, 272: 1, 276: 2, 286: 1, 307: 1, 310: 1, 314: 1, 348: 1, 371: 1, 382: 1, 394: 1, 425: 6, 429: 1, 484: 1, 530: 1, 560: 1, 566: 9, 573: 1, 669: 1, 731: 1, 818: 3, 853: 1, 877: 1, 899: 1, 994: 1, 1147: 1, 1373: 1, 1389: 1, 1396: 1, 1412: 1, 1466: 1, 1483: 1, 1620: 1, 1793: 1, 2252: 1, 2485: 1, 2498: 1, 2813: 1, 3496: 1, 3572: 1, 4076: 1, 4115: 1, 4156: 1, 4621: 1, 4883: 1, 5031: 1, 5096: 1, 5324: 1, 5361: 1, 5450: 1, 5756: 1, 5848: 1, 5907: 1, 6034: 1, 6052: 1, 6275: 1, 6325: 1, 6335: 1, 6811: 1, 7398: 1, 8901: 1, 10078: 1, 10376: 1, 10840: 1, 11001: 1, 11710: 1, 11995: 1, 12460: 1, 13221: 1, 13381: 1, 13410: 1, 13556: 1, 14309: 1, 16313: 1, 17030: 2, 17737: 1, 18006: 1, 19055: 1, 20367: 1, 21402: 1, 27055: 1, 27079: 2, 28235: 1, 30882: 1, 30886: 1, 30892: 1}
Dataset size: 50
Sample 0 labels shape: torch.Size([32])
Sample 1 labels shape: torch.Size([32])
Sample 0 labels shape: torch.Size([32])
Sample 1 labels shape: torch.Size([32])
Sample 2 labels shape: torch.Size([32])
Sample 3 labels shape: torch.Size([32])
Sample 4 labels shape: torch.Size([32])
🔎 Checking dataset sample shapes before batching...
Sample 0 - input_ids shape: torch.Size([32])
Sample 0 - labels shape: torch.Size([32])
Sample 1 - input_ids shape: torch.Size([32])
Sample 1 - labels shape: torch.Size([32])
Sample 2 - input_ids shape: torch.Size([32])
Sample 2 - labels shape: torch.Size([32])
Sample 3 - input_ids shape: torch.Size([32])
Sample 3 - labels shape: torch.Size([32])
Sample 4 - input_ids shape: torch.Size([32])
Sample 4 - labels shape: torch.Size([32])
-- Set up model fine-tuning --

Calculating baseline perplexity before fine-tuning...
Baseline Loss: 2.71, Perplexity: 14.97

======== Epoch 1 / 3 ========
Training...
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 6
Unique label values and counts: {-100: 58, 261: 1, 693: 1, 2087: 1, 6461: 1, 14489: 1, 30894: 1}
Train Loss: 1.4569567441940308
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 61, 10705: 1, 12716: 1, 13336: 1}
Train Loss: 2.3608598709106445
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 0.08174920827150345
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 2004: 1, 3749: 1, 7057: 1, 19954: 1}
Train Loss: 1.4726555347442627
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 5
Unique label values and counts: {-100: 59, 255: 1, 282: 1, 2736: 1, 6734: 1, 25855: 1}
Train Loss: 2.8396503925323486
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 13
Unique label values and counts: {-100: 51, 818: 2, 1256: 1, 2805: 1, 2900: 1, 5041: 1, 5988: 1, 6240: 1, 8588: 1, 10012: 1, 17131: 1, 22494: 1, 30881: 1}
Train Loss: 3.6817049980163574
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 2
Unique label values and counts: {-100: 62, 205: 1, 14585: 1}
Train Loss: 2.5422677993774414
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 630: 1}
Train Loss: 1.7488815784454346
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 180: 1, 818: 1, 4186: 1, 18286: 1}
Train Loss: 1.6503499746322632
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 17.688217163085938
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 15.306985855102539
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 6
Unique label values and counts: {-100: 58, 282: 1, 450: 1, 818: 1, 3927: 1, 12246: 1, 23567: 1}
Train Loss: 1.3155109882354736
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 17.051040649414062
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 62, 195: 1, 2985: 1}
Train Loss: 3.9797940254211426
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 5
Unique label values and counts: {-100: 59, 167: 1, 818: 1, 850: 1, 6473: 1, 6517: 1}
Train Loss: 0.7772972583770752
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 4
Unique label values and counts: {-100: 60, 232: 1, 333: 1, 369: 1, 19448: 1}
Train Loss: 0.7460715770721436
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 279: 1, 4034: 1, 5957: 1}
Train Loss: 1.308510661125183
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 462: 1, 8131: 1}
Train Loss: 1.3130685091018677
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 9
Unique label values and counts: {-100: 55, 201: 1, 408: 1, 702: 1, 818: 1, 1802: 1, 2505: 1, 4268: 1, 6715: 1, 27535: 1}
Train Loss: 2.008770704269409
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 123: 1, 386: 1, 14616: 1, 30853: 1}
Train Loss: 2.229302167892456
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 9
Unique label values and counts: {-100: 55, 180: 1, 916: 1, 1166: 1, 10251: 1, 11443: 1, 14480: 1, 17265: 1, 30881: 1, 31018: 1}
Train Loss: 3.4241554737091064
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 8
Unique label values and counts: {-100: 56, 208: 1, 818: 1, 1384: 1, 3202: 1, 4920: 1, 8227: 2, 14710: 1}
Train Loss: 1.6833221912384033
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 9
Unique label values and counts: {-100: 55, 285: 1, 343: 1, 566: 2, 818: 1, 2162: 1, 2234: 1, 3616: 1, 12676: 1}
Train Loss: 2.3714911937713623
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 3
Unique label values and counts: {-100: 61, 251: 1, 282: 1, 18120: 1}
Train Loss: 0.2648140788078308
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 4
Unique label values and counts: {-100: 60, 242: 1, 818: 1, 4268: 1, 26344: 1}
Train Loss: 2.6623687744140625
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 3
Unique label values and counts: {-100: 61, 424: 1, 5787: 1, 30933: 1}
Train Loss: 4.000293254852295
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 195: 1, 5774: 1}
Train Loss: 0.0025910332333296537
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 215: 1, 760: 1, 818: 1, 853: 1, 1761: 1, 5718: 1, 11336: 1}
Train Loss: 2.0070507526397705
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 462: 1}
Train Loss: 0.048906370997428894
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 106: 1, 434: 1, 472: 1, 1230: 1, 2572: 1, 3432: 1, 4823: 1, 22075: 1, 22958: 1, 30882: 1, 30887: 1}
Train Loss: 3.814145803451538
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 3
Unique label values and counts: {-100: 61, 2672: 1, 14491: 1, 21303: 1}
Train Loss: 7.624243259429932
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 628: 1}
Train Loss: 0.19865719974040985
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 6
Unique label values and counts: {-100: 58, 153: 1, 763: 1, 8936: 1, 13072: 1, 19798: 1, 30881: 1}
Train Loss: 0.697439432144165
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 6
Unique label values and counts: {-100: 58, 195: 1, 328: 1, 351: 1, 818: 1, 5871: 1, 30882: 1}
Train Loss: 0.607113778591156
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 10
Unique label values and counts: {-100: 54, 195: 2, 215: 1, 8760: 1, 8861: 1, 9176: 1, 9566: 1, 14037: 1, 16376: 1, 17190: 1}
Train Loss: 2.1862504482269287
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 23234: 1}
Train Loss: 1.8664703369140625
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 7556: 1}
Train Loss: 0.00016473367577418685
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 4
Unique label values and counts: {-100: 60, 143: 1, 279: 1, 818: 1, 4717: 1}
Train Loss: 0.5228610038757324
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 1178: 1, 4643: 1}
Train Loss: 8.232213973999023
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 8
Unique label values and counts: {-100: 56, 566: 1, 818: 1, 996: 1, 1257: 1, 1860: 1, 3937: 1, 6485: 1, 21362: 1}
Train Loss: 2.1288681030273438
  Batch    40  of     50.    Elapsed: 0:00:17.
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 2964: 1, 6681: 1}
Train Loss: 0.2524294853210449
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 7
Unique label values and counts: {-100: 57, 136: 1, 279: 1, 484: 1, 818: 1, 1761: 1, 2900: 1, 26774: 1}
Train Loss: 1.718865156173706
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 566: 1, 3978: 1}
Train Loss: 2.3716979026794434
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 9
Unique label values and counts: {-100: 55, 113: 1, 183: 1, 251: 1, 481: 1, 731: 1, 2008: 1, 3862: 1, 3953: 1, 9176: 1}
Train Loss: 0.9420356750488281
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 8
Unique label values and counts: {-100: 56, 136: 1, 199: 1, 290: 1, 484: 1, 4867: 1, 18301: 1, 24641: 1, 27917: 1}
Train Loss: 2.908318042755127
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 16438: 1, 26674: 1}
Train Loss: 6.282038688659668
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 195: 1, 358: 1, 14080: 1}
Train Loss: 0.945784866809845
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 8006: 1}
Train Loss: 3.7626328468322754
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 6
Unique label values and counts: {-100: 58, 307: 1, 717: 1, 1078: 1, 5995: 1, 10345: 1, 17554: 1}
Train Loss: 2.296224355697632
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 2
Unique label values and counts: {-100: 62, 333: 1, 7194: 1}
Train Loss: 0.08784785121679306


[Epoch 1] Average training loss: 2.95

[Epoch 1] Training Perplexity: 19.09
[Epoch 1] Training epoch took: 0:00:21

Running Validation...
Batch input_ids shape: torch.Size([50, 32])
Batch labels shape: torch.Size([50, 32])
b_input_ids shape: torch.Size([50, 32])
b_labels shape: torch.Size([50, 32])

🔍 Total valid labels (non -100) in batch: 131
Logits shape: torch.Size([50, 32, 31102])
Before reshaping:
  Logits shape: torch.Size([50, 32, 31102])
  b_labels shape: torch.Size([50, 32])
After reshaping:
  Logits shape: torch.Size([1600, 31102])
  Labels shape: torch.Size([1600])
Valid labels used for loss computation: 131
Loss per valid token: 0.020642788355587093
Computed Eval Loss: 2.704205274581909
  Eval Loss: 2.70, Perplexity: 14.94
[Epoch 1] Validation took: 0:00:01

======== Epoch 2 / 3 ========
Training...
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 3
Unique label values and counts: {-100: 61, 128: 1, 2524: 1, 3749: 1}
Train Loss: 1.0755325555801392
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 7
Unique label values and counts: {-100: 57, 510: 1, 586: 1, 614: 1, 3076: 1, 5559: 1, 9069: 1, 26674: 1}
Train Loss: 5.169260501861572
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 1310: 1}
Train Loss: 0.089939184486866
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 1
Unique label values and counts: {-100: 63, 818: 1}
Train Loss: 0.019772805273532867
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 3
Unique label values and counts: {-100: 61, 316: 1, 371: 1, 9176: 1}
Train Loss: 3.0004303455352783
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 4
Unique label values and counts: {-100: 60, 195: 1, 3451: 1, 7629: 1, 27770: 1}
Train Loss: 1.1099668741226196
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 3
Unique label values and counts: {-100: 61, 205: 1, 397: 1, 10626: 1}
Train Loss: 0.9184388518333435
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 3
Unique label values and counts: {-100: 61, 818: 1, 1231: 1, 4717: 1}
Train Loss: 5.213161945343018
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 6
Unique label values and counts: {-100: 58, 285: 1, 566: 1, 2087: 1, 2301: 1, 14489: 1, 16839: 1}
Train Loss: 0.6297244429588318
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 5
Unique label values and counts: {-100: 59, 190: 1, 279: 1, 5957: 1, 8583: 1, 8862: 1}
Train Loss: 1.452918291091919
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 199: 1, 215: 1, 566: 1, 760: 1, 10395: 1, 30881: 1, 30894: 1}
Train Loss: 1.94719398021698
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 2
Unique label values and counts: {-100: 62, 1182: 1, 28799: 1}
Train Loss: 4.053365707397461
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 13
Unique label values and counts: {-100: 51, 125: 1, 136: 1, 143: 1, 195: 1, 369: 1, 498: 1, 1137: 1, 1179: 1, 2122: 1, 4920: 1, 7740: 1, 11079: 1, 27394: 1}
Train Loss: 1.9983094930648804
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 3
Unique label values and counts: {-100: 61, 136: 1, 4508: 1, 5787: 1}
Train Loss: 0.8147162795066833
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 341: 1, 498: 1, 2342: 1, 14797: 1}
Train Loss: 1.9179855585098267
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 10
Unique label values and counts: {-100: 54, 153: 1, 251: 1, 566: 1, 730: 1, 818: 1, 2215: 1, 2554: 1, 4186: 1, 24372: 1, 24487: 1}
Train Loss: 1.5673844814300537
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 3
Unique label values and counts: {-100: 61, 153: 1, 232: 1, 577: 1}
Train Loss: 0.14703869819641113
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 6
Unique label values and counts: {-100: 58, 758: 1, 818: 1, 876: 1, 3013: 1, 4268: 1, 15147: 1}
Train Loss: 3.0065248012542725
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 5
Unique label values and counts: {-100: 59, 136: 1, 153: 1, 5744: 1, 14193: 1, 30881: 1}
Train Loss: 0.2829667329788208
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 3
Unique label values and counts: {-100: 61, 425: 1, 1078: 1, 4597: 1}
Train Loss: 0.3318043649196625
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 1
Unique label values and counts: {-100: 63, 1876: 1}
Train Loss: 6.28574275970459
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 255: 1, 2736: 1, 30892: 1}
Train Loss: 0.1430342048406601
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 8
Unique label values and counts: {-100: 56, 153: 1, 436: 1, 809: 1, 818: 1, 9577: 1, 11842: 1, 22327: 1, 30882: 1}
Train Loss: 1.6045482158660889
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 5
Unique label values and counts: {-100: 59, 336: 1, 628: 1, 818: 1, 1295: 1, 3588: 1}
Train Loss: 0.15917107462882996
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 3
Unique label values and counts: {-100: 61, 221: 1, 8954: 1, 12145: 1}
Train Loss: 1.6468263864517212
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 2612: 1, 20818: 1}
Train Loss: 4.081389904022217
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 63, 3978: 1}
Train Loss: 0.07684352248907089
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 4
Unique label values and counts: {-100: 60, 195: 1, 566: 1, 1472: 1, 3533: 1}
Train Loss: 0.4422124922275543
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 63, 343: 1}
Train Loss: 0.01658228412270546
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 125: 1, 195: 1, 432: 1, 818: 1, 1876: 1, 2805: 1, 4951: 1, 5988: 1, 6240: 1, 14710: 1, 28713: 1}
Train Loss: 3.8458411693573
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 4
Unique label values and counts: {-100: 60, 113: 1, 199: 1, 4268: 1, 15147: 1}
Train Loss: 2.0808520317077637
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 153: 1, 3476: 1, 3616: 1, 11350: 1, 18702: 1, 19973: 1}
Train Loss: 3.551595687866211
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 1
Unique label values and counts: {-100: 63, 15883: 1}
Train Loss: 0.4841013252735138
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 143: 1}
Train Loss: 0.0001311216183239594
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 6
Unique label values and counts: {-100: 58, 1060: 1, 3104: 1, 6863: 1, 10345: 1, 13554: 1, 30881: 1}
Train Loss: 4.7252984046936035
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 4
Unique label values and counts: {-100: 60, 136: 1, 143: 1, 566: 1, 7546: 1}
Train Loss: 0.2512144446372986
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 115: 1, 125: 1, 136: 1, 199: 1, 205: 1, 231: 1, 403: 1, 507: 1, 2087: 1, 11502: 1, 12836: 1}
Train Loss: 1.3926782608032227
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 4
Unique label values and counts: {-100: 60, 208: 1, 282: 1, 8528: 1, 22597: 1}
Train Loss: 3.1644515991210938
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 3043: 1, 3351: 1}
Train Loss: 1.7709507942199707
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 6
Unique label values and counts: {-100: 58, 450: 1, 631: 1, 1194: 1, 1243: 1, 8006: 1, 23567: 1}
Train Loss: 0.3936046361923218
  Batch    40  of     50.    Elapsed: 0:00:16.
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 3
Unique label values and counts: {-100: 61, 136: 1, 199: 1, 2964: 1}
Train Loss: 0.38265395164489746
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 5
Unique label values and counts: {-100: 59, 397: 1, 4643: 1, 6715: 1, 10660: 1, 25397: 1}
Train Loss: 2.513798475265503
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 128: 1, 8017: 1, 12345: 1, 17066: 1, 22222: 1, 24767: 1}
Train Loss: 2.1476993560791016
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 11.415641784667969
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 10
Unique label values and counts: {-100: 54, 136: 1, 153: 1, 282: 1, 3978: 1, 5504: 1, 5995: 1, 10345: 1, 15350: 2, 28755: 1}
Train Loss: 1.6380265951156616
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 2, 9176: 1, 18609: 1, 30883: 1}
Train Loss: 2.1871585845947266
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 2.612851619720459
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 5
Unique label values and counts: {-100: 59, 616: 1, 2863: 1, 5774: 1, 15194: 1, 24625: 1}
Train Loss: 7.34533166885376
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 3
Unique label values and counts: {-100: 61, 216: 1, 232: 1, 4442: 1}
Train Loss: 0.30907219648361206
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 6
Unique label values and counts: {-100: 58, 2448: 1, 3205: 1, 4586: 1, 11443: 1, 14480: 1, 22958: 1}
Train Loss: 2.812861204147339


[Epoch 2] Average training loss: 2.08

[Epoch 2] Training Perplexity: 8.04
[Epoch 2] Training epoch took: 0:00:20

Running Validation...
Batch input_ids shape: torch.Size([50, 32])
Batch labels shape: torch.Size([50, 32])
b_input_ids shape: torch.Size([50, 32])
b_labels shape: torch.Size([50, 32])

🔍 Total valid labels (non -100) in batch: 131
Logits shape: torch.Size([50, 32, 31102])
Before reshaping:
  Logits shape: torch.Size([50, 32, 31102])
  b_labels shape: torch.Size([50, 32])
After reshaping:
  Logits shape: torch.Size([1600, 31102])
  Labels shape: torch.Size([1600])
Valid labels used for loss computation: 131
Loss per valid token: 0.02087359574004894
Computed Eval Loss: 2.734441041946411
  Eval Loss: 2.73, Perplexity: 15.40
[Epoch 2] Validation took: 0:00:01

======== Epoch 3 / 3 ========
Training...
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 5
Unique label values and counts: {-100: 59, 223: 1, 818: 1, 1334: 1, 11440: 1, 21751: 1}
Train Loss: 1.4358055591583252
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 1
Unique label values and counts: {-100: 63, 424: 1}
Train Loss: 0.12225138396024704
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 1, 195: 1, 484: 1, 818: 1, 827: 1, 1643: 1, 5637: 1, 12836: 1, 19486: 1, 30881: 1}
Train Loss: 2.5053253173828125
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 818: 1}
Train Loss: 0.0005402297829277813
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 5
Unique label values and counts: {-100: 59, 231: 1, 2098: 1, 4734: 1, 14616: 1, 21980: 1}
Train Loss: 1.6271030902862549
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 3
Unique label values and counts: {-100: 61, 842: 1, 899: 1, 27433: 1}
Train Loss: 0.17979051172733307
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 4
Unique label values and counts: {-100: 60, 2569: 1, 3978: 1, 6433: 1, 18711: 1}
Train Loss: 3.4346442222595215
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 4
Unique label values and counts: {-100: 60, 282: 1, 818: 1, 5334: 1, 7740: 1}
Train Loss: 0.09324270486831665
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 10345: 1, 12069: 1, 13152: 1, 13554: 1}
Train Loss: 2.216477155685425
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 11
Unique label values and counts: {-100: 53, 195: 1, 215: 1, 805: 1, 2222: 1, 2957: 1, 8760: 1, 9176: 1, 14037: 1, 16376: 1, 30881: 1, 30883: 1}
Train Loss: 4.641637802124023
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 2
Unique label values and counts: {-100: 62, 188: 1, 3978: 1}
Train Loss: 0.3570360541343689
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 9.072793006896973
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 6
Unique label values and counts: {-100: 58, 152: 1, 1507: 1, 6111: 1, 9577: 1, 19048: 1, 28644: 1}
Train Loss: 1.1567423343658447
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 4
Unique label values and counts: {-100: 60, 232: 1, 5503: 1, 6838: 1, 12631: 1}
Train Loss: 0.4518266022205353
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 113: 1, 242: 1}
Train Loss: 1.6076122522354126
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 4
Unique label values and counts: {-100: 60, 285: 1, 1701: 1, 9536: 1, 30888: 1}
Train Loss: 1.5280189514160156
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 5
Unique label values and counts: {-100: 59, 566: 1, 818: 1, 1472: 1, 6365: 1, 8006: 1}
Train Loss: 0.07582221925258636
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 5
Unique label values and counts: {-100: 59, 221: 1, 389: 1, 1257: 1, 3043: 1, 12145: 1}
Train Loss: 0.7247346639633179
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 1966: 1}
Train Loss: 2.799576759338379
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 3
Unique label values and counts: {-100: 61, 566: 1, 5423: 1, 14491: 1}
Train Loss: 0.01953132636845112
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 2087: 1, 24523: 1}
Train Loss: 1.5391631126403809
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 2
Unique label values and counts: {-100: 62, 276: 1, 818: 1}
Train Loss: 0.0015203120419755578
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 180: 1, 1782: 1}
Train Loss: 0.39997389912605286
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 5263: 1}
Train Loss: 5.356334686279297
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 333: 1, 1812: 1, 2878: 1}
Train Loss: 0.7019852995872498
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 28171: 1}
Train Loss: 12.080215454101562
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 158: 1, 279: 1, 371: 1}
Train Loss: 1.741431713104248
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 7
Unique label values and counts: {-100: 57, 129: 1, 276: 1, 540: 1, 1422: 1, 2702: 1, 15194: 1, 26653: 1}
Train Loss: 2.9437572956085205
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 6
Unique label values and counts: {-100: 58, 180: 1, 566: 1, 730: 1, 2784: 1, 2913: 1, 24372: 1}
Train Loss: 0.935788631439209
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 1
Unique label values and counts: {-100: 63, 1096: 1}
Train Loss: 5.481118679046631
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 7
Unique label values and counts: {-100: 57, 113: 1, 205: 1, 336: 1, 731: 1, 1244: 1, 3241: 1, 30933: 1}
Train Loss: 0.8493490219116211
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 0.0035400837659835815
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 195: 1, 343: 1, 6485: 1}
Train Loss: 0.0009963911725208163
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 1, 195: 1, 232: 1, 285: 2, 940: 1, 2234: 1, 2564: 1, 3243: 1, 5276: 1}
Train Loss: 1.2441486120224
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 8
Unique label values and counts: {-100: 56, 136: 1, 195: 1, 2448: 1, 3131: 1, 4586: 1, 7696: 1, 30888: 1, 30892: 1}
Train Loss: 1.9127392768859863
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 3
Unique label values and counts: {-100: 61, 1535: 1, 8561: 1, 30933: 1}
Train Loss: 0.8820527195930481
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 4
Unique label values and counts: {-100: 60, 1742: 1, 9667: 1, 14491: 1, 28799: 1}
Train Loss: 4.591238021850586
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 4
Unique label values and counts: {-100: 60, 276: 1, 1256: 1, 9034: 1, 14710: 1}
Train Loss: 2.359488010406494
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 5
Unique label values and counts: {-100: 59, 326: 1, 387: 1, 1761: 1, 8639: 1, 14478: 1}
Train Loss: 5.13577127456665
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 1, 249: 1, 282: 1, 574: 1, 698: 1, 818: 1, 1459: 1, 2530: 1, 10028: 1, 10660: 1}
Train Loss: 1.0636554956436157
  Batch    40  of     50.    Elapsed: 0:00:17.
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 5
Unique label values and counts: {-100: 59, 484: 1, 566: 1, 638: 1, 940: 1, 28220: 1}
Train Loss: 0.3808630108833313
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 4
Unique label values and counts: {-100: 60, 10345: 1, 18432: 1, 19954: 1, 20985: 1}
Train Loss: 2.451631784439087
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 3
Unique label values and counts: {-100: 61, 513: 1, 818: 1, 30881: 1}
Train Loss: 1.0665327310562134
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 2
Unique label values and counts: {-100: 62, 427: 1, 14616: 1}
Train Loss: 0.39830249547958374
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 2
Unique label values and counts: {-100: 62, 195: 1, 566: 1}
Train Loss: 0.24044719338417053
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 2
Unique label values and counts: {-100: 62, 125: 1, 282: 1}
Train Loss: 0.12031621485948563
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 62, 30882: 1, 30887: 1}
Train Loss: 3.328428030014038
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 153: 2, 917: 1, 1257: 1, 6114: 1, 28948: 1, 30881: 1}
Train Loss: 0.07824301719665527
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 6
Unique label values and counts: {-100: 58, 136: 1, 208: 1, 2431: 1, 5543: 1, 13623: 1, 30929: 1}
Train Loss: 1.4755548238754272
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 7
Unique label values and counts: {-100: 57, 106: 1, 231: 1, 7044: 1, 9110: 1, 22075: 1, 24446: 1, 30882: 1}
Train Loss: 1.0709418058395386


[Epoch 3] Average training loss: 1.88

[Epoch 3] Training Perplexity: 6.54
[Epoch 3] Training epoch took: 0:00:21

Running Validation...
Batch input_ids shape: torch.Size([50, 32])
Batch labels shape: torch.Size([50, 32])
b_input_ids shape: torch.Size([50, 32])
b_labels shape: torch.Size([50, 32])

🔍 Total valid labels (non -100) in batch: 131
Logits shape: torch.Size([50, 32, 31102])
Before reshaping:
  Logits shape: torch.Size([50, 32, 31102])
  b_labels shape: torch.Size([50, 32])
After reshaping:
  Logits shape: torch.Size([1600, 31102])
  Labels shape: torch.Size([1600])
Valid labels used for loss computation: 131
Loss per valid token: 0.02066066974901971
Computed Eval Loss: 2.706547737121582
  Eval Loss: 2.71, Perplexity: 14.98
[Epoch 3] Validation took: 0:00:01

Fine-tuning complete!
Epoch 1: Train Loss = 2.95, Eval Loss = 2.70, Perplexity = 14.94
Epoch 2: Train Loss = 2.08, Eval Loss = 2.73, Perplexity = 15.40
Epoch 3: Train Loss = 1.88, Eval Loss = 2.71, Perplexity = 14.98
-- Calculate associations after fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0          [MASK] ist Trockenbaumontagekraft.
1               [MASK] ist Stahlarbeitskraft.
2     [MASK] ist Fachkraft für mobile Geräte.
3    [MASK] ist Mechanik Fachkraft für Busse.
4     [MASK] ist Kfz-Servicetechnikfachkraft.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 104, 215, 4471, 9180, 3854, 566, 103], [102, 104, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 104, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 104, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   104   215 11861  6717  1738   621  3854]
 [  102   104   215  4471  9180  3854   566   103]
 [  102   104   215  2394  3854   231 22615  7612]
 [  102   104   215 11016   269  2394  3854   231]
 [  102   104   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                            [MASK] ist [MASK].
1                            [MASK] ist [MASK].
2    [MASK] ist [MASK] [MASK] [MASK] [MASK]   .
3       [MASK] ist [MASK] [MASK] [MASK] [MASK].
4                            [MASK] ist [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 566, 103]]
Padded input IDs (first 5): [[102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 566 103   0   0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0          Er ist Trockenbaumontagekraft.
1               Er ist Stahlarbeitskraft.
2     Er ist Fachkraft für mobile Geräte.
3    Er ist Mechanik Fachkraft für Busse.
4     Er ist Kfz-Servicetechnikfachkraft.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 279, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 279, 215, 4471, 9180, 3854, 566, 103], [102, 279, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 279, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 279, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   279   215 11861  6717  1738   621  3854]
 [  102   279   215  4471  9180  3854   566   103]
 [  102   279   215  2394  3854   231 22615  7612]
 [  102   279   215 11016   269  2394  3854   231]
 [  102   279   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age', '##kraft']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile', 'Geräte']
For TM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft', 'für']
For TM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik', '##fach']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung', '##stechnik']
For TM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz', '-']
For TM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach', '##kraft']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz', '-']
For TM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der', 'Eisenbahn']
For TM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*', 'in']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft', 'für']
For TM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', 'Maurer', '*', 'in', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr', '.']
For TM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'hoch', 'de', 'über', '.'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', 'hoch', '##r', '.'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'hoch', 'de', 'über', '.'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', 'hoch', '##r', '.'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)
For TAM: MASK at position 3: ['.', 'eine', 'für', 'die', 'bei'] (top-5 predictions)
For TAM: MASK at position 4: ['.', 'und', ',', 'für', 'frei'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', '.', '[UNK]', 'ab', 'hoch'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'hoch', 'de', 'über', '.'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', 'hoch', '##r', '.'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)
For TAM: MASK at position 3: ['.', 'eine', 'für', 'die', 'bei'] (top-5 predictions)
For TAM: MASK at position 4: ['.', 'und', ',', 'für', 'frei'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', '.', '[UNK]', 'ab', 'hoch'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'hoch', 'de', 'über', '.'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', 'hoch', '##r', '.'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)
For TAM: MASK at position 3: ['.', 'eine', 'für', 'die', 'bei'] (top-5 predictions)
For TAM: MASK at position 4: ['.', 'und', ',', 'für', 'frei'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', '.', '[UNK]', 'ab', 'hoch'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)
For TAM: MASK at position 3: ['.', 'eine', 'für', 'die', 'bei'] (top-5 predictions)
For TAM: MASK at position 4: ['.', 'und', ',', 'für', 'frei'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', '.', '[UNK]', 'ab', 'hoch'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.00690090237185359
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 0: -2.4113435031214405
Processing sentence 1
Input IDs for sentence 1: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.65208500623703
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 1: 2.137179246388987
Processing sentence 2
Input IDs for sentence 2: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.14198037981987
Prior probability (p_prior): 0.0013174251653254032 for 279
Prior probability (p_prior) for sie: 2.2538502889801748e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0013174251653254032 (target word token id=279)
Association score for sentence 2: 4.68000967846412
Processing sentence 3
Input IDs for sentence 3: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.09145712107419968
Prior probability (p_prior): 0.0013174251653254032 for 279
Prior probability (p_prior) for sie: 2.2538502889801748e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0013174251653254032 (target word token id=279)
Association score for sentence 3: 4.240191040995768
Processing sentence 4
Input IDs for sentence 4: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.44001758098602295
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 4: 1.743818998197078
Processing sentence 5
Input IDs for sentence 5: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.5746150612831116
Prior probability (p_prior): 0.0019452712731435895 for 279
Prior probability (p_prior) for sie: 9.141589544015005e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0019452712731435895 (target word token id=279)
Association score for sentence 5: 5.688298918573441
Processing sentence 6
Input IDs for sentence 6: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5824078917503357
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 6: 2.024175362482771
Processing sentence 7
Input IDs for sentence 7: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5988465547561646
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 7: 2.0520097115737395
Processing sentence 8
Input IDs for sentence 8: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.26210638880729675
Prior probability (p_prior): 0.0013174251653254032 for 279
Prior probability (p_prior) for sie: 2.2538502889801748e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0013174251653254032 (target word token id=279)
Association score for sentence 8: 5.293071286181207
Processing sentence 9
Input IDs for sentence 9: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.678091824054718
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 9: 2.176287027775709
Processing sentence 10
Input IDs for sentence 10: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5466684103012085
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 10: 1.9608467370170357
Processing sentence 11
Input IDs for sentence 11: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.29921242594718933
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 11: 1.3581580911067153
Processing sentence 12
Input IDs for sentence 12: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5824078917503357
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 12: 2.024175362482771
Processing sentence 13
Input IDs for sentence 13: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.1020294651389122
Prior probability (p_prior): 0.0019452712731435895 for 279
Prior probability (p_prior) for sie: 9.141589544015005e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0019452712731435895 (target word token id=279)
Association score for sentence 13: 3.9598602060807937
Processing sentence 14
Input IDs for sentence 14: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.4417901039123535
Prior probability (p_prior): 0.0013174251653254032 for 279
Prior probability (p_prior) for sie: 2.2538502889801748e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0013174251653254032 (target word token id=279)
Association score for sentence 14: 5.815155691952928
Processing sentence 15
Input IDs for sentence 15: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.1462242603302002
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 15: 0.6421457881769315
Processing sentence 16
Input IDs for sentence 16: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.021203411743044853
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 16: -1.2888335846977748
Processing sentence 17
Input IDs for sentence 17: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.2915323078632355
Prior probability (p_prior): 0.0019452712731435895 for 279
Prior probability (p_prior) for sie: 9.141589544015005e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0019452712731435895 (target word token id=279)
Association score for sentence 17: 5.009749393400697
Processing sentence 18
Input IDs for sentence 18: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5665293335914612
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 18: 1.9965331748150381
Processing sentence 19
Input IDs for sentence 19: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.27511200308799744
Prior probability (p_prior): 0.0019452712731435895 for 279
Prior probability (p_prior) for sie: 9.141589544015005e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0019452712731435895 (target word token id=279)
Association score for sentence 19: 4.951776859306982
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz']
For TM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der']
For TM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft']
For TM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Maurer', '*', 'in', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr']
For TM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'de', 'über', 'eine'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'Art', 'und', '##e'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', '##er', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', 'de', '##r', '##er', '##e'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'de', 'über', 'eine'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'Art', 'und', '##e'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', '##er', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', 'de', '##r', '##er', '##e'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'bei', 'als', 'nur'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'in', 'nur', 'mit'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'gem', 'und', 'hoch', 'wenig'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'de', 'über', 'eine'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'Art', 'und', '##e'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', '##er', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', 'de', '##r', '##er', '##e'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'bei', 'als', 'nur'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'in', 'nur', 'mit'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'gem', 'und', 'hoch', 'wenig'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'de', 'über', 'eine'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'Art', 'und', '##e'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', '##er', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', 'de', '##r', '##er', '##e'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'bei', 'als', 'nur'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'in', 'nur', 'mit'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'gem', 'und', 'hoch', 'wenig'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'bei', 'als', 'nur'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'in', 'nur', 'mit'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'gem', 'und', 'hoch', 'wenig'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00018702942179515958
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 0: -3.071989048126897
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.014020661823451519
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 1: 1.245032376732945
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.001345084048807621
Prior probability (p_prior): 0.008602000772953033 for 960
Prior probability (p_prior) for sie: 5.211948064243188e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0008893158519640565 (target word token id=279)
Association score for sentence 2: -1.8555383232977156
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0012238690396770835
Prior probability (p_prior): 0.008602000772953033 for 960
Prior probability (p_prior) for sie: 5.211948064243188e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0008893158519640565 (target word token id=279)
Association score for sentence 3: -1.949977639580879
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00025918608298525214
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 4: -2.745708717558381
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.002792094135656953
Prior probability (p_prior): 0.04057104513049126 for 960
Prior probability (p_prior) for sie: 8.955343218985945e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00014736779849044979 (target word token id=279)
Association score for sentence 5: -2.676262737510956
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.05643972009420395
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 6: 2.6376734581945023
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.022477397695183754
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 7: 1.7170105480079194
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0020515224896371365
Prior probability (p_prior): 0.008602000772953033 for 960
Prior probability (p_prior) for sie: 5.211948064243188e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0008893158519640565 (target word token id=279)
Association score for sentence 8: -1.4334126288455526
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.009209227748215199
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 9: 0.8247062880544296
Processing sentence 10
Input IDs for sentence 10: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 10: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0039011917542666197
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 10: -0.0342176249531299
Processing sentence 11
Input IDs for sentence 11: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 11: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0008287837845273316
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 11: -1.5832956820954815
Processing sentence 12
Input IDs for sentence 12: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 12: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.05643972009420395
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 12: 2.6376734581945023
Processing sentence 13
Input IDs for sentence 13: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 13: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0024835385847836733
Prior probability (p_prior): 0.04057104513049126 for 960
Prior probability (p_prior) for sie: 8.955343218985945e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00014736779849044979 (target word token id=279)
Association score for sentence 13: -2.7933702461535685
Processing sentence 14
Input IDs for sentence 14: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 14: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.023067012429237366
Prior probability (p_prior): 0.008602000772953033 for 960
Prior probability (p_prior) for sie: 5.211948064243188e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0008893158519640565 (target word token id=279)
Association score for sentence 14: 0.9864087393310887
Processing sentence 15
Input IDs for sentence 15: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 15: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.008849548175930977
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 15: 0.7848666947230796
Processing sentence 16
Input IDs for sentence 16: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 16: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0007024173391982913
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 16: -1.748727260745505
Processing sentence 17
Input IDs for sentence 17: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 17: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.00026098196394741535
Prior probability (p_prior): 0.04057104513049126 for 960
Prior probability (p_prior) for sie: 8.955343218985945e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00014736779849044979 (target word token id=279)
Association score for sentence 17: -5.046358615749174
Processing sentence 18
Input IDs for sentence 18: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 18: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.06404566019773483
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 18: 2.7640965601451635
Processing sentence 19
Input IDs for sentence 19: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 19: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.004542384762316942
Prior probability (p_prior): 0.04057104513049126 for 960
Prior probability (p_prior) for sie: 8.955343218985945e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00014736779849044979 (target word token id=279)
Association score for sentence 19: -2.1896024858657968
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Name', 'Vater', 'Bruder', 'Leben', 'Körper'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', '.', '##r'] (top-5 predictions)
For TAM: MASK at position 5: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 6: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 7: ['##r', '.', 'de', 'hoch', '##e'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', '.', '##r'] (top-5 predictions)
For TAM: MASK at position 5: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 6: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 7: ['##r', '.', 'de', 'hoch', '##e'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Vater', 'Bruder', 'Leben', 'Körper'] (top-5 predictions)
For TAM: MASK at position 4: ['für', '.', 'eine', 'nur', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['.', 'für', '-', ',', 'und'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '##d', '*', '##n', '##haft'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', '.', '##r'] (top-5 predictions)
For TAM: MASK at position 5: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 6: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 7: ['##r', '.', 'de', 'hoch', '##e'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.010103782638907433
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 0: -1.3822392712925147
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.18644821643829346
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 1: 1.5330043969521348
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.25688478350639343
Prior probability (p_prior): 0.0013107458362355828 for 3726
Prior probability (p_prior) for sie: 3.59075147571275e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0001792296679923311 (target word token id=279)
Association score for sentence 2: 5.278031355330096
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.20873238146305084
Prior probability (p_prior): 0.0013107458362355828 for 3726
Prior probability (p_prior) for sie: 3.59075147571275e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0001792296679923311 (target word token id=279)
Association score for sentence 3: 5.070456644154048
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.14479747414588928
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 4: 1.2801868923268032
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2093,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 3726
Target probability (p_T): 0.2487489879131317
Prior probability (p_prior): 0.03273763507604599 for 3726
Prior probability (p_prior) for sie: 1.071704082278302e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.000285144429653883 (target word token id=279)
Association score for sentence 5: 2.027918971479937
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.20343373715877533
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 6: 1.6201911922020684
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.12759128212928772
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 7: 1.1536829029770477
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.2284923791885376
Prior probability (p_prior): 0.0013107458362355828 for 3726
Prior probability (p_prior) for sie: 3.59075147571275e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0001792296679923311 (target word token id=279)
Association score for sentence 8: 5.160906542467584
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.13884341716766357
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 9: 1.238197659232595
Batch 2: Associations calculated.
Evaluation completed.
Results saved to ../data/output_csv_files/german/results_DE_baseline_perplexity_gender_neutral_sample.csv
