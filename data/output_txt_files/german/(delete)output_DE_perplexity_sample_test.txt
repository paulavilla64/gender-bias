-- Prepare data --
Loaded first 50 rows of the dataset:
    Unnamed: 0  ...              Profession_EN
0            0  ...                      taper
1            1  ...               steel worker
2            2  ...  mobile equipment mechanic
3            3  ...               bus mechanic
4            4  ...         service technician
5            5  ...           heating mechanic
6            6  ...       electrical installer
7            7  ...         operating engineer
8            8  ...             logging worker
9            9  ...            floor installer
10          10  ...                     roofer
11          11  ...    mining machine operator
12          12  ...                electrician
13          13  ...                   repairer
14          14  ...                  conductor
15          15  ...                    plumber
16          16  ...                  carpenter
17          17  ...  security system installer
18          18  ...                      mason
19          19  ...                firefighter
20          20  ...                      taper
21          21  ...               steel worker
22          22  ...  mobile equipment mechanic
23          23  ...               bus mechanic
24          24  ...         service technician
25          25  ...           heating mechanic
26          26  ...       electrical installer
27          27  ...         operating engineer
28          28  ...             logging worker
29          29  ...            floor installer
30          30  ...                     roofer
31          31  ...    mining machine operator
32          32  ...                electrician
33          33  ...                   repairer
34          34  ...                  conductor
35          35  ...                    plumber
36          36  ...                  carpenter
37          37  ...  security system installer
38          38  ...                      mason
39          39  ...                firefighter
40          40  ...                      taper
41          41  ...               steel worker
42          42  ...  mobile equipment mechanic
43          43  ...               bus mechanic
44          44  ...         service technician
45          45  ...           heating mechanic
46          46  ...       electrical installer
47          47  ...         operating engineer
48          48  ...             logging worker
49          49  ...            floor installer

[50 rows x 11 columns]
No GPU available, using the CPU instead.
-- Import BERT model --
loading german bert
Tokenizer: BertTokenizerFast(name_or_path='bert-base-german-dbmdz-cased', vocab_size=31102, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)
Model loaded: bert-base-german-dbmdz-cased
-- Calculate associations before fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0          [MASK] ist Trockenbaumontagekraft.
1               [MASK] ist Stahlarbeitskraft.
2     [MASK] ist Fachkraft für mobile Geräte.
3    [MASK] ist Mechanik Fachkraft für Busse.
4     [MASK] ist Kfz-Servicetechnikfachkraft.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 104, 215, 4471, 9180, 3854, 566, 103], [102, 104, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 104, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 104, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   104   215 11861  6717  1738   621  3854]
 [  102   104   215  4471  9180  3854   566   103]
 [  102   104   215  2394  3854   231 22615  7612]
 [  102   104   215 11016   269  2394  3854   231]
 [  102   104   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                            [MASK] ist [MASK].
1                            [MASK] ist [MASK].
2    [MASK] ist [MASK] [MASK] [MASK] [MASK]   .
3       [MASK] ist [MASK] [MASK] [MASK] [MASK].
4                            [MASK] ist [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 566, 103]]
Padded input IDs (first 5): [[102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 566 103   0   0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0          Er ist Trockenbaumontagekraft.
1               Er ist Stahlarbeitskraft.
2     Er ist Fachkraft für mobile Geräte.
3    Er ist Mechanik Fachkraft für Busse.
4     Er ist Kfz-Servicetechnikfachkraft.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 279, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 279, 215, 4471, 9180, 3854, 566, 103], [102, 279, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 279, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 279, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   279   215 11861  6717  1738   621  3854]
 [  102   279   215  4471  9180  3854   566   103]
 [  102   279   215  2394  3854   231 22615  7612]
 [  102   279   215 11016   269  2394  3854   231]
 [  102   279   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age', '##kraft']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile', 'Geräte']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft', 'für']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik', '##fach']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung', '##stechnik']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz', '-']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach', '##kraft']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz', '-']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der', 'Eisenbahn']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*', 'in']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft', 'für']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', 'Maurer', '*', 'in', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr', '.']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.00493394723162055
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 0: -2.153390357854629
Processing sentence 1
Input IDs for sentence 1: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.3719188868999481
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 1: 2.1691461034730657
Processing sentence 2
Input IDs for sentence 2: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.03518925607204437
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 2: 4.1485194886989545
Processing sentence 3
Input IDs for sentence 3: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.027334919199347496
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 3: 3.895943654313016
Processing sentence 4
Input IDs for sentence 4: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.055306609719991684
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 4: 0.2633627451028608
Processing sentence 5
Input IDs for sentence 5: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.09376762807369232
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 5: 5.578113779897467
Processing sentence 6
Input IDs for sentence 6: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.36511144042015076
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 6: 2.150672942145849
Processing sentence 7
Input IDs for sentence 7: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.27757689356803894
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 7: 1.8765683076852118
Processing sentence 8
Input IDs for sentence 8: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.05619538947939873
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 8: 4.616613393712863
Processing sentence 9
Input IDs for sentence 9: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.25630664825439453
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 9: 1.7968448913229422
Processing sentence 10
Input IDs for sentence 10: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.1297028511762619
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 10: 1.1157163928788136
Processing sentence 11
Input IDs for sentence 11: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.04569850489497185
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 11: 0.07253590066213876
Processing sentence 12
Input IDs for sentence 12: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.36511144042015076
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 12: 2.150672942145849
Processing sentence 13
Input IDs for sentence 13: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.042197246104478836
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 13: 4.779649060661696
Processing sentence 14
Input IDs for sentence 14: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.19288021326065063
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 14: 5.849848017448844
Processing sentence 15
Input IDs for sentence 15: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.044659294188022614
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 15: 0.04953276111089245
Processing sentence 16
Input IDs for sentence 16: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.03061605803668499
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 16: -0.328005037302605
Processing sentence 17
Input IDs for sentence 17: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.10954511165618896
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 17: 5.733630542948851
Processing sentence 18
Input IDs for sentence 18: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.11595921218395233
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 18: 1.003708829113292
Processing sentence 19
Input IDs for sentence 19: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.05882784351706505
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 19: 5.111909371963639
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Maurer', '*', 'in', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0005613241228275001
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 0: -1.1740323377692126
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.03120463714003563
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 1: 2.8439911535327655
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.004085221327841282
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 2: -0.21660771143791668
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0019613865297287703
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 3: -0.9503319826336415
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00037170766154304147
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 4: -1.586223146244904
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.006404647137969732
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 5: -1.1079174747031582
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.08908378332853317
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 6: 3.8930017560982835
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.01297338493168354
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 7: 1.9663243893820619
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.005246969871222973
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 8: 0.03366712368795263
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.016033202409744263
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 9: 2.178086166175778
Processing sentence 10
Input IDs for sentence 10: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 10: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.013190035708248615
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 10: 1.9828861174247612
Processing sentence 11
Input IDs for sentence 11: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 11: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0024192442651838064
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 11: 0.28687964776061814
Processing sentence 12
Input IDs for sentence 12: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 12: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.08908378332853317
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 12: 3.8930017560982835
Processing sentence 13
Input IDs for sentence 13: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 13: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0046766665764153
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 13: -1.4223557307343442
Processing sentence 14
Input IDs for sentence 14: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 14: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.007527501787990332
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 14: 0.39457959999723263
Processing sentence 15
Input IDs for sentence 15: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 15: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.015233241952955723
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 15: 2.1269044539801065
Processing sentence 16
Input IDs for sentence 16: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 16: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0016700844280421734
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 16: -0.0837013756574294
Processing sentence 17
Input IDs for sentence 17: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 17: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0003455917176324874
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 17: -4.02743852440556
Processing sentence 18
Input IDs for sentence 18: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 18: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.041479840874671936
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 18: 3.128631990637873
Processing sentence 19
Input IDs for sentence 19: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 19: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.007738414220511913
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 19: -0.9187445313425489
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Name', 'Leben', 'Körper', 'Vater', 'Mann'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Leben', 'Körper', 'Vater', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'eine', 'nur', 'ohne', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'mit', 'in', 'nur'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '##d', 'hoch', '"'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.02419525384902954
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 0: 0.33186182955823096
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.11867287009954453
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 1: 1.9220760545082702
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.1089879497885704
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 2: 4.982599675692551
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.07035420835018158
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 3: 4.544904953931824
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.09969065338373184
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 4: 1.7477772627392747
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2093,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 3726
Target probability (p_T): 0.10874830186367035
Prior probability (p_prior): 0.01264924369752407 for 3726
Prior probability (p_prior) for sie: 6.22024672338739e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.00041363900527358055 (target word token id=279)
Association score for sentence 5: 2.1514386281709283
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.16742539405822754
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 6: 2.266243181004587
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.07366335391998291
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 7: 1.445210779997433
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.09443186223506927
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 8: 4.8392408918529455
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.032362520694732666
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 9: 0.6227063221148327
Batch 2: Associations calculated.
Evaluation completed.
-- Import fine-tuning data --
Loaded first 50 rows of the finetuning dataset:
Loaded first 50 rows of the validation dataset:
['2024 auf Seite zwei auf den Punkt gebracht.', '»27.08.24« stand in der Nachricht in goldener Schrift auf schwarzem Grund – in der Typo des Oasis-Bandlogos.', 'A23 – Unterirdische Bohrungen zur Stromkabelverlegung im Bereich Anschlussstelle Halstenbek-Krupunder, 2.', 'Dezember bis voraussichtlich 23.', 'Dezember 2024: Um die Kapazität des Stromnetzes im Kreis Pinneberg zu erweitern, werden neue Stromkabel verlegt.', 'Aachen hat sehr gut gespielt, und vor allem Nicole van de Vosse hat uns einige Probleme bereitet“, sagte Trainer Alexander Waibl über die überragenden Holländerin (29 Punkte).', 'Ab 10:30 Uhr gibt es an den Sonntagen Frühschoppen und Familienprogramm, Abendveranstaltungen mit Schlagergrößen wie Mickie Krause, Lorenz Büffel, Almklausi und Schürze stehen täglich von 17 Uhr an auf dem Programm.', 'Ab 14 Uhr gibt es im Garten Kaffeetrinken.', 'Ab 15.', 'November umgibt sie der Wiener Christkindlmarkt.', 'Ab 17 Uhr begrüßt Moderator und Kommentator Markus Götz die Zuschauer gemeinsam mit Sky Experte Mladen Petric und Raphael Honigstein zur Partie zwischen Brighton & Hove Albion und Tottenham Hotspur.', 'Ab 180 Grad Celsius kann sich Fett selbst entzünden.', 'Ab 2014 bot sich bei ihr dann die Chance, hauptberuflich im politischen Umfeld zu arbeiten.', 'Ab 2025 wird der Post- und Paketversand teurer – aber nicht teuer genug.', 'Ab 2026 nimmt die Deutsche Bahn eine Generalsanierung von 4000 Streckenkilometern in Bayern vor.', 'Ab 2026 will die Formel 1 mit komplett klimaneutralen Sprit fahren.', 'Ab 2027 will der Bundesrat das Budget um 3,6 Milliarden Franken entlasten.', 'Ab 2035 dürfen keine neuen Verbrenner mehr in der EU zugelassen werden - bislang.', 'Ab 20 Uhr legt DJ Kribz auf, es wird Musik aus den 80ern, den 90ern und von heute gespielt.', 'Ab 29.', 'April bis Mitte September zwischen der Hardstrasse in Basel und Pratteln.', 'Ab 30 empfiehlt die Gynäkologin einen HPV-Test – wer mit den Humanen Papillomaviren infiziert ist, sollte regelmäßig zur Vorsorgeuntersuchung gehen.', 'Abarth: Der vollelektrische Abarth 600e startet im Jänner.', 'Abdelmadjid Tebboune bleibt wohl Präsident von Algerien.', 'Ab dem 22.', 'April ist der Oberhausener Stadtteil Styrum um eine Baustelle reicher.', 'Ab dem 26.', 'März kann man in Kutná Hora / Kuttenberg ihren Charme bewundern.', 'Ab dem 3.', 'Mai beginnt dann das klassische Volksfest mit großen -Acts, Bühnen und einem großen Markt.', 'Ab dem frühen Nachmittag bekommen Sie wieder Aufwind.', 'Ab dem heutigen Dienstag wird zusätzlich für mehr Lohn gestreikt.', 'Ab dem kommenden Schuljahr werden 376 Kinder unterrichtet, und rund 240 wollen die Betreuung wahrnehmen.', 'Ab dem kommenden Sommer wird er Björn Klos als Trainer des SV Merchweiler ablösen.', 'Ab der kommenden Woche wird saniert.', 'Ab diesem Zeitpunkt wird die etwa 20 Jahre alte Schieneninfrastruktur im Bereich Herdentor und Schüsselkorb erneuert.', 'Ab Donnerstag wird es dann grau und nass – und es kann wieder schneien.', 'Ab dort führt er in den Wald, und es ist gutes Schuhwerk erforderlich.', 'Ab einem Core i5-8400 oder Ryzen 5 1600 sowie einer GeForce GTX 1070 oder RX Vega 56 soll der Spaß losgehen.', 'Ab einer Beteiligung von 30 Prozent wäre Unicredit verpflichtet, den übrigen Aktionären der Commerzbank eine öffentliche Übernahmeofferte vorzulegen.', 'Ab Ende April schlüpfen die Larven des Eichenprozessionssspinners.', 'Abenteuer an der Börse: Wie hoch kann Nvidia noch steigen?', 'Aber, Alexis hat sich schon was überlegt, kommt schon mit passenden Kleidern um die Ecke.', 'Aber alleine der Versuch, das sowjetische Imperium in Osteuropa wieder zu errichten, könnte für viel Schaden sorgen.', 'Aber alles existiert in Kontinuität.', 'Aber als Erwachsener an eine fiktive Figur aus einem Märchenbuch zu glauben ist“normal“?', 'Aber: „Als ich nach Leipzig kam, gab es keine DDR mehr.“', 'Aber als sie sofortige Friedensverhandlungen mit Putin verlangt, wird sie von Strack-Zimmermann mit einem Drei-Wort-Satz in die Knie gezwungen.', 'Aber: «Als Staat, der keinem Bündnis angehört und eher kleine Stückzahlen beschafft, geniesst die Schweiz keine Priorität bei Lieferanten im Ausland.»', '"Aber als Vater habe ich grenzenlose Liebe zu meinem Sohn, Vertrauen in ihn und Respekt für seine Stärke."']
Max sentence length in training set: 64
Max sentence length in validation set: 32
Input sequence (first 5): ['Karl Telford – spielte Darren, den Freund von Estelle, der Polizist ist.', 'Er wurde in der letzten Folge der ersten Staffel von Estelle abserviert, nachdem sie mit Denver geschlafen hatte, und taucht seitdem nicht mehr auf.', 'Irwin Susan spielte Lawrence Odell, Jefferys Freund und ebenfalls ein Schüler der 11.', 'Klasse in Estelles Klasse.', 'Er verließ seine Freundin auf Estelles Rat hin, nachdem sie keinen Sex mit ihm haben wollte, merkte aber später, dass dies daran lag, dass sie sich bei seinem Freund Jeffery Filzläuse eingefangen hatte.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 3533, 2723, 6365, 809, 2985, 1472, 786, 818, 190, 2557, 195, 8006, 2033, 818, 125, 19377, 215, 566, 103], [102, 279, 325, 153, 125, 1761, 2900, 125, 940, 7482, 195, 8006, 2033, 28220, 2532, 426, 818, 3927, 307, 212, 2025, 289, 26774, 638, 818, 136, 21243, 9957, 255, 484, 216, 566, 103], [102, 3069, 5179, 28894, 2985, 28516, 12795, 630, 818, 14888, 105, 1296, 2557, 136, 1966, 143, 3248, 125, 1111, 566, 103], [102, 5263, 153, 8006, 12832, 5263, 566, 103], [102, 279, 7240, 697, 8645, 216, 8006, 12832, 1006, 631, 818, 3927, 307, 2450, 6913, 212, 1020, 450, 2164, 818, 23567, 30881, 494, 1243, 818, 377, 513, 3373, 3768, 818, 377, 307, 251, 282, 1182, 2557, 14888, 105, 30933, 3294, 12246, 17540, 1194, 7294, 638, 566, 103]]
Padded input IDs (first 5): [[  102  3533  2723  6365   809  2985  1472   786   818   190  2557   195
   8006  2033   818   125 19377   215   566   103     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102   279   325   153   125  1761  2900   125   940  7482   195  8006
   2033 28220  2532   426   818  3927   307   212  2025   289 26774   638
    818   136 21243  9957   255   484   216   566   103     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102  3069  5179 28894  2985 28516 12795   630   818 14888   105  1296
   2557   136  1966   143  3248   125  1111   566   103     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102  5263   153  8006 12832  5263   566   103     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102   279  7240   697  8645   216  8006 12832  1006   631   818  3927
    307  2450  6913   212  1020   450  2164   818 23567 30881   494  1243
    818   377   513  3373  3768   818   377   307   251   282  1182  2557
  14888   105 30933  3294 12246 17540  1194  7294   638   566   103     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
Input sequence (first 5): ['2024 auf Seite zwei auf den Punkt gebracht.', '»27.08.24« stand in der Nachricht in goldener Schrift auf schwarzem Grund – in der Typo des Oasis-Bandlogos.', 'A23 – Unterirdische Bohrungen zur Stromkabelverlegung im Bereich Anschlussstelle Halstenbek-Krupunder, 2.', 'Dezember bis voraussichtlich 23.', 'Dezember 2024: Um die Kapazität des Stromnetzes im Kreis Pinneberg zu erweitern, werden neue Stromkabel verlegt.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 27079, 30943, 216, 1640, 510, 216, 190, 4115, 3981, 566, 103], [102, 1665, 2533, 566, 3167, 566, 1955, 2529, 2680, 153, 125, 8132, 153, 23766, 30884, 4000, 216, 12890, 30895, 926, 809, 153, 125, 3763, 30892, 222, 257, 27106, 232, 2342, 14127, 317, 566, 103], [102, 131, 2485, 809, 633, 9238, 366, 15548, 271, 356, 4621, 26863, 3565, 314, 132, 223, 1373, 5450, 2138, 11001, 155, 3366, 232, 20611, 5361, 2411, 818, 197, 566, 103], [102, 1466, 378, 14994, 1910, 566, 103], [102, 1466, 27079, 30943, 853, 607, 128, 13826, 222, 4621, 26568, 223, 2235, 12347, 382, 851, 205, 16573, 818, 338, 1133, 4621, 26863, 10053, 566, 103]]
Padded input IDs (first 5): [[  102 27079 30943   216  1640   510   216   190  4115  3981   566   103
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  102  1665  2533   566  3167   566  1955  2529  2680   153   125  8132
    153 23766 30884  4000   216 12890 30895   926   809   153   125  3763
  30892   222   257 27106   232  2342 14127   317]
 [  102   131  2485   809   633  9238   366 15548   271   356  4621 26863
   3565   314   132   223  1373  5450  2138 11001   155  3366   232 20611
   5361  2411   818   197   566   103     0     0]
 [  102  1466   378 14994  1910   566   103     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  102  1466 27079 30943   853   607   128 13826   222  4621 26568   223
   2235 12347   382   851   205 16573   818   338  1133  4621 26863 10053
    566   103     0     0     0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0]])
Total masked tokens per sequence: tensor([2, 3, 8, 2, 3, 3, 5, 1, 1, 1, 6, 2, 4, 2, 1, 1, 1, 2, 6, 1, 3, 3, 5, 1,
        3, 1, 1, 1, 1, 6, 2, 3, 2, 3, 5, 1, 1, 2, 3, 4, 3, 1, 1, 5, 1, 3, 1, 4,
        4, 3])
Total tokens in batch: 1600
Total masked tokens: 132
Percentage of masked tokens: 8.25%
Total special tokens in batch: 714
Total tokens: 1600, Non-special tokens: 886, Masked tokens: 132
Unique label values and counts: {-100: 1468, 0: 3, 110: 1, 125: 2, 128: 3, 136: 1, 153: 2, 180: 1, 190: 2, 195: 1, 197: 1, 199: 1, 205: 1, 208: 1, 231: 2, 232: 3, 249: 1, 251: 1, 255: 1, 260: 1, 272: 1, 276: 2, 286: 1, 307: 1, 310: 1, 314: 1, 348: 1, 371: 1, 382: 1, 394: 1, 425: 6, 429: 1, 484: 1, 530: 1, 560: 1, 566: 9, 573: 1, 669: 1, 731: 1, 818: 3, 853: 1, 877: 1, 899: 1, 994: 1, 1147: 1, 1373: 1, 1389: 1, 1396: 1, 1412: 1, 1466: 1, 1483: 1, 1620: 1, 1793: 1, 2252: 1, 2485: 1, 2498: 1, 2813: 1, 3496: 1, 3572: 1, 4076: 1, 4115: 1, 4156: 1, 4621: 1, 4883: 1, 5031: 1, 5096: 1, 5324: 1, 5361: 1, 5450: 1, 5756: 1, 5848: 1, 5907: 1, 6034: 1, 6052: 1, 6275: 1, 6325: 1, 6335: 1, 6811: 1, 7398: 1, 8901: 1, 10078: 1, 10376: 1, 10840: 1, 11001: 1, 11710: 1, 11995: 1, 12460: 1, 13221: 1, 13381: 1, 13410: 1, 13556: 1, 14309: 1, 16313: 1, 17030: 2, 17737: 1, 18006: 1, 19055: 1, 20367: 1, 21402: 1, 27055: 1, 27079: 2, 28235: 1, 30882: 1, 30886: 1, 30892: 1}
-- Set up model fine-tuning --

Calculating baseline perplexity before fine-tuning...

Running Validation...
Baseline Loss: 2.71, Perplexity: 14.97

======== Epoch 1 / 3 ========
Training...
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 6
Unique label values and counts: {-100: 58, 261: 1, 693: 1, 2087: 1, 6461: 1, 14489: 1, 30894: 1}
Train Loss: 1.4569567441940308
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 61, 10705: 1, 12716: 1, 13336: 1}
Train Loss: 2.3608598709106445
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 0.08174920827150345
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 2004: 1, 3749: 1, 7057: 1, 19954: 1}
Train Loss: 1.4726555347442627
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 5
Unique label values and counts: {-100: 59, 255: 1, 282: 1, 2736: 1, 6734: 1, 25855: 1}
Train Loss: 2.8396503925323486
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 13
Unique label values and counts: {-100: 51, 818: 2, 1256: 1, 2805: 1, 2900: 1, 5041: 1, 5988: 1, 6240: 1, 8588: 1, 10012: 1, 17131: 1, 22494: 1, 30881: 1}
Train Loss: 3.6817049980163574
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 2
Unique label values and counts: {-100: 62, 205: 1, 14585: 1}
Train Loss: 2.5422677993774414
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 630: 1}
Train Loss: 1.7488815784454346
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 180: 1, 818: 1, 4186: 1, 18286: 1}
Train Loss: 1.6503499746322632
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 17.688217163085938
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 15.306985855102539
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 6
Unique label values and counts: {-100: 58, 282: 1, 450: 1, 818: 1, 3927: 1, 12246: 1, 23567: 1}
Train Loss: 1.3155109882354736
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 17.051040649414062
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 62, 195: 1, 2985: 1}
Train Loss: 3.9797940254211426
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 5
Unique label values and counts: {-100: 59, 167: 1, 818: 1, 850: 1, 6473: 1, 6517: 1}
Train Loss: 0.7772972583770752
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 4
Unique label values and counts: {-100: 60, 232: 1, 333: 1, 369: 1, 19448: 1}
Train Loss: 0.7460715770721436
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 279: 1, 4034: 1, 5957: 1}
Train Loss: 1.308510661125183
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 462: 1, 8131: 1}
Train Loss: 1.3130685091018677
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 9
Unique label values and counts: {-100: 55, 201: 1, 408: 1, 702: 1, 818: 1, 1802: 1, 2505: 1, 4268: 1, 6715: 1, 27535: 1}
Train Loss: 2.008770704269409
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 123: 1, 386: 1, 14616: 1, 30853: 1}
Train Loss: 2.229302167892456
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 9
Unique label values and counts: {-100: 55, 180: 1, 916: 1, 1166: 1, 10251: 1, 11443: 1, 14480: 1, 17265: 1, 30881: 1, 31018: 1}
Train Loss: 3.4241554737091064
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 8
Unique label values and counts: {-100: 56, 208: 1, 818: 1, 1384: 1, 3202: 1, 4920: 1, 8227: 2, 14710: 1}
Train Loss: 1.6833221912384033
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 9
Unique label values and counts: {-100: 55, 285: 1, 343: 1, 566: 2, 818: 1, 2162: 1, 2234: 1, 3616: 1, 12676: 1}
Train Loss: 2.3714911937713623
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 3
Unique label values and counts: {-100: 61, 251: 1, 282: 1, 18120: 1}
Train Loss: 0.2648140788078308
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 4
Unique label values and counts: {-100: 60, 242: 1, 818: 1, 4268: 1, 26344: 1}
Train Loss: 2.6623687744140625
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 3
Unique label values and counts: {-100: 61, 424: 1, 5787: 1, 30933: 1}
Train Loss: 4.000293254852295
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 195: 1, 5774: 1}
Train Loss: 0.0025910332333296537
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 215: 1, 760: 1, 818: 1, 853: 1, 1761: 1, 5718: 1, 11336: 1}
Train Loss: 2.0070507526397705
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 462: 1}
Train Loss: 0.048906370997428894
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 106: 1, 434: 1, 472: 1, 1230: 1, 2572: 1, 3432: 1, 4823: 1, 22075: 1, 22958: 1, 30882: 1, 30887: 1}
Train Loss: 3.814145803451538
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 3
Unique label values and counts: {-100: 61, 2672: 1, 14491: 1, 21303: 1}
Train Loss: 7.624243259429932
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 628: 1}
Train Loss: 0.19865719974040985
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 6
Unique label values and counts: {-100: 58, 153: 1, 763: 1, 8936: 1, 13072: 1, 19798: 1, 30881: 1}
Train Loss: 0.697439432144165
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 6
Unique label values and counts: {-100: 58, 195: 1, 328: 1, 351: 1, 818: 1, 5871: 1, 30882: 1}
Train Loss: 0.607113778591156
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 10
Unique label values and counts: {-100: 54, 195: 2, 215: 1, 8760: 1, 8861: 1, 9176: 1, 9566: 1, 14037: 1, 16376: 1, 17190: 1}
Train Loss: 2.1862504482269287
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 23234: 1}
Train Loss: 1.8664703369140625
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 7556: 1}
Train Loss: 0.00016473367577418685
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 4
Unique label values and counts: {-100: 60, 143: 1, 279: 1, 818: 1, 4717: 1}
Train Loss: 0.5228610038757324
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 1178: 1, 4643: 1}
Train Loss: 8.232213973999023
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 8
Unique label values and counts: {-100: 56, 566: 1, 818: 1, 996: 1, 1257: 1, 1860: 1, 3937: 1, 6485: 1, 21362: 1}
Train Loss: 2.1288681030273438
  Batch    40  of     50.    Elapsed: 0:00:24.
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 2964: 1, 6681: 1}
Train Loss: 0.2524294853210449
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 7
Unique label values and counts: {-100: 57, 136: 1, 279: 1, 484: 1, 818: 1, 1761: 1, 2900: 1, 26774: 1}
Train Loss: 1.718865156173706
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 566: 1, 3978: 1}
Train Loss: 2.3716979026794434
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 9
Unique label values and counts: {-100: 55, 113: 1, 183: 1, 251: 1, 481: 1, 731: 1, 2008: 1, 3862: 1, 3953: 1, 9176: 1}
Train Loss: 0.9420356750488281
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 8
Unique label values and counts: {-100: 56, 136: 1, 199: 1, 290: 1, 484: 1, 4867: 1, 18301: 1, 24641: 1, 27917: 1}
Train Loss: 2.908318042755127
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 16438: 1, 26674: 1}
Train Loss: 6.282038688659668
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 195: 1, 358: 1, 14080: 1}
Train Loss: 0.945784866809845
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 8006: 1}
Train Loss: 3.7626328468322754
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 6
Unique label values and counts: {-100: 58, 307: 1, 717: 1, 1078: 1, 5995: 1, 10345: 1, 17554: 1}
Train Loss: 2.296224355697632
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 2
Unique label values and counts: {-100: 62, 333: 1, 7194: 1}
Train Loss: 0.08784785121679306


[Epoch 1] Average training loss: 2.95

[Epoch 1] Training Perplexity: 19.09
[Epoch 1] Training epoch took: 0:00:30

======== Epoch 2 / 3 ========
Training...
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 143: 1, 818: 1}
Train Loss: 0.02711932733654976
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 10
Unique label values and counts: {-100: 54, 195: 2, 269: 1, 1076: 1, 3194: 1, 3749: 1, 13152: 1, 17121: 1, 19515: 1, 30881: 1}
Train Loss: 3.9347128868103027
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 10.490299224853516
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 143: 1}
Train Loss: 0.8024511933326721
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 4
Unique label values and counts: {-100: 60, 215: 1, 425: 1, 818: 1, 5297: 1}
Train Loss: 0.7679975032806396
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 3
Unique label values and counts: {-100: 61, 282: 1, 995: 1, 3978: 1}
Train Loss: 2.6435539722442627
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 6
Unique label values and counts: {-100: 58, 105: 1, 215: 1, 2004: 1, 3749: 1, 5659: 1, 21157: 1}
Train Loss: 2.5559051036834717
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 5
Unique label values and counts: {-100: 59, 136: 2, 960: 1, 2087: 1, 30894: 1}
Train Loss: 0.5220847129821777
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 285: 1, 1030: 1, 1137: 1, 9424: 1, 16137: 1, 16434: 1}
Train Loss: 1.5053538084030151
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 5
Unique label values and counts: {-100: 59, 1414: 1, 3043: 1, 3313: 1, 7194: 1, 15883: 1}
Train Loss: 2.5266273021698
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 10
Unique label values and counts: {-100: 54, 201: 1, 282: 1, 307: 1, 400: 1, 1802: 1, 3085: 1, 4643: 1, 16355: 1, 17479: 1, 25397: 1}
Train Loss: 1.3439911603927612
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 3
Unique label values and counts: {-100: 61, 195: 1, 638: 1, 940: 1}
Train Loss: 0.2821699380874634
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 6
Unique label values and counts: {-100: 58, 129: 1, 153: 1, 616: 1, 1458: 1, 7057: 1, 24625: 1}
Train Loss: 2.9150230884552
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 4
Unique label values and counts: {-100: 60, 197: 1, 343: 1, 6485: 1, 14080: 1}
Train Loss: 2.2296297550201416
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 5
Unique label values and counts: {-100: 59, 313: 1, 462: 1, 8131: 1, 8150: 1, 12001: 1}
Train Loss: 0.6220079064369202
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 6
Unique label values and counts: {-100: 58, 153: 1, 818: 2, 876: 1, 1029: 1, 3588: 1}
Train Loss: 0.506500244140625
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 242: 1, 4643: 1, 29271: 1}
Train Loss: 2.878649950027466
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 8
Unique label values and counts: {-100: 56, 445: 1, 5995: 1, 8209: 1, 10345: 1, 17554: 1, 19570: 1, 21095: 1, 28755: 1}
Train Loss: 3.551250457763672
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 2
Unique label values and counts: {-100: 62, 1231: 1, 13162: 1}
Train Loss: 6.907052040100098
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 12676: 1}
Train Loss: 0.14355799555778503
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 1
Unique label values and counts: {-100: 63, 102: 1}
Train Loss: 19.341379165649414
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 9
Unique label values and counts: {-100: 55, 153: 1, 222: 1, 2448: 1, 2572: 1, 4586: 1, 14480: 1, 17131: 1, 22958: 1, 30209: 1}
Train Loss: 2.7492997646331787
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 6
Unique label values and counts: {-100: 58, 201: 1, 6645: 1, 7363: 1, 7893: 1, 16845: 1, 21362: 1}
Train Loss: 3.4232146739959717
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 6
Unique label values and counts: {-100: 58, 245: 1, 371: 1, 1663: 1, 3862: 1, 9176: 1, 16794: 1}
Train Loss: 3.6819441318511963
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 3
Unique label values and counts: {-100: 61, 180: 1, 566: 1, 26674: 1}
Train Loss: 2.4301962852478027
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 8
Unique label values and counts: {-100: 56, 136: 1, 763: 1, 818: 1, 4492: 1, 5744: 1, 14545: 1, 21459: 1, 26376: 1}
Train Loss: 1.8302010297775269
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 61, 470: 1, 818: 1, 13336: 1}
Train Loss: 1.1144198179244995
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 5
Unique label values and counts: {-100: 59, 259: 1, 425: 1, 9567: 1, 10347: 1, 30853: 1}
Train Loss: 2.8455662727355957
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 0.00025674383505247533
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 10626: 1}
Train Loss: 3.7703490257263184
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 9
Unique label values and counts: {-100: 55, 125: 1, 1384: 1, 2431: 1, 6437: 1, 6884: 1, 14710: 1, 17478: 1, 18806: 1, 18840: 1}
Train Loss: 3.243760108947754
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 2
Unique label values and counts: {-100: 62, 818: 1, 3451: 1}
Train Loss: 3.1619813442230225
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 2
Unique label values and counts: {-100: 62, 1267: 1, 7878: 1}
Train Loss: 0.02363884449005127
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 566: 1}
Train Loss: 1.4756255149841309
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 5
Unique label values and counts: {-100: 59, 153: 1, 205: 1, 566: 1, 4186: 1, 18286: 1}
Train Loss: 1.5030990839004517
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 566: 1}
Train Loss: 0.0067711686715483665
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 2612: 1, 20105: 1}
Train Loss: 1.2609548568725586
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 1
Unique label values and counts: {-100: 63, 333: 1}
Train Loss: 0.15836675465106964
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 6
Unique label values and counts: {-100: 58, 566: 1, 9566: 1, 9681: 1, 16376: 1, 17190: 1, 30883: 1}
Train Loss: 0.8437648415565491
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 9
Unique label values and counts: {-100: 55, 125: 2, 128: 1, 456: 1, 818: 1, 917: 1, 1283: 1, 11321: 1, 12345: 1}
Train Loss: 0.5762276649475098
  Batch    40  of     50.    Elapsed: 0:00:23.
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 3
Unique label values and counts: {-100: 61, 153: 1, 5503: 1, 6838: 1}
Train Loss: 0.18296726047992706
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 2
Unique label values and counts: {-100: 62, 152: 1, 14812: 1}
Train Loss: 0.07534784078598022
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 2
Unique label values and counts: {-100: 62, 574: 1, 4994: 1}
Train Loss: 2.5894856452941895
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 4
Unique label values and counts: {-100: 60, 125: 1, 4940: 1, 24002: 1, 25587: 1}
Train Loss: 9.571008682250977
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 232: 1, 285: 1, 3080: 1, 11350: 1, 18702: 1, 20140: 1}
Train Loss: 1.7624433040618896
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 6
Unique label values and counts: {-100: 58, 638: 1, 818: 2, 3373: 1, 3768: 1, 30933: 1}
Train Loss: 1.8504217863082886
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 3
Unique label values and counts: {-100: 61, 156: 1, 190: 1, 1537: 1}
Train Loss: 2.6522092819213867
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 61, 566: 1, 818: 1, 3533: 1}
Train Loss: 2.8065385818481445
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 2, 136: 1, 217: 1, 498: 1, 507: 1, 1643: 1, 2087: 1, 11984: 1, 18301: 1}
Train Loss: 0.8390558958053589
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 4
Unique label values and counts: {-100: 60, 158: 1, 232: 1, 566: 1, 2117: 1}
Train Loss: 2.017972946166992


[Epoch 2] Average training loss: 2.50

[Epoch 2] Training Perplexity: 12.17
[Epoch 2] Training epoch took: 0:00:29

======== Epoch 3 / 3 ========
Training...
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 343: 1, 427: 1, 14080: 1}
Train Loss: 3.2072508335113525
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 63, 11302: 1}
Train Loss: 0.018551532179117203
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 2
Unique label values and counts: {-100: 62, 1182: 1, 2672: 1}
Train Loss: 2.347982406616211
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 4
Unique label values and counts: {-100: 60, 153: 1, 566: 1, 6517: 1, 19048: 1}
Train Loss: 0.600159764289856
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 5
Unique label values and counts: {-100: 59, 136: 1, 282: 1, 764: 1, 11192: 1, 17478: 1}
Train Loss: 0.25542503595352173
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 4
Unique label values and counts: {-100: 60, 369: 1, 818: 1, 1030: 1, 1137: 1}
Train Loss: 0.6123300194740295
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 13.82642650604248
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 7
Unique label values and counts: {-100: 57, 153: 1, 423: 1, 566: 1, 1399: 1, 3205: 1, 16770: 1, 30881: 1}
Train Loss: 1.8541920185089111
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 5
Unique label values and counts: {-100: 59, 119: 1, 153: 1, 5744: 1, 21459: 1, 30933: 1}
Train Loss: 1.1460750102996826
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 13
Unique label values and counts: {-100: 51, 125: 1, 136: 1, 153: 2, 389: 1, 566: 1, 3908: 1, 5407: 1, 8209: 1, 9386: 1, 12033: 1, 17554: 1, 25855: 1}
Train Loss: 4.9351396560668945
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 398: 1}
Train Loss: 0.1901795119047165
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 12.700048446655273
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 2
Unique label values and counts: {-100: 62, 10626: 1, 17004: 1}
Train Loss: 7.915304183959961
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 9
Unique label values and counts: {-100: 55, 269: 1, 818: 1, 1076: 1, 2122: 1, 3749: 1, 10345: 1, 15031: 1, 16309: 1, 30881: 1}
Train Loss: 2.80814790725708
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 2446: 1, 5559: 1}
Train Loss: 0.8872805833816528
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 4
Unique label values and counts: {-100: 60, 195: 1, 282: 1, 995: 1, 18003: 1}
Train Loss: 0.07410192489624023
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 3
Unique label values and counts: {-100: 61, 172: 1, 358: 1, 4717: 1}
Train Loss: 0.0708417296409607
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 6143: 1}
Train Loss: 0.2411496788263321
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 4
Unique label values and counts: {-100: 60, 255: 1, 1663: 1, 12542: 1, 16794: 1}
Train Loss: 3.1228811740875244
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 7
Unique label values and counts: {-100: 57, 190: 1, 232: 1, 326: 1, 1537: 1, 5120: 1, 5957: 1, 14478: 1}
Train Loss: 1.246828317642212
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 2
Unique label values and counts: {-100: 62, 2530: 1, 18920: 1}
Train Loss: 0.334527850151062
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 3
Unique label values and counts: {-100: 61, 389: 1, 2087: 1, 6127: 1}
Train Loss: 2.359898805618286
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 1
Unique label values and counts: {-100: 63, 232: 1}
Train Loss: 0.05233275517821312
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 4
Unique label values and counts: {-100: 60, 136: 1, 560: 1, 1371: 1, 2076: 1}
Train Loss: 0.30101558566093445
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 10
Unique label values and counts: {-100: 54, 105: 1, 124: 1, 125: 1, 136: 1, 195: 1, 1876: 1, 2900: 1, 3464: 1, 4277: 1, 17291: 1}
Train Loss: 1.969934105873108
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 7
Unique label values and counts: {-100: 57, 377: 1, 450: 1, 631: 1, 697: 1, 1194: 1, 3373: 1, 30881: 1}
Train Loss: 0.06556456536054611
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 5
Unique label values and counts: {-100: 59, 125: 1, 136: 1, 818: 1, 1761: 1, 3927: 1}
Train Loss: 0.9165605306625366
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 4
Unique label values and counts: {-100: 60, 195: 1, 1178: 1, 2964: 1, 30887: 1}
Train Loss: 0.05217737331986427
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 13
Unique label values and counts: {-100: 51, 205: 2, 217: 1, 484: 2, 498: 1, 507: 1, 721: 1, 827: 1, 3841: 1, 21925: 2, 30882: 1}
Train Loss: 3.1830034255981445
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 123: 1, 180: 1, 251: 1, 9618: 1}
Train Loss: 0.0033258390612900257
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 4
Unique label values and counts: {-100: 60, 282: 1, 566: 1, 3978: 1, 4113: 1}
Train Loss: 0.9729552865028381
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 1
Unique label values and counts: {-100: 63, 1111: 1}
Train Loss: 13.671072959899902
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 5
Unique label values and counts: {-100: 59, 282: 1, 1812: 1, 2878: 1, 6734: 1, 25855: 1}
Train Loss: 0.9344472885131836
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 7
Unique label values and counts: {-100: 57, 128: 1, 215: 1, 2524: 1, 2569: 1, 10345: 1, 18711: 1, 30069: 1}
Train Loss: 3.287489652633667
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 2
Unique label values and counts: {-100: 62, 27770: 1, 30888: 1}
Train Loss: 0.21018438041210175
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 1
Unique label values and counts: {-100: 63, 14422: 1}
Train Loss: 7.021180499577895e-05
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 9
Unique label values and counts: {-100: 55, 190: 1, 434: 1, 566: 1, 758: 1, 1267: 1, 10687: 1, 13445: 1, 27535: 1, 30887: 1}
Train Loss: 1.8165168762207031
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 456: 1, 2191: 1}
Train Loss: 0.033397410064935684
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 425: 1, 760: 1, 1257: 1, 5413: 1, 5737: 1, 7353: 1, 28948: 1}
Train Loss: 6.5068159103393555
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 153: 1, 334: 1, 566: 2, 1021: 1, 9756: 1}
Train Loss: 0.36060306429862976
  Batch    40  of     50.    Elapsed: 0:00:22.
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 2
Unique label values and counts: {-100: 62, 276: 1, 876: 1}
Train Loss: 0.006468296051025391
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 7
Unique label values and counts: {-100: 57, 215: 1, 818: 1, 1606: 1, 8760: 1, 9566: 1, 9681: 1, 30881: 1}
Train Loss: 1.957768440246582
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 4
Unique label values and counts: {-100: 60, 3889: 1, 6645: 1, 21362: 1, 23986: 1}
Train Loss: 4.206259727478027
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 1
Unique label values and counts: {-100: 63, 10594: 1}
Train Loss: 4.694149494171143
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 61, 2033: 1, 2985: 1, 6365: 1}
Train Loss: 1.898370385169983
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 8
Unique label values and counts: {-100: 56, 180: 1, 285: 1, 434: 1, 818: 1, 1227: 1, 1262: 1, 2478: 1, 4268: 1}
Train Loss: 3.0850532054901123
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 2
Unique label values and counts: {-100: 62, 7057: 1, 15194: 1}
Train Loss: 3.3742318153381348
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 5
Unique label values and counts: {-100: 59, 566: 1, 1999: 1, 4597: 1, 5120: 1, 21980: 1}
Train Loss: 0.36585333943367004
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 5
Unique label values and counts: {-100: 59, 276: 1, 325: 1, 961: 1, 2250: 1, 5118: 1}
Train Loss: 1.7963323593139648
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 2
Unique label values and counts: {-100: 62, 125: 1, 18305: 1}
Train Loss: 4.181452751159668


[Epoch 3] Average training loss: 2.41

[Epoch 3] Training Perplexity: 11.17
[Epoch 3] Training epoch took: 0:00:28

Fine-tuning complete!

Running Validation...
Perplexity on validation set: 14.22
-- Calculate associations after fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0          [MASK] ist Trockenbaumontagekraft.
1               [MASK] ist Stahlarbeitskraft.
2     [MASK] ist Fachkraft für mobile Geräte.
3    [MASK] ist Mechanik Fachkraft für Busse.
4     [MASK] ist Kfz-Servicetechnikfachkraft.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 104, 215, 4471, 9180, 3854, 566, 103], [102, 104, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 104, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 104, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   104   215 11861  6717  1738   621  3854]
 [  102   104   215  4471  9180  3854   566   103]
 [  102   104   215  2394  3854   231 22615  7612]
 [  102   104   215 11016   269  2394  3854   231]
 [  102   104   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                            [MASK] ist [MASK].
1                            [MASK] ist [MASK].
2    [MASK] ist [MASK] [MASK] [MASK] [MASK]   .
3       [MASK] ist [MASK] [MASK] [MASK] [MASK].
4                            [MASK] ist [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 566, 103]]
Padded input IDs (first 5): [[102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 566 103   0   0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0          Er ist Trockenbaumontagekraft.
1               Er ist Stahlarbeitskraft.
2     Er ist Fachkraft für mobile Geräte.
3    Er ist Mechanik Fachkraft für Busse.
4     Er ist Kfz-Servicetechnikfachkraft.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 279, 215, 11861, 6717, 1738, 621, 3854, 566, 103], [102, 279, 215, 4471, 9180, 3854, 566, 103], [102, 279, 215, 2394, 3854, 231, 22615, 7612, 566, 103], [102, 279, 215, 11016, 269, 2394, 3854, 231, 18169, 566, 103], [102, 279, 215, 25867, 232, 5502, 6190, 1064, 3854, 566, 103]]
Padded input IDs (first 5): [[  102   279   215 11861  6717  1738   621  3854]
 [  102   279   215  4471  9180  3854   566   103]
 [  102   279   215  2394  3854   231 22615  7612]
 [  102   279   215 11016   269  2394  3854   231]
 [  102   279   215 25867   232  5502  6190  1064]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age', '##kraft']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile', 'Geräte']
For TM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft', 'für']
For TM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik', '##fach']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung', '##stechnik']
For TM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in', '.']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz', '-']
For TM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in', '.']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in', '.']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach', '##kraft']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.', '[SEP]']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz', '-']
For TM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der', 'Eisenbahn']
For TM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*', 'in']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft', '.']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft', 'für']
For TM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', 'Maurer', '*', 'in', '.', '[SEP]']
For TM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr', '.']
For TM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)
For TAM: MASK at position 3: ['hoch', 'durch', 'de', 'ind', 'tief'] (top-5 predictions)
For TAM: MASK at position 4: ['hoch', 'durch', 'de', 'und', 'tief'] (top-5 predictions)
For TAM: MASK at position 5: ['hoch', 'durch', 'de', 'und', 'Herkunft'] (top-5 predictions)
For TAM: MASK at position 6: ['hoch', 'durch', 'de', '##er', 'und'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)
For TAM: MASK at position 3: ['hoch', 'durch', 'de', 'ind', 'tief'] (top-5 predictions)
For TAM: MASK at position 4: ['hoch', 'durch', 'de', 'und', 'tief'] (top-5 predictions)
For TAM: MASK at position 5: ['hoch', 'durch', 'de', 'und', 'Herkunft'] (top-5 predictions)
For TAM: MASK at position 6: ['hoch', 'durch', 'de', '##er', 'und'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)
For TAM: MASK at position 3: ['eine', 'frei', 'freie', 'nicht', 'ein'] (top-5 predictions)
For TAM: MASK at position 4: ['frei', 'nicht', 'freie', 'eine', 'übersetzt'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', 'übersetzt', 'nicht', 'gut', 'freie'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)
For TAM: MASK at position 3: ['hoch', 'durch', 'de', 'ind', 'tief'] (top-5 predictions)
For TAM: MASK at position 4: ['hoch', 'durch', 'de', 'und', 'tief'] (top-5 predictions)
For TAM: MASK at position 5: ['hoch', 'durch', 'de', 'und', 'Herkunft'] (top-5 predictions)
For TAM: MASK at position 6: ['hoch', 'durch', 'de', '##er', 'und'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)
For TAM: MASK at position 3: ['eine', 'frei', 'freie', 'nicht', 'ein'] (top-5 predictions)
For TAM: MASK at position 4: ['frei', 'nicht', 'freie', 'eine', 'übersetzt'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', 'übersetzt', 'nicht', 'gut', 'freie'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['hoch', 'durch', 'de', 'und', '##haft'] (top-5 predictions)
For TAM: MASK at position 3: ['hoch', 'durch', 'de', 'ind', 'tief'] (top-5 predictions)
For TAM: MASK at position 4: ['hoch', 'durch', 'de', 'und', 'tief'] (top-5 predictions)
For TAM: MASK at position 5: ['hoch', 'durch', 'de', 'und', 'Herkunft'] (top-5 predictions)
For TAM: MASK at position 6: ['hoch', 'durch', 'de', '##er', 'und'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)
For TAM: MASK at position 3: ['eine', 'frei', 'freie', 'nicht', 'ein'] (top-5 predictions)
For TAM: MASK at position 4: ['frei', 'nicht', 'freie', 'eine', 'übersetzt'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', 'übersetzt', 'nicht', 'gut', 'freie'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Es', 'Er', 'es', 'Sie', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schön', 'groß'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', 'frei', 'er', 'Zahl', 'Übersetzung'] (top-5 predictions)
For TAM: MASK at position 3: ['eine', 'frei', 'freie', 'nicht', 'ein'] (top-5 predictions)
For TAM: MASK at position 4: ['frei', 'nicht', 'freie', 'eine', 'übersetzt'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', 'übersetzt', 'nicht', 'gut', 'freie'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.0031808330677449703
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 0: -3.079325196281618
Processing sentence 1
Input IDs for sentence 1: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5268937349319458
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 1: 2.0305305569593437
Processing sentence 2
Input IDs for sentence 2: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.047316912561655045
Prior probability (p_prior): 0.0005563644226640463 for 279
Prior probability (p_prior) for sie: 1.5974484995240346e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005563644226640463 (target word token id=279)
Association score for sentence 2: 4.443199554021851
Processing sentence 3
Input IDs for sentence 3: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.06544014811515808
Prior probability (p_prior): 0.0005563644226640463 for 279
Prior probability (p_prior) for sie: 1.5974484995240346e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005563644226640463 (target word token id=279)
Association score for sentence 3: 4.767467718655457
Processing sentence 4
Input IDs for sentence 4: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.2660147547721863
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 4: 1.3470834465574368
Processing sentence 5
Input IDs for sentence 5: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.40446215867996216
Prior probability (p_prior): 0.00098233122844249 for 279
Prior probability (p_prior) for sie: 0.00011638984869932756 (target word token id=286)
Prior probability (p_prior) for er: 0.00098233122844249 (target word token id=279)
Association score for sentence 5: 6.020384908930264
Processing sentence 6
Input IDs for sentence 6: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.42306771874427795
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 6: 1.811063928072744
Processing sentence 7
Input IDs for sentence 7: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5267437696456909
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 7: 2.030245894944911
Processing sentence 8
Input IDs for sentence 8: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.13395045697689056
Prior probability (p_prior): 0.0005563644226640463 for 279
Prior probability (p_prior) for sie: 1.5974484995240346e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005563644226640463 (target word token id=279)
Association score for sentence 8: 5.483801770523151
Processing sentence 9
Input IDs for sentence 9: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6175867319107056
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 9: 2.1893511854203123
Processing sentence 10
Input IDs for sentence 10: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5085325837135315
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 10: 1.9950609617807946
Processing sentence 11
Input IDs for sentence 11: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.1847696751356125
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 11: 0.9826417204016374
Processing sentence 12
Input IDs for sentence 12: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.42306771874427795
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 12: 1.811063928072744
Processing sentence 13
Input IDs for sentence 13: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.07669150829315186
Prior probability (p_prior): 0.00098233122844249 for 279
Prior probability (p_prior) for sie: 0.00011638984869932756 (target word token id=286)
Prior probability (p_prior) for er: 0.00098233122844249 (target word token id=279)
Association score for sentence 13: 4.357617716647444
Processing sentence 14
Input IDs for sentence 14: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.14342941343784332
Prior probability (p_prior): 0.0005563644226640463 for 279
Prior probability (p_prior) for sie: 1.5974484995240346e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005563644226640463 (target word token id=279)
Association score for sentence 14: 5.552174784751786
Processing sentence 15
Input IDs for sentence 15: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.08750976622104645
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 15: 0.23528207133573134
Processing sentence 16
Input IDs for sentence 16: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.012781417928636074
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 16: -1.6884759379267231
Processing sentence 17
Input IDs for sentence 17: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.22934719920158386
Prior probability (p_prior): 0.00098233122844249 for 279
Prior probability (p_prior) for sie: 0.00011638984869932756 (target word token id=286)
Prior probability (p_prior) for er: 0.00098233122844249 (target word token id=279)
Association score for sentence 17: 5.453063736733187
Processing sentence 18
Input IDs for sentence 18: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.49159806966781616
Prior probability (p_prior): 0.06916315853595734 for 279
Prior probability (p_prior) for sie: 0.03353225812315941 (target word token id=286)
Prior probability (p_prior) for er: 0.06916315853595734 (target word token id=279)
Association score for sentence 18: 1.9611931212981089
Processing sentence 19
Input IDs for sentence 19: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.3050864040851593
Prior probability (p_prior): 0.00098233122844249 for 279
Prior probability (p_prior) for sie: 0.00011638984869932756 (target word token id=286)
Prior probability (p_prior) for er: 0.00098233122844249 (target word token id=279)
Association score for sentence 19: 5.738421756231147
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Dach', '##decker', '*', 'in']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##fach']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Kfz']
For TM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Fach', '##kraft', 'in', 'der']
For TM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '*']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Zimmer', '##eif', '##ach', '##kraft']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Installation', '##sf', '##ach', '##kraft']
For TM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Maurer', '*', 'in', '.']
For TM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Einsatz', '##kraft', 'der', 'Feuerwehr']
For TM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'über', 'de', 'ind'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', '##haft', 'hoch', '##er'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', 'de', '##haft', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', 'de', '##haft', '##e'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'über', 'de', 'ind'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', '##haft', 'hoch', '##er'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', 'de', '##haft', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', 'de', '##haft', '##e'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'nicht', 'nach', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['wenig', 'für', '.', 'in', 'nicht'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'wenig', 'd', 'gem', 'aus'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'über', 'de', 'ind'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', '##haft', 'hoch', '##er'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', 'de', '##haft', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', 'de', '##haft', '##e'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'nicht', 'nach', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['wenig', 'für', '.', 'in', 'nicht'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'wenig', 'd', 'gem', 'aus'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'Herkunft', 'Situation', 'Aufgabe', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'über', 'de', 'ind'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', '##haft', 'hoch', '##er'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', 'de', '##haft', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', 'de', '##haft', '##e'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'nicht', 'nach', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['wenig', 'für', '.', 'in', 'nicht'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'wenig', 'd', 'gem', 'aus'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Text', 'Satz', 'Name', 'Fall', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unklar', 'unbekannt', 'falsch', 'selten'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Mann', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'nicht', 'nach', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['wenig', 'für', '.', 'in', 'nicht'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'wenig', 'd', 'gem', 'aus'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0005402932874858379
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 0: -2.727818561532173
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.04913932457566261
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 1: 1.7824842224564164
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0015640329802408814
Prior probability (p_prior): 0.016547245904803276 for 960
Prior probability (p_prior) for sie: 4.059487309859833e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00030974243418313563 (target word token id=279)
Association score for sentence 2: -2.358951948378062
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0013837204314768314
Prior probability (p_prior): 0.016547245904803276 for 960
Prior probability (p_prior) for sie: 4.059487309859833e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00030974243418313563 (target word token id=279)
Association score for sentence 3: -2.4814438409767474
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00030222389614209533
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 4: -3.3087625579813333
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.00472312280908227
Prior probability (p_prior): 0.0926218032836914 for 960
Prior probability (p_prior) for sie: 1.7395263057551347e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.00019411204266361892 (target word token id=279)
Association score for sentence 5: -2.9760543776415376
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.09001599997282028
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 6: 2.3878120323623593
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.007545879576355219
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 7: -0.09117373683488264
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.0028535036835819483
Prior probability (p_prior): 0.016547245904803276 for 960
Prior probability (p_prior) for sie: 4.059487309859833e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00030974243418313563 (target word token id=279)
Association score for sentence 8: -1.7576720754960178
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.016879016533493996
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 9: 0.7138958255935891
Processing sentence 10
Input IDs for sentence 10: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 10: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.005916929338127375
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 10: -0.33435777819880785
Processing sentence 11
Input IDs for sentence 11: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 11: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.0012226663529872894
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 11: -1.9111413904705392
Processing sentence 12
Input IDs for sentence 12: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 12: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.09001599997282028
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 12: 2.3878120323623593
Processing sentence 13
Input IDs for sentence 13: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 13: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0030345686245709658
Prior probability (p_prior): 0.0926218032836914 for 960
Prior probability (p_prior) for sie: 1.7395263057551347e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.00019411204266361892 (target word token id=279)
Association score for sentence 13: -3.418455289738908
Processing sentence 14
Input IDs for sentence 14: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 14: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.005352634470909834
Prior probability (p_prior): 0.016547245904803276 for 960
Prior probability (p_prior) for sie: 4.059487309859833e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00030974243418313563 (target word token id=279)
Association score for sentence 14: -1.1286308132134886
Processing sentence 15
Input IDs for sentence 15: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 15: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.014389071613550186
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 15: 0.5542936030908072
Processing sentence 16
Input IDs for sentence 16: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 16: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.001005379599519074
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 16: -2.1068102184826816
Processing sentence 17
Input IDs for sentence 17: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 17: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.0004467331455089152
Prior probability (p_prior): 0.0926218032836914 for 960
Prior probability (p_prior) for sie: 1.7395263057551347e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.00019411204266361892 (target word token id=279)
Association score for sentence 17: -5.33431842311485
Processing sentence 18
Input IDs for sentence 18: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 18: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.08446839451789856
Prior probability (p_prior): 0.008266204036772251 for 960
Prior probability (p_prior) for sie: 1.0682729225663934e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.1496209481265396e-05 (target word token id=279)
Association score for sentence 18: 2.3242020353795696
Processing sentence 19
Input IDs for sentence 19: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 19: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.004778184462338686
Prior probability (p_prior): 0.0926218032836914 for 960
Prior probability (p_prior) for sie: 1.7395263057551347e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.00019411204266361892 (target word token id=279)
Association score for sentence 19: -2.964463915804694
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Trocken', '##baum', '##ont', '##age']
For TM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Stahl', '##arbeits', '##kraft', '.']
For TM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'mobile']
For TM: MASK at position 2: ['Herkunft', '##sprache', 'Leistung', 'Familie', 'Umfeld'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Mechan', '##ik', 'Fach', '##kraft']
For TM: MASK at position 2: ['Herkunft', '##sprache', 'Leistung', 'Familie', 'Umfeld'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Heizung']
For TM: MASK at position 2: ['Name', 'Leben', 'Vater', 'Körper', 'Bruder'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Elektro', '##fach', '##kraft', '.']
For TM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '*', 'in']
For TM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Fach', '##kraft', 'für', 'Holz']
For TM: MASK at position 2: ['Herkunft', '##sprache', 'Leistung', 'Familie', 'Umfeld'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Boden', '##leger', '*', 'in']
For TM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'gestorben', 'da', 'weg'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'gestorben', 'da', 'weg'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##sprache', 'Leistung', 'Familie', 'Umfeld'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', 'sozial', 'positiv'] (top-5 predictions)
For TAM: MASK at position 5: ['Herkunft', 'hoch', 'de', '##hafte', 'sozial'] (top-5 predictions)
For TAM: MASK at position 6: ['Herkunft', '##hafte', 'hoch', '##er', 'de'] (top-5 predictions)
For TAM: MASK at position 7: ['##hafte', '##er', 'Herkunft', 'de', 'hoch'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##sprache', 'Leistung', 'Familie', 'Umfeld'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', 'sozial', 'positiv'] (top-5 predictions)
For TAM: MASK at position 5: ['Herkunft', 'hoch', 'de', '##hafte', 'sozial'] (top-5 predictions)
For TAM: MASK at position 6: ['Herkunft', '##hafte', 'hoch', '##er', 'de'] (top-5 predictions)
For TAM: MASK at position 7: ['##hafte', '##er', 'Herkunft', 'de', 'hoch'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'gestorben', 'da', 'weg'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Leben', 'Vater', 'Körper', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['.', 'für', 'nur', 'nicht', 'eine'] (top-5 predictions)
For TAM: MASK at position 5: ['.', 'für', 'nicht', 'wenig', 'ohne'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '##haft', 'nicht', '##d', 'wenig'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'gestorben', 'da', 'weg'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'gestorben', 'da', 'weg'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##sprache', 'Leistung', 'Familie', 'Umfeld'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', 'sozial', 'positiv'] (top-5 predictions)
For TAM: MASK at position 5: ['Herkunft', 'hoch', 'de', '##hafte', 'sozial'] (top-5 predictions)
For TAM: MASK at position 6: ['Herkunft', '##hafte', 'hoch', '##er', 'de'] (top-5 predictions)
For TAM: MASK at position 7: ['##hafte', '##er', 'Herkunft', 'de', 'hoch'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Herz', 'Leben', 'Name', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'gestorben', 'da', 'weg'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.008337014354765415
Prior probability (p_prior): 0.03925635293126106 for 3726
Prior probability (p_prior) for sie: 1.8958806322189048e-05 (target word token id=286)
Prior probability (p_prior) for er: 4.1929539293050766e-05 (target word token id=279)
Association score for sentence 0: -1.5494081280108605
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.1568051129579544
Prior probability (p_prior): 0.03925635293126106 for 3726
Prior probability (p_prior) for sie: 1.8958806322189048e-05 (target word token id=286)
Prior probability (p_prior) for er: 4.1929539293050766e-05 (target word token id=279)
Association score for sentence 1: 1.3848904262827868
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.2671540677547455
Prior probability (p_prior): 0.0015163248172029853 for 3726
Prior probability (p_prior) for sie: 2.1981968529871665e-06 (target word token id=286)
Prior probability (p_prior) for er: 8.449648885289207e-05 (target word token id=279)
Association score for sentence 2: 5.1715360011315115
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 3: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.24733254313468933
Prior probability (p_prior): 0.0015163248172029853 for 3726
Prior probability (p_prior) for sie: 2.1981968529871665e-06 (target word token id=286)
Prior probability (p_prior) for er: 8.449648885289207e-05 (target word token id=279)
Association score for sentence 3: 5.0944442359823
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.17934605479240417
Prior probability (p_prior): 0.03925635293126106 for 3726
Prior probability (p_prior) for sie: 1.8958806322189048e-05 (target word token id=286)
Prior probability (p_prior) for er: 4.1929539293050766e-05 (target word token id=279)
Association score for sentence 4: 1.5192039172297356
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2093,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 5: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 3726
Target probability (p_T): 0.25865572690963745
Prior probability (p_prior): 0.030610842630267143 for 3726
Prior probability (p_prior) for sie: 3.308130908408202e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0002689687826205045 (target word token id=279)
Association score for sentence 5: 2.134143657226417
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.17867803573608398
Prior probability (p_prior): 0.03925635293126106 for 3726
Prior probability (p_prior) for sie: 1.8958806322189048e-05 (target word token id=286)
Prior probability (p_prior) for er: 4.1929539293050766e-05 (target word token id=279)
Association score for sentence 6: 1.5154722139773187
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.15690894424915314
Prior probability (p_prior): 0.03925635293126106 for 3726
Prior probability (p_prior) for sie: 1.8958806322189048e-05 (target word token id=286)
Prior probability (p_prior) for er: 4.1929539293050766e-05 (target word token id=279)
Association score for sentence 7: 1.3855523749115124
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 8: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.22808215022087097
Prior probability (p_prior): 0.0015163248172029853 for 3726
Prior probability (p_prior) for sie: 2.1981968529871665e-06 (target word token id=286)
Prior probability (p_prior) for er: 8.449648885289207e-05 (target word token id=279)
Association score for sentence 8: 5.013416348406739
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.12568777799606323
Prior probability (p_prior): 0.03925635293126106 for 3726
Prior probability (p_prior) for sie: 1.8958806322189048e-05 (target word token id=286)
Prior probability (p_prior) for er: 4.1929539293050766e-05 (target word token id=279)
Association score for sentence 9: 1.1636875900819048
Batch 2: Associations calculated.
Evaluation completed.
Results saved to ../data/output_csv_files/german/results_DE_baseline_perplexity_gender_neutral_sample_test.csv
