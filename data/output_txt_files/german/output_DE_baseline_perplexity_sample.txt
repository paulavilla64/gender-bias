-- Prepare evaluation data --
Loaded first 50 rows of the dataset:
    Unnamed: 0  ...              Profession_EN
0            0  ...                      taper
1            1  ...               steel worker
2            2  ...  mobile equipment mechanic
3            3  ...               bus mechanic
4            4  ...         service technician
5            5  ...           heating mechanic
6            6  ...       electrical installer
7            7  ...         operating engineer
8            8  ...             logging worker
9            9  ...            floor installer
10          10  ...                     roofer
11          11  ...    mining machine operator
12          12  ...                electrician
13          13  ...                   repairer
14          14  ...                  conductor
15          15  ...                    plumber
16          16  ...                  carpenter
17          17  ...  security system installer
18          18  ...                      mason
19          19  ...                firefighter
20          20  ...                      taper
21          21  ...               steel worker
22          22  ...  mobile equipment mechanic
23          23  ...               bus mechanic
24          24  ...         service technician
25          25  ...           heating mechanic
26          26  ...       electrical installer
27          27  ...         operating engineer
28          28  ...             logging worker
29          29  ...            floor installer
30          30  ...                     roofer
31          31  ...    mining machine operator
32          32  ...                electrician
33          33  ...                   repairer
34          34  ...                  conductor
35          35  ...                    plumber
36          36  ...                  carpenter
37          37  ...  security system installer
38          38  ...                      mason
39          39  ...                firefighter
40          40  ...                      taper
41          41  ...               steel worker
42          42  ...  mobile equipment mechanic
43          43  ...               bus mechanic
44          44  ...         service technician
45          45  ...           heating mechanic
46          46  ...       electrical installer
47          47  ...         operating engineer
48          48  ...             logging worker
49          49  ...            floor installer

[50 rows x 11 columns]
No GPU available, using the CPU instead.
-- Import BERT model --
loading german bert
Tokenizer: BertTokenizerFast(name_or_path='bert-base-german-dbmdz-cased', vocab_size=31102, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)
Model loaded: bert-base-german-dbmdz-cased
-- Calculate associations before fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0               [MASK] ist Trockenbaumonteur.
1                   [MASK] ist Stahlarbeiter.
2    [MASK] ist Mechaniker für mobile Geräte.
3                   [MASK] ist Busmechaniker.
4            [MASK] ist Kfz-Servicetechniker.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 11861, 6717, 6089, 138, 566, 103], [102, 104, 215, 4471, 1978, 566, 103], [102, 104, 215, 11016, 2335, 231, 22615, 7612, 566, 103], [102, 104, 215, 3203, 12064, 2335, 566, 103], [102, 104, 215, 25867, 232, 5502, 6190, 105, 566, 103]]
Padded input IDs (first 5): [[  102   104   215 11861  6717  6089   138   566]
 [  102   104   215  4471  1978   566   103     0]
 [  102   104   215 11016  2335   231 22615  7612]
 [  102   104   215  3203 12064  2335   566   103]
 [  102   104   215 25867   232  5502  6190   105]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                         [MASK] ist [MASK].
1                         [MASK] ist [MASK].
2    [MASK] ist [MASK] [MASK] [MASK] [MASK].
3                         [MASK] ist [MASK].
4                         [MASK] ist [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103]]
Padded input IDs (first 5): [[102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0               Er ist Trockenbaumonteur.
1                   Er ist Stahlarbeiter.
2    Er ist Mechaniker für mobile Geräte.
3                   Er ist Busmechaniker.
4            Er ist Kfz-Servicetechniker.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 279, 215, 11861, 6717, 6089, 138, 566, 103], [102, 279, 215, 4471, 1978, 566, 103], [102, 279, 215, 11016, 2335, 231, 22615, 7612, 566, 103], [102, 279, 215, 3203, 12064, 2335, 566, 103], [102, 279, 215, 25867, 232, 5502, 6190, 105, 566, 103]]
Padded input IDs (first 5): [[  102   279   215 11861  6717  6089   138   566]
 [  102   279   215  4471  1978   566   103     0]
 [  102   279   215 11016  2335   231 22615  7612]
 [  102   279   215  3203 12064  2335   566   103]
 [  102   279   215 25867   232  5502  6190   105]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', 'Trocken', '##baum', '##onte', '##ur', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', 'Stahl', '##arbeiter', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', 'Mechan', '##iker', 'für', 'mobile', 'Geräte']
For TM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', 'Bus', '##mechan', '##iker', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik', '##er']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', 'Heizung', '##sm', '##echan', '##iker', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##install', '##ateur', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', 'Holz', '##fälle', '##r', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', 'Boden', '##leger', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', 'Dach', '##ede', '##cker', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##er', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', 'Elekt', '##rike', '##r', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Mechan', '##iker', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', 'Schaff', '##ner', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', 'Zimmermann', '.', '[SEP]', '[PAD]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', 'Install', '##ateur', 'von', 'Sicherheits', '##systemen']
For TM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', 'Maurer', '.', '[SEP]', '[PAD]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', 'Feuerwehr', '##mann', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'in', 'und', 'über', 'mit'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'und', '##e', 'in', '##er'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'und', '##e', '##er', 'in'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'und', '##e', '##er', '[UNK]'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['“', '–', '[UNK]', '„', ','] (top-5 predictions)
For TAM: MASK at position 3: [',', 'in', 'die', '“', '–'] (top-5 predictions)
For TAM: MASK at position 4: [',', '“', '–', '[UNK]', 'in'] (top-5 predictions)
For TAM: MASK at position 5: ['“', '[UNK]', '–', ',', '1'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', '“', 'Das'] (top-5 predictions)
For TAM: MASK at position 3: ['es', 'er', '“', 'das', 'ein'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6548355221748352
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 0: 2.7348544118613805
Processing sentence 1
Input IDs for sentence 1: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.5799470543861389
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 1: 2.613407132775825
Processing sentence 2
Input IDs for sentence 2: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.40485215187072754
Prior probability (p_prior): 0.0005555599927902222 for 279
Prior probability (p_prior) for sie: 3.585729791666381e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0005555599927902222 (target word token id=279)
Association score for sentence 2: 6.591300621259272
Processing sentence 3
Input IDs for sentence 3: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.7367945909500122
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 3: 2.852779462584372
Processing sentence 4
Input IDs for sentence 4: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.12144947797060013
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 4: 1.049968676075101
Processing sentence 5
Input IDs for sentence 5: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6549922823905945
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 5: 2.7350937718993356
Processing sentence 6
Input IDs for sentence 6: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.734160304069519
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 6: 2.849197721627317
Processing sentence 7
Input IDs for sentence 7: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.7177786231040955
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 7: 2.8266315160822173
Processing sentence 8
Input IDs for sentence 8: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.555702805519104
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 8: 2.5707039478381555
Processing sentence 9
Input IDs for sentence 9: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6781193614006042
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 9: 2.769793640652249
Processing sentence 10
Input IDs for sentence 10: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.2633265554904938
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 10: 1.8238652367612969
Processing sentence 11
Input IDs for sentence 11: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6253470778465271
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 11: 2.688777139098504
Processing sentence 12
Input IDs for sentence 12: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6403006315231323
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 12: 2.7124081217601184
Processing sentence 13
Input IDs for sentence 13: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.519420325756073
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 13: 2.5031837505523233
Processing sentence 14
Input IDs for sentence 14: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.48469826579093933
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 14: 2.4339968838862878
Processing sentence 15
Input IDs for sentence 15: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6374427080154419
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 15: 2.70793472215763
Processing sentence 16
Input IDs for sentence 16: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.534690260887146
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 16: 2.5321579465692756
Processing sentence 17
Input IDs for sentence 17: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.6930078268051147
Prior probability (p_prior): 0.00035441239015199244 for 279
Prior probability (p_prior) for sie: 0.0002659999008756131 (target word token id=286)
Prior probability (p_prior) for er: 0.00035441239015199244 (target word token id=279)
Association score for sentence 17: 7.578335393154044
Processing sentence 18
Input IDs for sentence 18: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6388275027275085
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 18: 2.7101047880948403
Processing sentence 19
Input IDs for sentence 19: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.7099321484565735
Prior probability (p_prior): 0.04250108823180199 for 279
Prior probability (p_prior) for sie: 0.025895683094859123 (target word token id=286)
Prior probability (p_prior) for er: 0.04250108823180199 (target word token id=279)
Association score for sentence 19: 2.815639718857839
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Trocken', '##baum', '##onte', '##ur']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Stahl', '##arbeiter', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Mechan', '##iker', 'für', 'mobile']
For TM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bus', '##mechan', '##iker', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Heizung', '##sm', '##echan', '##iker']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##install', '##ateur', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Holz', '##fälle', '##r', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Boden', '##leger', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Dach', '##ede', '##cker', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##er']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elekt', '##rike', '##r', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Mechan', '##iker']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Schaff', '##ner', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '.']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Zimmermann', '.', '[SEP]', '[PAD]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Install', '##ateur', 'von', 'Sicherheits']
For TM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Maurer', '.', '[SEP]', '[PAD]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Feuerwehr', '##mann', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['durch', 'Art', 'Aufgabe', 'und', 'Form'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'in', '##e', 'über', 'und'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', '##e', '##er', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', '##er', '##e', 'und', 'Art'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', '##er', '##e', 'und', '##r'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Begriff', 'Name', 'Raum', 'Mann', 'Artikel'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'als', 'ohne', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['für', 'in', 'mit', 'ohne', 'im'] (top-5 predictions)
For TAM: MASK at position 6: ['.', '[UNK]', '"', 'und', 'mit'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Artikel', 'Prozess', 'Begriff', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'selten', 'unklar', 'einzigartig'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.045538634061813354
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 0: 3.2219855094127703
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.20907290279865265
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 1: 4.74610745183859
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.013181670568883419
Prior probability (p_prior): 0.005073260050266981 for 960
Prior probability (p_prior) for sie: 1.8620286937220953e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0008838657522574067 (target word token id=279)
Association score for sentence 2: 0.9548436524325995
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 3: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.1042265072464943
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 3: 4.049990928608145
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00037170766154304147
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 4: -1.586223146244904
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 5: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.008569994941353798
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 5: 1.5516915858136726
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.044713061302900314
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 6: 3.2036901015856167
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.10029816627502441
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 7: 4.011571855876144
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 8: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.3062897324562073
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 8: 5.127955935554049
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.03945038840174675
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 9: 3.0784683361395135
Processing sentence 10
Input IDs for sentence 10: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 10: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.018313445150852203
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 10: 2.3110599412565693
Processing sentence 11
Input IDs for sentence 11: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 11: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.03424545377492905
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 11: 2.936978262877471
Processing sentence 12
Input IDs for sentence 12: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 12: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.22651872038841248
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 12: 4.8262520357071015
Processing sentence 13
Input IDs for sentence 13: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 13: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.006762351840734482
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 13: 1.3147951784618326
Processing sentence 14
Input IDs for sentence 14: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 14: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.18183913826942444
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 14: 4.606546884061495
Processing sentence 15
Input IDs for sentence 15: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 15: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.19813476502895355
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 15: 4.692371874107282
Processing sentence 16
Input IDs for sentence 16: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 16: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.17652854323387146
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 16: 4.576907024815259
Processing sentence 17
Input IDs for sentence 17: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 17: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.040175460278987885
Prior probability (p_prior): 0.019393565133213997 for 960
Prior probability (p_prior) for sie: 3.8969246816122904e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0006882004672661424 (target word token id=279)
Association score for sentence 17: 0.7283150515265424
Processing sentence 18
Input IDs for sentence 18: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 18: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.1527394950389862
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 18: 4.43215826691585
Processing sentence 19
Input IDs for sentence 19: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 19: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.2592661678791046
Prior probability (p_prior): 0.0018158897291868925 for 960
Prior probability (p_prior) for sie: 1.9757921108976007e-05 (target word token id=286)
Prior probability (p_prior) for er: 9.196378414344508e-06 (target word token id=279)
Association score for sentence 19: 4.961279652689699
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Trocken', '##baum', '##onte', '##ur']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Stahl', '##arbeiter', '.', '[SEP]']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Mechan', '##iker', 'für', 'mobile']
For TM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Bus', '##mechan', '##iker', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Heizung', '##sm', '##echan', '##iker']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Elektro', '##install', '##ateur', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '.', '[SEP]']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Holz', '##fälle', '##r', '.']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Boden', '##leger', '.', '[SEP]']
For TM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Aufgabe', 'Herkunft', '##r', 'System', 'Leistung'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'eine', '##e', 'hoch', 'mit'] (top-5 predictions)
For TAM: MASK at position 5: ['##e', 'durch', '##er', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 6: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)
For TAM: MASK at position 7: ['##e', '##er', 'durch', 'und', '##r'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Herz', 'Name', 'Vater', 'Buch'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'gestorben', 'frei', 'krank', 'groß'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.12764237821102142
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 0: 1.9949377709840084
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.0641258955001831
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 1: 1.3065536058795022
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.1458219587802887
Prior probability (p_prior): 0.0007472448633052409 for 3726
Prior probability (p_prior) for sie: 1.243902624992188e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0001826712250476703 (target word token id=279)
Association score for sentence 2: 5.273748769125688
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 3: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.1287897229194641
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 3: 2.003886357059774
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.09969065338373184
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 4: 1.7477772627392747
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 5: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.11065343022346497
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 5: 1.852108404251429
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.126480832695961
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 6: 1.9857961140681406
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.06207655742764473
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 7: 1.274073758101016
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 8: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.1318855732679367
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 8: 2.0276400150266434
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.09955447912216187
Prior probability (p_prior): 0.017362186685204506 for 3726
Prior probability (p_prior) for sie: 2.512822175049223e-05 (target word token id=286)
Prior probability (p_prior) for er: 3.5376233427086845e-05 (target word token id=279)
Association score for sentence 9: 1.7464103607621986
Batch 2: Associations calculated.
Evaluation completed.
-- Import fine-tuning data --
Loaded first 50 rows of the finetuning dataset:
Loaded first 50 rows of the validation dataset:
['2024 auf Seite zwei auf den Punkt gebracht.', '»27.08.24« stand in der Nachricht in goldener Schrift auf schwarzem Grund – in der Typo des Oasis-Bandlogos.', 'A23 – Unterirdische Bohrungen zur Stromkabelverlegung im Bereich Anschlussstelle Halstenbek-Krupunder, 2.', 'Dezember bis voraussichtlich 23.', 'Dezember 2024: Um die Kapazität des Stromnetzes im Kreis Pinneberg zu erweitern, werden neue Stromkabel verlegt.', 'Aachen hat sehr gut gespielt, und vor allem Nicole van de Vosse hat uns einige Probleme bereitet“, sagte Trainer Alexander Waibl über die überragenden Holländerin (29 Punkte).', 'Ab 10:30 Uhr gibt es an den Sonntagen Frühschoppen und Familienprogramm, Abendveranstaltungen mit Schlagergrößen wie Mickie Krause, Lorenz Büffel, Almklausi und Schürze stehen täglich von 17 Uhr an auf dem Programm.', 'Ab 14 Uhr gibt es im Garten Kaffeetrinken.', 'Ab 15.', 'November umgibt sie der Wiener Christkindlmarkt.', 'Ab 17 Uhr begrüßt Moderator und Kommentator Markus Götz die Zuschauer gemeinsam mit Sky Experte Mladen Petric und Raphael Honigstein zur Partie zwischen Brighton & Hove Albion und Tottenham Hotspur.', 'Ab 180 Grad Celsius kann sich Fett selbst entzünden.', 'Ab 2014 bot sich bei ihr dann die Chance, hauptberuflich im politischen Umfeld zu arbeiten.', 'Ab 2025 wird der Post- und Paketversand teurer – aber nicht teuer genug.', 'Ab 2026 nimmt die Deutsche Bahn eine Generalsanierung von 4000 Streckenkilometern in Bayern vor.', 'Ab 2026 will die Formel 1 mit komplett klimaneutralen Sprit fahren.', 'Ab 2027 will der Bundesrat das Budget um 3,6 Milliarden Franken entlasten.', 'Ab 2035 dürfen keine neuen Verbrenner mehr in der EU zugelassen werden - bislang.', 'Ab 20 Uhr legt DJ Kribz auf, es wird Musik aus den 80ern, den 90ern und von heute gespielt.', 'Ab 29.', 'April bis Mitte September zwischen der Hardstrasse in Basel und Pratteln.', 'Ab 30 empfiehlt die Gynäkologin einen HPV-Test – wer mit den Humanen Papillomaviren infiziert ist, sollte regelmäßig zur Vorsorgeuntersuchung gehen.', 'Abarth: Der vollelektrische Abarth 600e startet im Jänner.', 'Abdelmadjid Tebboune bleibt wohl Präsident von Algerien.', 'Ab dem 22.', 'April ist der Oberhausener Stadtteil Styrum um eine Baustelle reicher.', 'Ab dem 26.', 'März kann man in Kutná Hora / Kuttenberg ihren Charme bewundern.', 'Ab dem 3.', 'Mai beginnt dann das klassische Volksfest mit großen -Acts, Bühnen und einem großen Markt.', 'Ab dem frühen Nachmittag bekommen Sie wieder Aufwind.', 'Ab dem heutigen Dienstag wird zusätzlich für mehr Lohn gestreikt.', 'Ab dem kommenden Schuljahr werden 376 Kinder unterrichtet, und rund 240 wollen die Betreuung wahrnehmen.', 'Ab dem kommenden Sommer wird er Björn Klos als Trainer des SV Merchweiler ablösen.', 'Ab der kommenden Woche wird saniert.', 'Ab diesem Zeitpunkt wird die etwa 20 Jahre alte Schieneninfrastruktur im Bereich Herdentor und Schüsselkorb erneuert.', 'Ab Donnerstag wird es dann grau und nass – und es kann wieder schneien.', 'Ab dort führt er in den Wald, und es ist gutes Schuhwerk erforderlich.', 'Ab einem Core i5-8400 oder Ryzen 5 1600 sowie einer GeForce GTX 1070 oder RX Vega 56 soll der Spaß losgehen.', 'Ab einer Beteiligung von 30 Prozent wäre Unicredit verpflichtet, den übrigen Aktionären der Commerzbank eine öffentliche Übernahmeofferte vorzulegen.', 'Ab Ende April schlüpfen die Larven des Eichenprozessionssspinners.', 'Abenteuer an der Börse: Wie hoch kann Nvidia noch steigen?', 'Aber, Alexis hat sich schon was überlegt, kommt schon mit passenden Kleidern um die Ecke.', 'Aber alleine der Versuch, das sowjetische Imperium in Osteuropa wieder zu errichten, könnte für viel Schaden sorgen.', 'Aber alles existiert in Kontinuität.', 'Aber als Erwachsener an eine fiktive Figur aus einem Märchenbuch zu glauben ist“normal“?', 'Aber: „Als ich nach Leipzig kam, gab es keine DDR mehr.“', 'Aber als sie sofortige Friedensverhandlungen mit Putin verlangt, wird sie von Strack-Zimmermann mit einem Drei-Wort-Satz in die Knie gezwungen.', 'Aber: «Als Staat, der keinem Bündnis angehört und eher kleine Stückzahlen beschafft, geniesst die Schweiz keine Priorität bei Lieferanten im Ausland.»', '"Aber als Vater habe ich grenzenlose Liebe zu meinem Sohn, Vertrauen in ihn und Respekt für seine Stärke."']
Max sentence length in training set: 64
Max sentence length in validation set: 32
Input sequence (first 5): ['Karl Telford – spielte Darren, den Freund von Estelle, der Polizist ist.', 'Er wurde in der letzten Folge der ersten Staffel von Estelle abserviert, nachdem sie mit Denver geschlafen hatte, und taucht seitdem nicht mehr auf.', 'Irwin Susan spielte Lawrence Odell, Jefferys Freund und ebenfalls ein Schüler der 11.', 'Klasse in Estelles Klasse.', 'Er verließ seine Freundin auf Estelles Rat hin, nachdem sie keinen Sex mit ihm haben wollte, merkte aber später, dass dies daran lag, dass sie sich bei seinem Freund Jeffery Filzläuse eingefangen hatte.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 3533, 2723, 6365, 809, 2985, 1472, 786, 818, 190, 2557, 195, 8006, 2033, 818, 125, 19377, 215, 566, 103], [102, 279, 325, 153, 125, 1761, 2900, 125, 940, 7482, 195, 8006, 2033, 28220, 2532, 426, 818, 3927, 307, 212, 2025, 289, 26774, 638, 818, 136, 21243, 9957, 255, 484, 216, 566, 103], [102, 3069, 5179, 28894, 2985, 28516, 12795, 630, 818, 14888, 105, 1296, 2557, 136, 1966, 143, 3248, 125, 1111, 566, 103], [102, 5263, 153, 8006, 12832, 5263, 566, 103], [102, 279, 7240, 697, 8645, 216, 8006, 12832, 1006, 631, 818, 3927, 307, 2450, 6913, 212, 1020, 450, 2164, 818, 23567, 30881, 494, 1243, 818, 377, 513, 3373, 3768, 818, 377, 307, 251, 282, 1182, 2557, 14888, 105, 30933, 3294, 12246, 17540, 1194, 7294, 638, 566, 103]]
Padded input IDs (first 5): [[  102  3533  2723  6365   809  2985  1472   786   818   190  2557   195
   8006  2033   818   125 19377   215   566   103     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102   279   325   153   125  1761  2900   125   940  7482   195  8006
   2033 28220  2532   426   818  3927   307   212  2025   289 26774   638
    818   136 21243  9957   255   484   216   566   103     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102  3069  5179 28894  2985 28516 12795   630   818 14888   105  1296
   2557   136  1966   143  3248   125  1111   566   103     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102  5263   153  8006 12832  5263   566   103     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]
 [  102   279  7240   697  8645   216  8006 12832  1006   631   818  3927
    307  2450  6913   212  1020   450  2164   818 23567 30881   494  1243
    818   377   513  3373  3768   818   377   307   251   282  1182  2557
  14888   105 30933  3294 12246 17540  1194  7294   638   566   103     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
Input sequence (first 5): ['2024 auf Seite zwei auf den Punkt gebracht.', '»27.08.24« stand in der Nachricht in goldener Schrift auf schwarzem Grund – in der Typo des Oasis-Bandlogos.', 'A23 – Unterirdische Bohrungen zur Stromkabelverlegung im Bereich Anschlussstelle Halstenbek-Krupunder, 2.', 'Dezember bis voraussichtlich 23.', 'Dezember 2024: Um die Kapazität des Stromnetzes im Kreis Pinneberg zu erweitern, werden neue Stromkabel verlegt.']
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 27079, 30943, 216, 1640, 510, 216, 190, 4115, 3981, 566, 103], [102, 1665, 2533, 566, 3167, 566, 1955, 2529, 2680, 153, 125, 8132, 153, 23766, 30884, 4000, 216, 12890, 30895, 926, 809, 153, 125, 3763, 30892, 222, 257, 27106, 232, 2342, 14127, 317, 566, 103], [102, 131, 2485, 809, 633, 9238, 366, 15548, 271, 356, 4621, 26863, 3565, 314, 132, 223, 1373, 5450, 2138, 11001, 155, 3366, 232, 20611, 5361, 2411, 818, 197, 566, 103], [102, 1466, 378, 14994, 1910, 566, 103], [102, 1466, 27079, 30943, 853, 607, 128, 13826, 222, 4621, 26568, 223, 2235, 12347, 382, 851, 205, 16573, 818, 338, 1133, 4621, 26863, 10053, 566, 103]]
Padded input IDs (first 5): [[  102 27079 30943   216  1640   510   216   190  4115  3981   566   103
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  102  1665  2533   566  3167   566  1955  2529  2680   153   125  8132
    153 23766 30884  4000   216 12890 30895   926   809   153   125  3763
  30892   222   257 27106   232  2342 14127   317]
 [  102   131  2485   809   633  9238   366 15548   271   356  4621 26863
   3565   314   132   223  1373  5450  2138 11001   155  3366   232 20611
   5361  2411   818   197   566   103     0     0]
 [  102  1466   378 14994  1910   566   103     0     0     0     0     0
      0     0     0     0     0     0     0     0     0     0     0     0
      0     0     0     0     0     0     0     0]
 [  102  1466 27079 30943   853   607   128 13826   222  4621 26568   223
   2235 12347   382   851   205 16573   818   338  1133  4621 26863 10053
    566   103     0     0     0     0     0     0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0]])
val_tokens shape before masking: torch.Size([50, 32])
Total masked tokens per sequence: tensor([2, 3, 8, 2, 3, 3, 5, 1, 1, 1, 6, 2, 4, 2, 1, 1, 1, 2, 6, 1, 3, 3, 5, 1,
        3, 1, 1, 1, 1, 6, 2, 3, 2, 3, 5, 1, 1, 2, 3, 4, 3, 1, 1, 5, 1, 3, 1, 4,
        4, 3])
Total tokens in batch: 1600
Total masked tokens: 132
Percentage of masked tokens: 8.25%
Total special tokens in batch: 714
Total tokens: 1600, Non-special tokens: 886, Masked tokens: 132
Unique label values and counts: {-100: 1468, 0: 3, 110: 1, 125: 2, 128: 3, 136: 1, 153: 2, 180: 1, 190: 2, 195: 1, 197: 1, 199: 1, 205: 1, 208: 1, 231: 2, 232: 3, 249: 1, 251: 1, 255: 1, 260: 1, 272: 1, 276: 2, 286: 1, 307: 1, 310: 1, 314: 1, 348: 1, 371: 1, 382: 1, 394: 1, 425: 6, 429: 1, 484: 1, 530: 1, 560: 1, 566: 9, 573: 1, 669: 1, 731: 1, 818: 3, 853: 1, 877: 1, 899: 1, 994: 1, 1147: 1, 1373: 1, 1389: 1, 1396: 1, 1412: 1, 1466: 1, 1483: 1, 1620: 1, 1793: 1, 2252: 1, 2485: 1, 2498: 1, 2813: 1, 3496: 1, 3572: 1, 4076: 1, 4115: 1, 4156: 1, 4621: 1, 4883: 1, 5031: 1, 5096: 1, 5324: 1, 5361: 1, 5450: 1, 5756: 1, 5848: 1, 5907: 1, 6034: 1, 6052: 1, 6275: 1, 6325: 1, 6335: 1, 6811: 1, 7398: 1, 8901: 1, 10078: 1, 10376: 1, 10840: 1, 11001: 1, 11710: 1, 11995: 1, 12460: 1, 13221: 1, 13381: 1, 13410: 1, 13556: 1, 14309: 1, 16313: 1, 17030: 2, 17737: 1, 18006: 1, 19055: 1, 20367: 1, 21402: 1, 27055: 1, 27079: 2, 28235: 1, 30882: 1, 30886: 1, 30892: 1}
Dataset size: 50
Sample 0 labels shape: torch.Size([32])
Sample 1 labels shape: torch.Size([32])
Sample 0 labels shape: torch.Size([32])
Sample 1 labels shape: torch.Size([32])
Sample 2 labels shape: torch.Size([32])
Sample 3 labels shape: torch.Size([32])
Sample 4 labels shape: torch.Size([32])
🔎 Checking dataset sample shapes before batching...
Sample 0 - input_ids shape: torch.Size([32])
Sample 0 - labels shape: torch.Size([32])
Sample 1 - input_ids shape: torch.Size([32])
Sample 1 - labels shape: torch.Size([32])
Sample 2 - input_ids shape: torch.Size([32])
Sample 2 - labels shape: torch.Size([32])
Sample 3 - input_ids shape: torch.Size([32])
Sample 3 - labels shape: torch.Size([32])
Sample 4 - input_ids shape: torch.Size([32])
Sample 4 - labels shape: torch.Size([32])
-- Set up model fine-tuning --

Calculating baseline perplexity before fine-tuning...
Baseline Loss: 2.71, Perplexity: 14.97

======== Epoch 1 / 3 ========
Training...
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 6
Unique label values and counts: {-100: 58, 261: 1, 693: 1, 2087: 1, 6461: 1, 14489: 1, 30894: 1}
Train Loss: 1.4569567441940308
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 3
Unique label values and counts: {-100: 61, 10705: 1, 12716: 1, 13336: 1}
Train Loss: 2.3608598709106445
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 0.08174920827150345
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 2004: 1, 3749: 1, 7057: 1, 19954: 1}
Train Loss: 1.4726555347442627
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 5
Unique label values and counts: {-100: 59, 255: 1, 282: 1, 2736: 1, 6734: 1, 25855: 1}
Train Loss: 2.8396503925323486
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 13
Unique label values and counts: {-100: 51, 818: 2, 1256: 1, 2805: 1, 2900: 1, 5041: 1, 5988: 1, 6240: 1, 8588: 1, 10012: 1, 17131: 1, 22494: 1, 30881: 1}
Train Loss: 3.6817049980163574
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 2
Unique label values and counts: {-100: 62, 205: 1, 14585: 1}
Train Loss: 2.5422677993774414
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 630: 1}
Train Loss: 1.7488815784454346
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 180: 1, 818: 1, 4186: 1, 18286: 1}
Train Loss: 1.6503499746322632
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 17.688217163085938
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 15.306985855102539
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 6
Unique label values and counts: {-100: 58, 282: 1, 450: 1, 818: 1, 3927: 1, 12246: 1, 23567: 1}
Train Loss: 1.3155109882354736
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 17.051040649414062
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 62, 195: 1, 2985: 1}
Train Loss: 3.9797940254211426
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 5
Unique label values and counts: {-100: 59, 167: 1, 818: 1, 850: 1, 6473: 1, 6517: 1}
Train Loss: 0.7772972583770752
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 4
Unique label values and counts: {-100: 60, 232: 1, 333: 1, 369: 1, 19448: 1}
Train Loss: 0.7460715770721436
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 279: 1, 4034: 1, 5957: 1}
Train Loss: 1.308510661125183
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 462: 1, 8131: 1}
Train Loss: 1.3130685091018677
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 9
Unique label values and counts: {-100: 55, 201: 1, 408: 1, 702: 1, 818: 1, 1802: 1, 2505: 1, 4268: 1, 6715: 1, 27535: 1}
Train Loss: 2.008770704269409
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 4
Unique label values and counts: {-100: 60, 123: 1, 386: 1, 14616: 1, 30853: 1}
Train Loss: 2.229302167892456
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 9
Unique label values and counts: {-100: 55, 180: 1, 916: 1, 1166: 1, 10251: 1, 11443: 1, 14480: 1, 17265: 1, 30881: 1, 31018: 1}
Train Loss: 3.4241554737091064
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 8
Unique label values and counts: {-100: 56, 208: 1, 818: 1, 1384: 1, 3202: 1, 4920: 1, 8227: 2, 14710: 1}
Train Loss: 1.6833221912384033
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 9
Unique label values and counts: {-100: 55, 285: 1, 343: 1, 566: 2, 818: 1, 2162: 1, 2234: 1, 3616: 1, 12676: 1}
Train Loss: 2.3714911937713623
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 3
Unique label values and counts: {-100: 61, 251: 1, 282: 1, 18120: 1}
Train Loss: 0.2648140788078308
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 4
Unique label values and counts: {-100: 60, 242: 1, 818: 1, 4268: 1, 26344: 1}
Train Loss: 2.6623687744140625
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 3
Unique label values and counts: {-100: 61, 424: 1, 5787: 1, 30933: 1}
Train Loss: 4.000293254852295
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 195: 1, 5774: 1}
Train Loss: 0.0025910332333296537
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 215: 1, 760: 1, 818: 1, 853: 1, 1761: 1, 5718: 1, 11336: 1}
Train Loss: 2.0070507526397705
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 462: 1}
Train Loss: 0.048906370997428894
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 106: 1, 434: 1, 472: 1, 1230: 1, 2572: 1, 3432: 1, 4823: 1, 22075: 1, 22958: 1, 30882: 1, 30887: 1}
Train Loss: 3.814145803451538
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 3
Unique label values and counts: {-100: 61, 2672: 1, 14491: 1, 21303: 1}
Train Loss: 7.624243259429932
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 628: 1}
Train Loss: 0.19865719974040985
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 6
Unique label values and counts: {-100: 58, 153: 1, 763: 1, 8936: 1, 13072: 1, 19798: 1, 30881: 1}
Train Loss: 0.697439432144165
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 6
Unique label values and counts: {-100: 58, 195: 1, 328: 1, 351: 1, 818: 1, 5871: 1, 30882: 1}
Train Loss: 0.607113778591156
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 10
Unique label values and counts: {-100: 54, 195: 2, 215: 1, 8760: 1, 8861: 1, 9176: 1, 9566: 1, 14037: 1, 16376: 1, 17190: 1}
Train Loss: 2.1862504482269287
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 23234: 1}
Train Loss: 1.8664703369140625
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 1
Unique label values and counts: {-100: 63, 7556: 1}
Train Loss: 0.00016473367577418685
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 4
Unique label values and counts: {-100: 60, 143: 1, 279: 1, 818: 1, 4717: 1}
Train Loss: 0.5228610038757324
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 1178: 1, 4643: 1}
Train Loss: 8.232213973999023
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 8
Unique label values and counts: {-100: 56, 566: 1, 818: 1, 996: 1, 1257: 1, 1860: 1, 3937: 1, 6485: 1, 21362: 1}
Train Loss: 2.1288681030273438
  Batch    40  of     50.    Elapsed: 0:00:16.
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 2964: 1, 6681: 1}
Train Loss: 0.2524294853210449
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 7
Unique label values and counts: {-100: 57, 136: 1, 279: 1, 484: 1, 818: 1, 1761: 1, 2900: 1, 26774: 1}
Train Loss: 1.718865156173706
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 566: 1, 3978: 1}
Train Loss: 2.3716979026794434
Total masked tokens per sequence: tensor([9])
Total tokens in batch: 64
Total masked tokens: 9
Percentage of masked tokens: 14.06%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 9
Unique label values and counts: {-100: 55, 113: 1, 183: 1, 251: 1, 481: 1, 731: 1, 2008: 1, 3862: 1, 3953: 1, 9176: 1}
Train Loss: 0.9420356750488281
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 8
Unique label values and counts: {-100: 56, 136: 1, 199: 1, 290: 1, 484: 1, 4867: 1, 18301: 1, 24641: 1, 27917: 1}
Train Loss: 2.908318042755127
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 16438: 1, 26674: 1}
Train Loss: 6.282038688659668
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 195: 1, 358: 1, 14080: 1}
Train Loss: 0.945784866809845
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 8006: 1}
Train Loss: 3.7626328468322754
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 6
Unique label values and counts: {-100: 58, 307: 1, 717: 1, 1078: 1, 5995: 1, 10345: 1, 17554: 1}
Train Loss: 2.296224355697632
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 2
Unique label values and counts: {-100: 62, 333: 1, 7194: 1}
Train Loss: 0.08784785121679306


[Epoch 1] Average training loss: 2.95

[Epoch 1] Training Perplexity: 19.09
[Epoch 1] Training epoch took: 0:00:20

Running Validation...
Batch input_ids shape: torch.Size([50, 32])
Batch labels shape: torch.Size([50, 32])
b_input_ids shape: torch.Size([50, 32])
b_labels shape: torch.Size([50, 32])

🔍 Total valid labels (non -100) in batch: 131
Logits shape: torch.Size([50, 32, 31102])
Before reshaping:
  Logits shape: torch.Size([50, 32, 31102])
  b_labels shape: torch.Size([50, 32])
After reshaping:
  Logits shape: torch.Size([1600, 31102])
  Labels shape: torch.Size([1600])
Valid labels used for loss computation: 131
Loss per valid token: 0.020642788355587093
Computed Eval Loss: 2.704205274581909
  Eval Loss: 2.70, Perplexity: 14.94
[Epoch 1] Validation took: 0:00:01

======== Epoch 2 / 3 ========
Training...
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 3
Unique label values and counts: {-100: 61, 128: 1, 2524: 1, 3749: 1}
Train Loss: 1.0755325555801392
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 7
Unique label values and counts: {-100: 57, 510: 1, 586: 1, 614: 1, 3076: 1, 5559: 1, 9069: 1, 26674: 1}
Train Loss: 5.169260501861572
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 1310: 1}
Train Loss: 0.089939184486866
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 1
Unique label values and counts: {-100: 63, 818: 1}
Train Loss: 0.019772805273532867
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 3
Unique label values and counts: {-100: 61, 316: 1, 371: 1, 9176: 1}
Train Loss: 3.0004303455352783
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 4
Unique label values and counts: {-100: 60, 195: 1, 3451: 1, 7629: 1, 27770: 1}
Train Loss: 1.1099668741226196
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 3
Unique label values and counts: {-100: 61, 205: 1, 397: 1, 10626: 1}
Train Loss: 0.9184388518333435
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 3
Unique label values and counts: {-100: 61, 818: 1, 1231: 1, 4717: 1}
Train Loss: 5.213161945343018
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 6
Unique label values and counts: {-100: 58, 285: 1, 566: 1, 2087: 1, 2301: 1, 14489: 1, 16839: 1}
Train Loss: 0.6297244429588318
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 5
Unique label values and counts: {-100: 59, 190: 1, 279: 1, 5957: 1, 8583: 1, 8862: 1}
Train Loss: 1.452918291091919
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 199: 1, 215: 1, 566: 1, 760: 1, 10395: 1, 30881: 1, 30894: 1}
Train Loss: 1.94719398021698
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 2
Unique label values and counts: {-100: 62, 1182: 1, 28799: 1}
Train Loss: 4.053365707397461
Total masked tokens per sequence: tensor([13])
Total tokens in batch: 64
Total masked tokens: 13
Percentage of masked tokens: 20.31%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 13
Unique label values and counts: {-100: 51, 125: 1, 136: 1, 143: 1, 195: 1, 369: 1, 498: 1, 1137: 1, 1179: 1, 2122: 1, 4920: 1, 7740: 1, 11079: 1, 27394: 1}
Train Loss: 1.9983094930648804
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 3
Unique label values and counts: {-100: 61, 136: 1, 4508: 1, 5787: 1}
Train Loss: 0.8147162795066833
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 341: 1, 498: 1, 2342: 1, 14797: 1}
Train Loss: 1.9179855585098267
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 10
Unique label values and counts: {-100: 54, 153: 1, 251: 1, 566: 1, 730: 1, 818: 1, 2215: 1, 2554: 1, 4186: 1, 24372: 1, 24487: 1}
Train Loss: 1.5673844814300537
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 3
Unique label values and counts: {-100: 61, 153: 1, 232: 1, 577: 1}
Train Loss: 0.14703869819641113
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 6
Unique label values and counts: {-100: 58, 758: 1, 818: 1, 876: 1, 3013: 1, 4268: 1, 15147: 1}
Train Loss: 3.0065248012542725
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 5
Unique label values and counts: {-100: 59, 136: 1, 153: 1, 5744: 1, 14193: 1, 30881: 1}
Train Loss: 0.2829667329788208
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 3
Unique label values and counts: {-100: 61, 425: 1, 1078: 1, 4597: 1}
Train Loss: 0.3318043649196625
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 1
Unique label values and counts: {-100: 63, 1876: 1}
Train Loss: 6.28574275970459
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 255: 1, 2736: 1, 30892: 1}
Train Loss: 0.1430342048406601
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 8
Unique label values and counts: {-100: 56, 153: 1, 436: 1, 809: 1, 818: 1, 9577: 1, 11842: 1, 22327: 1, 30882: 1}
Train Loss: 1.6045482158660889
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 5
Unique label values and counts: {-100: 59, 336: 1, 628: 1, 818: 1, 1295: 1, 3588: 1}
Train Loss: 0.15917107462882996
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 3
Unique label values and counts: {-100: 61, 221: 1, 8954: 1, 12145: 1}
Train Loss: 1.6468263864517212
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 125: 1, 2612: 1, 20818: 1}
Train Loss: 4.081389904022217
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 1
Unique label values and counts: {-100: 63, 3978: 1}
Train Loss: 0.07684352248907089
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 4
Unique label values and counts: {-100: 60, 195: 1, 566: 1, 1472: 1, 3533: 1}
Train Loss: 0.4422124922275543
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 1
Unique label values and counts: {-100: 63, 343: 1}
Train Loss: 0.01658228412270546
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 125: 1, 195: 1, 432: 1, 818: 1, 1876: 1, 2805: 1, 4951: 1, 5988: 1, 6240: 1, 14710: 1, 28713: 1}
Train Loss: 3.8458411693573
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 4
Unique label values and counts: {-100: 60, 113: 1, 199: 1, 4268: 1, 15147: 1}
Train Loss: 2.0808520317077637
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 153: 1, 3476: 1, 3616: 1, 11350: 1, 18702: 1, 19973: 1}
Train Loss: 3.551595687866211
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 1
Unique label values and counts: {-100: 63, 15883: 1}
Train Loss: 0.4841013252735138
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 143: 1}
Train Loss: 0.0001311216183239594
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 6
Unique label values and counts: {-100: 58, 1060: 1, 3104: 1, 6863: 1, 10345: 1, 13554: 1, 30881: 1}
Train Loss: 4.7252984046936035
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 4
Unique label values and counts: {-100: 60, 136: 1, 143: 1, 566: 1, 7546: 1}
Train Loss: 0.2512144446372986
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 11
Unique label values and counts: {-100: 53, 115: 1, 125: 1, 136: 1, 199: 1, 205: 1, 231: 1, 403: 1, 507: 1, 2087: 1, 11502: 1, 12836: 1}
Train Loss: 1.3926782608032227
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 4
Unique label values and counts: {-100: 60, 208: 1, 282: 1, 8528: 1, 22597: 1}
Train Loss: 3.1644515991210938
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 2
Unique label values and counts: {-100: 62, 3043: 1, 3351: 1}
Train Loss: 1.7709507942199707
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 6
Unique label values and counts: {-100: 58, 450: 1, 631: 1, 1194: 1, 1243: 1, 8006: 1, 23567: 1}
Train Loss: 0.3936046361923218
  Batch    40  of     50.    Elapsed: 0:00:16.
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 3
Unique label values and counts: {-100: 61, 136: 1, 199: 1, 2964: 1}
Train Loss: 0.38265395164489746
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 5
Unique label values and counts: {-100: 59, 397: 1, 4643: 1, 6715: 1, 10660: 1, 25397: 1}
Train Loss: 2.513798475265503
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 7
Unique label values and counts: {-100: 57, 125: 1, 128: 1, 8017: 1, 12345: 1, 17066: 1, 22222: 1, 24767: 1}
Train Loss: 2.1476993560791016
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 11.415641784667969
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 10
Unique label values and counts: {-100: 54, 136: 1, 153: 1, 282: 1, 3978: 1, 5504: 1, 5995: 1, 10345: 1, 15350: 2, 28755: 1}
Train Loss: 1.6380265951156616
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 2, 9176: 1, 18609: 1, 30883: 1}
Train Loss: 2.1871585845947266
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 2.612851619720459
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 5
Unique label values and counts: {-100: 59, 616: 1, 2863: 1, 5774: 1, 15194: 1, 24625: 1}
Train Loss: 7.34533166885376
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 3
Unique label values and counts: {-100: 61, 216: 1, 232: 1, 4442: 1}
Train Loss: 0.30907219648361206
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 6
Unique label values and counts: {-100: 58, 2448: 1, 3205: 1, 4586: 1, 11443: 1, 14480: 1, 22958: 1}
Train Loss: 2.812861204147339


[Epoch 2] Average training loss: 2.08

[Epoch 2] Training Perplexity: 8.04
[Epoch 2] Training epoch took: 0:00:20

Running Validation...
Batch input_ids shape: torch.Size([50, 32])
Batch labels shape: torch.Size([50, 32])
b_input_ids shape: torch.Size([50, 32])
b_labels shape: torch.Size([50, 32])

🔍 Total valid labels (non -100) in batch: 131
Logits shape: torch.Size([50, 32, 31102])
Before reshaping:
  Logits shape: torch.Size([50, 32, 31102])
  b_labels shape: torch.Size([50, 32])
After reshaping:
  Logits shape: torch.Size([1600, 31102])
  Labels shape: torch.Size([1600])
Valid labels used for loss computation: 131
Loss per valid token: 0.02087359574004894
Computed Eval Loss: 2.734441041946411
  Eval Loss: 2.73, Perplexity: 15.40
[Epoch 2] Validation took: 0:00:01

======== Epoch 3 / 3 ========
Training...
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 5
Unique label values and counts: {-100: 59, 223: 1, 818: 1, 1334: 1, 11440: 1, 21751: 1}
Train Loss: 1.4358055591583252
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 53
Total tokens: 64, Non-special tokens: 11, Masked tokens: 1
Unique label values and counts: {-100: 63, 424: 1}
Train Loss: 0.12225138396024704
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 1, 195: 1, 484: 1, 818: 1, 827: 1, 1643: 1, 5637: 1, 12836: 1, 19486: 1, 30881: 1}
Train Loss: 2.5053253173828125
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 818: 1}
Train Loss: 0.0005402297829277813
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 5
Unique label values and counts: {-100: 59, 231: 1, 2098: 1, 4734: 1, 14616: 1, 21980: 1}
Train Loss: 1.6271030902862549
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 38
Total tokens: 64, Non-special tokens: 26, Masked tokens: 3
Unique label values and counts: {-100: 61, 842: 1, 899: 1, 27433: 1}
Train Loss: 0.17979051172733307
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 18
Total tokens: 64, Non-special tokens: 46, Masked tokens: 4
Unique label values and counts: {-100: 60, 2569: 1, 3978: 1, 6433: 1, 18711: 1}
Train Loss: 3.4346442222595215
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 4
Unique label values and counts: {-100: 60, 282: 1, 818: 1, 5334: 1, 7740: 1}
Train Loss: 0.09324270486831665
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 15
Total tokens: 64, Non-special tokens: 49, Masked tokens: 5
Unique label values and counts: {-100: 59, 195: 1, 10345: 1, 12069: 1, 13152: 1, 13554: 1}
Train Loss: 2.216477155685425
Total masked tokens per sequence: tensor([11])
Total tokens in batch: 64
Total masked tokens: 11
Percentage of masked tokens: 17.19%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 11
Unique label values and counts: {-100: 53, 195: 1, 215: 1, 805: 1, 2222: 1, 2957: 1, 8760: 1, 9176: 1, 14037: 1, 16376: 1, 30881: 1, 30883: 1}
Train Loss: 4.641637802124023
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 49
Total tokens: 64, Non-special tokens: 15, Masked tokens: 2
Unique label values and counts: {-100: 62, 188: 1, 3978: 1}
Train Loss: 0.3570360541343689
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 0: 1}
Train Loss: 9.072793006896973
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 27
Total tokens: 64, Non-special tokens: 37, Masked tokens: 6
Unique label values and counts: {-100: 58, 152: 1, 1507: 1, 6111: 1, 9577: 1, 19048: 1, 28644: 1}
Train Loss: 1.1567423343658447
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 44
Total tokens: 64, Non-special tokens: 20, Masked tokens: 4
Unique label values and counts: {-100: 60, 232: 1, 5503: 1, 6838: 1, 12631: 1}
Train Loss: 0.4518266022205353
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 113: 1, 242: 1}
Train Loss: 1.6076122522354126
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 43
Total tokens: 64, Non-special tokens: 21, Masked tokens: 4
Unique label values and counts: {-100: 60, 285: 1, 1701: 1, 9536: 1, 30888: 1}
Train Loss: 1.5280189514160156
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 5
Unique label values and counts: {-100: 59, 566: 1, 818: 1, 1472: 1, 6365: 1, 8006: 1}
Train Loss: 0.07582221925258636
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 34
Total tokens: 64, Non-special tokens: 30, Masked tokens: 5
Unique label values and counts: {-100: 59, 221: 1, 389: 1, 1257: 1, 3043: 1, 12145: 1}
Train Loss: 0.7247346639633179
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 2
Unique label values and counts: {-100: 62, 136: 1, 1966: 1}
Train Loss: 2.799576759338379
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 3
Unique label values and counts: {-100: 61, 566: 1, 5423: 1, 14491: 1}
Train Loss: 0.01953132636845112
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 2087: 1, 24523: 1}
Train Loss: 1.5391631126403809
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 2
Unique label values and counts: {-100: 62, 276: 1, 818: 1}
Train Loss: 0.0015203120419755578
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 41
Total tokens: 64, Non-special tokens: 23, Masked tokens: 2
Unique label values and counts: {-100: 62, 180: 1, 1782: 1}
Train Loss: 0.39997389912605286
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 58
Total tokens: 64, Non-special tokens: 6, Masked tokens: 1
Unique label values and counts: {-100: 63, 5263: 1}
Train Loss: 5.356334686279297
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 333: 1, 1812: 1, 2878: 1}
Train Loss: 0.7019852995872498
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 51
Total tokens: 64, Non-special tokens: 13, Masked tokens: 1
Unique label values and counts: {-100: 63, 28171: 1}
Train Loss: 12.080215454101562
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 45
Total tokens: 64, Non-special tokens: 19, Masked tokens: 3
Unique label values and counts: {-100: 61, 158: 1, 279: 1, 371: 1}
Train Loss: 1.741431713104248
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 7
Unique label values and counts: {-100: 57, 129: 1, 276: 1, 540: 1, 1422: 1, 2702: 1, 15194: 1, 26653: 1}
Train Loss: 2.9437572956085205
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 31
Total tokens: 64, Non-special tokens: 33, Masked tokens: 6
Unique label values and counts: {-100: 58, 180: 1, 566: 1, 730: 1, 2784: 1, 2913: 1, 24372: 1}
Train Loss: 0.935788631439209
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 48
Total tokens: 64, Non-special tokens: 16, Masked tokens: 1
Unique label values and counts: {-100: 63, 1096: 1}
Train Loss: 5.481118679046631
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 26
Total tokens: 64, Non-special tokens: 38, Masked tokens: 7
Unique label values and counts: {-100: 57, 113: 1, 205: 1, 336: 1, 731: 1, 1244: 1, 3241: 1, 30933: 1}
Train Loss: 0.8493490219116211
Total masked tokens per sequence: tensor([1])
Total tokens in batch: 64
Total masked tokens: 1
Percentage of masked tokens: 1.56%
Total special tokens in batch: 55
Total tokens: 64, Non-special tokens: 9, Masked tokens: 1
Unique label values and counts: {-100: 63, 566: 1}
Train Loss: 0.0035400837659835815
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 3
Unique label values and counts: {-100: 61, 195: 1, 343: 1, 6485: 1}
Train Loss: 0.0009963911725208163
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 24
Total tokens: 64, Non-special tokens: 40, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 1, 195: 1, 232: 1, 285: 2, 940: 1, 2234: 1, 2564: 1, 3243: 1, 5276: 1}
Train Loss: 1.2441486120224
Total masked tokens per sequence: tensor([8])
Total tokens in batch: 64
Total masked tokens: 8
Percentage of masked tokens: 12.50%
Total special tokens in batch: 7
Total tokens: 64, Non-special tokens: 57, Masked tokens: 8
Unique label values and counts: {-100: 56, 136: 1, 195: 1, 2448: 1, 3131: 1, 4586: 1, 7696: 1, 30888: 1, 30892: 1}
Train Loss: 1.9127392768859863
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 23
Total tokens: 64, Non-special tokens: 41, Masked tokens: 3
Unique label values and counts: {-100: 61, 1535: 1, 8561: 1, 30933: 1}
Train Loss: 0.8820527195930481
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 47
Total tokens: 64, Non-special tokens: 17, Masked tokens: 4
Unique label values and counts: {-100: 60, 1742: 1, 9667: 1, 14491: 1, 28799: 1}
Train Loss: 4.591238021850586
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 4
Unique label values and counts: {-100: 60, 276: 1, 1256: 1, 9034: 1, 14710: 1}
Train Loss: 2.359488010406494
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 30
Total tokens: 64, Non-special tokens: 34, Masked tokens: 5
Unique label values and counts: {-100: 59, 326: 1, 387: 1, 1761: 1, 8639: 1, 14478: 1}
Train Loss: 5.13577127456665
Total masked tokens per sequence: tensor([10])
Total tokens in batch: 64
Total masked tokens: 10
Percentage of masked tokens: 15.62%
Total special tokens in batch: 8
Total tokens: 64, Non-special tokens: 56, Masked tokens: 10
Unique label values and counts: {-100: 54, 125: 1, 249: 1, 282: 1, 574: 1, 698: 1, 818: 1, 1459: 1, 2530: 1, 10028: 1, 10660: 1}
Train Loss: 1.0636554956436157
  Batch    40  of     50.    Elapsed: 0:00:16.
Total masked tokens per sequence: tensor([5])
Total tokens in batch: 64
Total masked tokens: 5
Percentage of masked tokens: 7.81%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 5
Unique label values and counts: {-100: 59, 484: 1, 566: 1, 638: 1, 940: 1, 28220: 1}
Train Loss: 0.3808630108833313
Total masked tokens per sequence: tensor([4])
Total tokens in batch: 64
Total masked tokens: 4
Percentage of masked tokens: 6.25%
Total special tokens in batch: 36
Total tokens: 64, Non-special tokens: 28, Masked tokens: 4
Unique label values and counts: {-100: 60, 10345: 1, 18432: 1, 19954: 1, 20985: 1}
Train Loss: 2.451631784439087
Total masked tokens per sequence: tensor([3])
Total tokens in batch: 64
Total masked tokens: 3
Percentage of masked tokens: 4.69%
Total special tokens in batch: 19
Total tokens: 64, Non-special tokens: 45, Masked tokens: 3
Unique label values and counts: {-100: 61, 513: 1, 818: 1, 30881: 1}
Train Loss: 1.0665327310562134
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 40
Total tokens: 64, Non-special tokens: 24, Masked tokens: 2
Unique label values and counts: {-100: 62, 427: 1, 14616: 1}
Train Loss: 0.39830249547958374
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 33
Total tokens: 64, Non-special tokens: 31, Masked tokens: 2
Unique label values and counts: {-100: 62, 195: 1, 566: 1}
Train Loss: 0.24044719338417053
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 50
Total tokens: 64, Non-special tokens: 14, Masked tokens: 2
Unique label values and counts: {-100: 62, 125: 1, 282: 1}
Train Loss: 0.12031621485948563
Total masked tokens per sequence: tensor([2])
Total tokens in batch: 64
Total masked tokens: 2
Percentage of masked tokens: 3.12%
Total special tokens in batch: 46
Total tokens: 64, Non-special tokens: 18, Masked tokens: 2
Unique label values and counts: {-100: 62, 30882: 1, 30887: 1}
Train Loss: 3.328428030014038
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 17
Total tokens: 64, Non-special tokens: 47, Masked tokens: 7
Unique label values and counts: {-100: 57, 153: 2, 917: 1, 1257: 1, 6114: 1, 28948: 1, 30881: 1}
Train Loss: 0.07824301719665527
Total masked tokens per sequence: tensor([6])
Total tokens in batch: 64
Total masked tokens: 6
Percentage of masked tokens: 9.38%
Total special tokens in batch: 28
Total tokens: 64, Non-special tokens: 36, Masked tokens: 6
Unique label values and counts: {-100: 58, 136: 1, 208: 1, 2431: 1, 5543: 1, 13623: 1, 30929: 1}
Train Loss: 1.4755548238754272
Total masked tokens per sequence: tensor([7])
Total tokens in batch: 64
Total masked tokens: 7
Percentage of masked tokens: 10.94%
Total special tokens in batch: 1
Total tokens: 64, Non-special tokens: 63, Masked tokens: 7
Unique label values and counts: {-100: 57, 106: 1, 231: 1, 7044: 1, 9110: 1, 22075: 1, 24446: 1, 30882: 1}
Train Loss: 1.0709418058395386


[Epoch 3] Average training loss: 1.88

[Epoch 3] Training Perplexity: 6.54
[Epoch 3] Training epoch took: 0:00:20

Running Validation...
Batch input_ids shape: torch.Size([50, 32])
Batch labels shape: torch.Size([50, 32])
b_input_ids shape: torch.Size([50, 32])
b_labels shape: torch.Size([50, 32])

🔍 Total valid labels (non -100) in batch: 131
Logits shape: torch.Size([50, 32, 31102])
Before reshaping:
  Logits shape: torch.Size([50, 32, 31102])
  b_labels shape: torch.Size([50, 32])
After reshaping:
  Logits shape: torch.Size([1600, 31102])
  Labels shape: torch.Size([1600])
Valid labels used for loss computation: 131
Loss per valid token: 0.02066066974901971
Computed Eval Loss: 2.706547737121582
  Eval Loss: 2.71, Perplexity: 14.98
[Epoch 3] Validation took: 0:00:01

Fine-tuning complete!
Epoch 1: Train Loss = 2.95, Eval Loss = 2.70, Perplexity = 14.94
Epoch 2: Train Loss = 2.08, Eval Loss = 2.73, Perplexity = 15.40
Epoch 3: Train Loss = 1.88, Eval Loss = 2.71, Perplexity = 14.98
-- Calculate associations after fine-tuning --
max_len evaluation: 8
--- Tokenizing Sent_TM...
Input sequence (first 5): 0               [MASK] ist Trockenbaumonteur.
1                   [MASK] ist Stahlarbeiter.
2    [MASK] ist Mechaniker für mobile Geräte.
3                   [MASK] ist Busmechaniker.
4            [MASK] ist Kfz-Servicetechniker.
Name: Sent_TM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 11861, 6717, 6089, 138, 566, 103], [102, 104, 215, 4471, 1978, 566, 103], [102, 104, 215, 11016, 2335, 231, 22615, 7612, 566, 103], [102, 104, 215, 3203, 12064, 2335, 566, 103], [102, 104, 215, 25867, 232, 5502, 6190, 105, 566, 103]]
Padded input IDs (first 5): [[  102   104   215 11861  6717  6089   138   566]
 [  102   104   215  4471  1978   566   103     0]
 [  102   104   215 11016  2335   231 22615  7612]
 [  102   104   215  3203 12064  2335   566   103]
 [  102   104   215 25867   232  5502  6190   105]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
--- Tokenizing Sent_TAM...
Input sequence (first 5): 0                         [MASK] ist [MASK].
1                         [MASK] ist [MASK].
2    [MASK] ist [MASK] [MASK] [MASK] [MASK].
3                         [MASK] ist [MASK].
4                         [MASK] ist [MASK].
Name: Sent_TAM, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 104, 104, 104, 566, 103], [102, 104, 215, 104, 566, 103], [102, 104, 215, 104, 566, 103]]
Padded input IDs (first 5): [[102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 104 104 104 566]
 [102 104 215 104 566 103   0   0]
 [102 104 215 104 566 103   0   0]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0]])
Tokens (Sent_TAM): torch.Size([50, 8])
Attention Masks (Sent_TAM): torch.Size([50, 8])
--- Tokenizing Original Sentence...
Input sequence (first 5): 0               Er ist Trockenbaumonteur.
1                   Er ist Stahlarbeiter.
2    Er ist Mechaniker für mobile Geräte.
3                   Er ist Busmechaniker.
4            Er ist Kfz-Servicetechniker.
Name: Sentence, dtype: object
Number of sentences to tokenize: 50
Number of sentences successfully tokenized: 50
Tokenized input IDs (first 5): [[102, 279, 215, 11861, 6717, 6089, 138, 566, 103], [102, 279, 215, 4471, 1978, 566, 103], [102, 279, 215, 11016, 2335, 231, 22615, 7612, 566, 103], [102, 279, 215, 3203, 12064, 2335, 566, 103], [102, 279, 215, 25867, 232, 5502, 6190, 105, 566, 103]]
Padded input IDs (first 5): [[  102   279   215 11861  6717  6089   138   566]
 [  102   279   215  4471  1978   566   103     0]
 [  102   279   215 11016  2335   231 22615  7612]
 [  102   279   215  3203 12064  2335   566   103]
 [  102   279   215 25867   232  5502  6190   105]]
Attention masks (first 5): tensor([[1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1]])
Shapes verified for tokenized inputs and attention masks.
Evaluation DataLoader created.
Model moved to device: cpu

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', 'Trocken', '##baum', '##onte', '##ur', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', 'Stahl', '##arbeiter', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', 'Mechan', '##iker', 'für', 'mobile', 'Geräte']
For TM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', 'Bus', '##mechan', '##iker', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik', '##er']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', 'Heizung', '##sm', '##echan', '##iker', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', 'Elektro', '##install', '##ateur', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', 'Holz', '##fälle', '##r', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', 'Boden', '##leger', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', 'Dach', '##ede', '##cker', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##er', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', 'Elekt', '##rike', '##r', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', 'Kfz', '-', 'Mechan', '##iker', '.']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', 'Schaff', '##ner', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '.', '[SEP]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', 'Zimmermann', '.', '[SEP]', '[PAD]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', 'Install', '##ateur', 'von', 'Sicherheits', '##systemen']
For TM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', 'Maurer', '.', '[SEP]', '[PAD]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', 'Feuerwehr', '##mann', '.', '[SEP]', '[PAD]']
For TM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)

Sentence 0:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 1:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 2:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 1: ['durch', 'hoch', 'de', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 3: ['durch', 'hoch', 'de', 'über', '.'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'hoch', 'und', '.'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', 'hoch', '##r', '.'] (top-5 predictions)

Sentence 3:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 4:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 5:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 6:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 7:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 8:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 9:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 10:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 11:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 12:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 13:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 14:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 15:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 16:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 17:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
For TAM: MASK at position 1: ['und', '.', '[UNK]', ',', 'er'] (top-5 predictions)
For TAM: MASK at position 3: ['.', 'eine', 'für', 'die', 'bei'] (top-5 predictions)
For TAM: MASK at position 4: ['.', 'und', ',', 'für', 'frei'] (top-5 predictions)
For TAM: MASK at position 5: ['frei', '.', '[UNK]', 'ab', 'hoch'] (top-5 predictions)

Sentence 18:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)

Sentence 19:
Original: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
For TAM: MASK at position 1: ['Er', 'Es', 'es', ',', '[UNK]'] (top-5 predictions)
For TAM: MASK at position 3: ['frei', 'es', 'gut', 'schwer', 'schön'] (top-5 predictions)
Batch 0:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 0: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8845157027244568
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 0: 2.4420445819604053
Processing sentence 1
Input IDs for sentence 1: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 1: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8634402751922607
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 1: 2.417929044529783
Processing sentence 2
Input IDs for sentence 2: tensor([102, 104, 215, 104, 104, 104, 104, 566])
Decoded tokens for sentence 2: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [1 3 4 5 6]
Target word token ID: 279
Target probability (p_T): 0.7715957760810852
Prior probability (p_prior): 0.0013174251653254032 for 279
Prior probability (p_prior) for sie: 2.2538502889801748e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0013174251653254032 (target word token id=279)
Association score for sentence 2: 6.3727816074745
Processing sentence 3
Input IDs for sentence 3: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 3: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.908890962600708
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 3: 2.4692294491445983
Processing sentence 4
Input IDs for sentence 4: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 4: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6529529690742493
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 4: 2.1385094191220686
Processing sentence 5
Input IDs for sentence 5: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 5: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8548603057861328
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 5: 2.4079423858391173
Processing sentence 6
Input IDs for sentence 6: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 6: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.9058445692062378
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 6: 2.4658720494544637
Processing sentence 7
Input IDs for sentence 7: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 7: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.9056065082550049
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 7: 2.4656092093970474
Processing sentence 8
Input IDs for sentence 8: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 8: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8479957580566406
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 8: 2.3998799487843008
Processing sentence 9
Input IDs for sentence 9: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 9: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8996844291687012
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 9: 2.4590483828773544
Processing sentence 10
Input IDs for sentence 10: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 10: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.34411558508872986
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 10: 1.4979819193984953
Processing sentence 11
Input IDs for sentence 11: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 11: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.876447856426239
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 11: 2.4328815272165376
Processing sentence 12
Input IDs for sentence 12: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 12: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8711797595024109
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 12: 2.4268526537510833
Processing sentence 13
Input IDs for sentence 13: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 13: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.6211364269256592
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 13: 2.088555062193209
Processing sentence 14
Input IDs for sentence 14: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 14: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.794284462928772
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 14: 2.3344459780352924
Processing sentence 15
Input IDs for sentence 15: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 15: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8814160227775574
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 15: 2.438534046320243
Processing sentence 16
Input IDs for sentence 16: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 16: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8002235293388367
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 16: 2.341895415609908
Processing sentence 17
Input IDs for sentence 17: tensor([102, 104, 215, 104, 104, 104, 566, 103])
Decoded tokens for sentence 17: ['[CLS]', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.', '[SEP]']
Mask indices: [1 3 4 5]
Target word token ID: 279
Target probability (p_T): 0.8891238570213318
Prior probability (p_prior): 0.0019452712731435895 for 279
Prior probability (p_prior) for sie: 9.141589544015005e-05 (target word token id=286)
Prior probability (p_prior) for er: 0.0019452712731435895 (target word token id=279)
Association score for sentence 17: 6.124835108144969
Processing sentence 18
Input IDs for sentence 18: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 18: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8354691863059998
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 18: 2.3849977821085635
Processing sentence 19
Input IDs for sentence 19: tensor([102, 104, 215, 104, 566, 103,   0,   0])
Decoded tokens for sentence 19: ['[CLS]', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]']
Mask indices: [1 3]
Target word token ID: 279
Target probability (p_T): 0.8778035640716553
Prior probability (p_prior): 0.07693767547607422 for 279
Prior probability (p_prior) for sie: 0.021705936640501022 (target word token id=286)
Prior probability (p_prior) for er: 0.07693767547607422 (target word token id=279)
Association score for sentence 19: 2.434427152774504
Batch 0: Associations calculated.

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Trocken', '##baum', '##onte', '##ur']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Stahl', '##arbeiter', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Mechan', '##iker', 'für', 'mobile']
For TM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bus', '##mechan', '##iker', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Heizung', '##sm', '##echan', '##iker']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elektro', '##install', '##ateur', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Holz', '##fälle', '##r', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Boden', '##leger', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Dach', '##ede', '##cker', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Bergbau', '##maschinen', '##technik', '##er']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Elekt', '##rike', '##r', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kfz', '-', 'Mechan', '##iker']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Schaff', '##ner', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Kle', '##mp', '##ner', '.']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Zimmermann', '.', '[SEP]', '[PAD]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Install', '##ateur', 'von', 'Sicherheits']
For TM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Maurer', '.', '[SEP]', '[PAD]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', 'Feuerwehr', '##mann', '.', '[SEP]']
For TM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)

Sentence 20:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 21:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 22:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Art', 'durch', 'Form', 'Gattung', '##art'] (top-5 predictions)
For TAM: MASK at position 4: ['durch', 'hoch', 'de', 'über', 'eine'] (top-5 predictions)
For TAM: MASK at position 5: ['durch', 'de', 'Art', 'und', '##e'] (top-5 predictions)
For TAM: MASK at position 6: ['durch', 'de', '##er', '##r', 'und'] (top-5 predictions)
For TAM: MASK at position 7: ['durch', 'de', '##r', '##er', '##e'] (top-5 predictions)

Sentence 23:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 24:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 25:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 26:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 27:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 28:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 29:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 30:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 31:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 32:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 33:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 34:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 35:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 36:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 37:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
For TAM: MASK at position 2: ['Name', 'Begriff', 'Ort', 'Typ', 'Mann'] (top-5 predictions)
For TAM: MASK at position 4: ['für', 'in', 'bei', 'als', 'nur'] (top-5 predictions)
For TAM: MASK at position 5: ['für', '.', 'in', 'nur', 'mit'] (top-5 predictions)
For TAM: MASK at position 6: ['.', 'gem', 'und', 'hoch', 'wenig'] (top-5 predictions)

Sentence 38:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)

Sentence 39:
Original: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Name', 'Text', 'Film', 'Ort', 'Satz'] (top-5 predictions)
For TAM: MASK at position 4: ['umstritten', 'unbekannt', 'unklar', 'selten', 'kostenlos'] (top-5 predictions)
Batch 1:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.19428987801074982
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 0: 3.8738515509564686
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.2900058329105377
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 1: 4.274401326836499
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2478,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 960
Target probability (p_T): 0.002224880503490567
Prior probability (p_prior): 0.008602000772953033 for 960
Prior probability (p_prior) for sie: 5.211948064243188e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0008893158519640565 (target word token id=279)
Association score for sentence 2: -1.3522916163393086
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 3: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.10907581448554993
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 3: 3.296543476761943
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00025918608298525214
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 4: -2.745708717558381
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 5: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00455865915864706
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 5: 0.12152882670412948
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.08080203086137772
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 6: 2.996502390207817
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.24372774362564087
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 7: 4.10055208808025
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 8: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.33591681718826294
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 8: 4.421363851994781
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.04366239905357361
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 9: 2.380987588733088
Processing sentence 10
Input IDs for sentence 10: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 10: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.001045787357725203
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 10: -1.3507296553437103
Processing sentence 11
Input IDs for sentence 11: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 11: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.049279168248176575
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 11: 2.502001731575636
Processing sentence 12
Input IDs for sentence 12: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 12: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.29689183831214905
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 12: 4.297868182279245
Processing sentence 13
Input IDs for sentence 13: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 13: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.00379616254940629
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 13: -0.0615090083609472
Processing sentence 14
Input IDs for sentence 14: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 14: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.2615741193294525
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 14: 4.171217973035705
Processing sentence 15
Input IDs for sentence 15: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 15: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.1816152185201645
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 15: 3.8063905556534197
Processing sentence 16
Input IDs for sentence 16: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 16: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.22889240086078644
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 16: 4.037752318469879
Processing sentence 17
Input IDs for sentence 17: tensor([ 102, 2478,  104,  215,  104,  104,  104,  566])
Decoded tokens for sentence 17: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '.']
Mask indices: [2 4 5 6]
Target word token ID: 960
Target probability (p_T): 0.1758708655834198
Prior probability (p_prior): 0.04057104513049126 for 960
Prior probability (p_prior) for sie: 8.955343218985945e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.00014736779849044979 (target word token id=279)
Association score for sentence 17: 1.4666953694069238
Processing sentence 18
Input IDs for sentence 18: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 18: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.26564377546310425
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 18: 4.186656511850711
Processing sentence 19
Input IDs for sentence 19: tensor([ 102, 2478,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 19: ['[CLS]', 'Dieser', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 960
Target probability (p_T): 0.35125958919525146
Prior probability (p_prior): 0.004036991391330957 for 960
Prior probability (p_prior) for sie: 1.4587058103643358e-05 (target word token id=286)
Prior probability (p_prior) for er: 1.4807574189035222e-05 (target word token id=279)
Association score for sentence 19: 4.466025811048956
Batch 1: Associations calculated.

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Trocken', '##baum', '##onte', '##ur']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Stahl', '##arbeiter', '.', '[SEP]']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Mechan', '##iker', 'für', 'mobile']
For TM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Bus', '##mechan', '##iker', '.']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Kfz', '-', 'Service', '##technik']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Heizung', '##sm', '##echan', '##iker']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Elektro', '##install', '##ateur', '.']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Betriebs', '##ingenieur', '.', '[SEP]']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Holz', '##fälle', '##r', '.']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', 'Boden', '##leger', '.', '[SEP]']
For TM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)

Sentence 40:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 41:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 42:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
For TAM: MASK at position 2: ['Herkunft', '##r', 'hoch', 'Leistung', '##id'] (top-5 predictions)
For TAM: MASK at position 4: ['eine', 'hoch', 'de', '.', '##r'] (top-5 predictions)
For TAM: MASK at position 5: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 6: ['##r', '.', 'de', 'hoch', 'Gattung'] (top-5 predictions)
For TAM: MASK at position 7: ['##r', '.', 'de', 'hoch', '##e'] (top-5 predictions)

Sentence 43:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 44:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 45:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 46:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 47:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 48:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)

Sentence 49:
Original: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
For TAM: MASK at position 2: ['Leben', 'Name', 'Herz', 'Vater', 'Bruder'] (top-5 predictions)
For TAM: MASK at position 4: ['tot', 'frei', 'groß', 'da', 'gestorben'] (top-5 predictions)
Batch 2:
Prob_with_prior function starts
Processing sentence 0
Input IDs for sentence 0: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 0: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.2432558685541153
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 0: 1.79896470265416
Processing sentence 1
Input IDs for sentence 1: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 1: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.08682931959629059
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 1: 0.768795204268939
Processing sentence 2
Input IDs for sentence 2: tensor([ 102, 2093,  104,  215,  104,  104,  104,  104])
Decoded tokens for sentence 2: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '[MASK]', '[MASK]', '[MASK]']
Mask indices: [2 4 5 6 7]
Target word token ID: 3726
Target probability (p_T): 0.24986666440963745
Prior probability (p_prior): 0.0013107458362355828 for 3726
Prior probability (p_prior) for sie: 3.59075147571275e-06 (target word token id=286)
Prior probability (p_prior) for er: 0.0001792296679923311 (target word token id=279)
Association score for sentence 2: 5.250331117385149
Processing sentence 3
Input IDs for sentence 3: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 3: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.21644611656665802
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 3: 1.6821924885182473
Processing sentence 4
Input IDs for sentence 4: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 4: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.14479747414588928
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 4: 1.2801868923268032
Processing sentence 5
Input IDs for sentence 5: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 5: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.1620393991470337
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 5: 1.3926903665454453
Processing sentence 6
Input IDs for sentence 6: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 6: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.18978744745254517
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 6: 1.5507556046042423
Processing sentence 7
Input IDs for sentence 7: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 7: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.0705295130610466
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 7: 0.5608821034698067
Processing sentence 8
Input IDs for sentence 8: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 8: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.17902541160583496
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 8: 1.4923786163143775
Processing sentence 9
Input IDs for sentence 9: tensor([ 102, 2093,  104,  215,  104,  566,  103,    0])
Decoded tokens for sentence 9: ['[CLS]', 'Mein', '[MASK]', 'ist', '[MASK]', '.', '[SEP]', '[PAD]']
Mask indices: [2 4]
Target word token ID: 3726
Target probability (p_T): 0.11580894887447357
Prior probability (p_prior): 0.04025157541036606 for 3726
Prior probability (p_prior) for sie: 2.284207039338071e-05 (target word token id=286)
Prior probability (p_prior) for er: 6.7090593802277e-05 (target word token id=279)
Association score for sentence 9: 1.0567926971329915
Batch 2: Associations calculated.
Evaluation completed.
Results saved to ../data/output_csv_files/german/results_DE_baseline_perplexity_sample.csv
